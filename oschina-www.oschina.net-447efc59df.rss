<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:media="http://www.rssboard.org/media-rss" version="2.0">
  <channel>
    <title><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></title>
    <link>undefined</link>
    <image>
      <url>https://www.oschina.net/img/logo.gif</url>
      <title>OSCHINA 社区最新新闻[RSS+]</title>
      <link>undefined</link>
    </image>
    <language>zh-CN</language>
    <atom:link href="https://www.oschina.net/news/rss" rel="self" type="application/rss+xml"/>
    <copyright><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></copyright>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[
      OSCHINA - 中文开源技术交流社区<br />
<br />
<a href="https://www.oschina.net/news/rss" target="_blank">https://www.oschina.net/news/rss</a>
      ]]>
    </itunes:summary>
    <description>
      <![CDATA[
      OSCHINA - 中文开源技术交流社区<br />
<br />
<a href="https://www.oschina.net/news/rss" target="_blank">https://www.oschina.net/news/rss</a>
      ]]>
    </description>
    <itunes:owner>
      <itunes:name><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:name>
    </itunes:owner>
    <itunes:image href="https://www.oschina.net/img/logo.gif"/>
<item>
    <title><![CDATA[GNOME Flatpak 运行时不再兼容 32 位扩展]]></title>
    <link>https://www.oschina.net/news/377416</link>
    <itunes:title><![CDATA[GNOME Flatpak 运行时不再兼容 32 位扩展]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>GNOME 团队近日<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.gnome.org%2Falatiera%2F2025%2F10%2F13%2Fflatpak-32bit%2F" target="_blank">宣布</a>，在最新的 <strong>GNOME 49</strong> 中，Flatpak 运行时正式移除用于 32 位环境的兼容扩展（<code>org.gnome.Platform.i386.Compat</code>），意味着 GNOME 平台今后将仅支持 <strong>x86_64</strong> 和 <strong>AArch64</strong> 架构。</p>
<p>更早之前，GNOME 已经移除了对 <strong>ARMv7（32 位 ARM）</strong> 的支持，以及对 i386（传统的 32 位 x86） 构建的支持。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-deeab67562.png"></p>
<p>这一扩展最初用于支持如 Wine 等依赖 32 位库的应用，但由于维护资源有限，GNOME 开发者决定放弃该功能。据称，这一调整部分原因是<span><strong>项目失去了持续集成环境中的捐赠硬件</strong></span>，32 位兼容模块因此成为首个被精简的目标。</p>
<p>在官方公告中（由 Jordan Petridis 发布）提到，如果有发行版或用户还依赖于 32 位 GNOME 构建，那么他们将需要自行维护、调试这些版本，或直接参与上游（GNOME 社区）的工作，以避免 32 位版本逐渐“腐蚀”（bit rot）。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>GNOME 团队近日<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.gnome.org%2Falatiera%2F2025%2F10%2F13%2Fflatpak-32bit%2F" target="_blank">宣布</a>，在最新的 <strong>GNOME 49</strong> 中，Flatpak 运行时正式移除用于 32 位环境的兼容扩展（<code>org.gnome.Platform.i386.Compat</code>），意味着 GNOME 平台今后将仅支持 <strong>x86_64</strong> 和 <strong>AArch64</strong> 架构。</p>
<p>更早之前，GNOME 已经移除了对 <strong>ARMv7（32 位 ARM）</strong> 的支持，以及对 i386（传统的 32 位 x86） 构建的支持。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-deeab67562.png"></p>
<p>这一扩展最初用于支持如 Wine 等依赖 32 位库的应用，但由于维护资源有限，GNOME 开发者决定放弃该功能。据称，这一调整部分原因是<span><strong>项目失去了持续集成环境中的捐赠硬件</strong></span>，32 位兼容模块因此成为首个被精简的目标。</p>
<p>在官方公告中（由 Jordan Petridis 发布）提到，如果有发行版或用户还依赖于 32 位 GNOME 构建，那么他们将需要自行维护、调试这些版本，或直接参与上游（GNOME 社区）的工作，以避免 32 位版本逐渐“腐蚀”（bit rot）。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>GNOME 团队近日<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.gnome.org%2Falatiera%2F2025%2F10%2F13%2Fflatpak-32bit%2F" target="_blank">宣布</a>，在最新的 <strong>GNOME 49</strong> 中，Flatpak 运行时正式移除用于 32 位环境的兼容扩展（<code>org.gnome.Platform.i386.Compat</code>），意味着 GNOME 平台今后将仅支持 <strong>x86_64</strong> 和 <strong>AArch64</strong> 架构。</p>
<p>更早之前，GNOME 已经移除了对 <strong>ARMv7（32 位 ARM）</strong> 的支持，以及对 i386（传统的 32 位 x86） 构建的支持。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-deeab67562.png"></p>
<p>这一扩展最初用于支持如 Wine 等依赖 32 位库的应用，但由于维护资源有限，GNOME 开发者决定放弃该功能。据称，这一调整部分原因是<span><strong>项目失去了持续集成环境中的捐赠硬件</strong></span>，32 位兼容模块因此成为首个被精简的目标。</p>
<p>在官方公告中（由 Jordan Petridis 发布）提到，如果有发行版或用户还依赖于 32 位 GNOME 构建，那么他们将需要自行维护、调试这些版本，或直接参与上游（GNOME 社区）的工作，以避免 32 位版本逐渐“腐蚀”（bit rot）。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-deeab67562.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-deeab67562.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 19:26:44 +0800</pubDate>
  </item><item>
    <title><![CDATA[WinBoat - 在 Linux 上运行 Windows 应用]]></title>
    <link>https://www.oschina.net/p/winboat</link>
    <itunes:title><![CDATA[WinBoat - 在 Linux 上运行 Windows 应用]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>WinBoat 是一款 Electron 应用，它允许你使用容器化方法在 Linux 上运行 Windows 应用。Windows 以虚拟机的形式在 Docker 容器内运行，使用&nbsp;<span><span>&nbsp;</span></span><a href="https://github.com/TibixDev/winboat/tree/main/guest_server">WinBoat Guest Server</a><span><span>&nbsp;</span></span>与其通信，从 Windows 中检索所需的数据。为了将应用程序合成为原生操作系统级窗口，将 FreeRDP 与 Windows 的 RemoteApp 协议结合使用。</p>
<p><span>WinBoat 目前处于测试阶段。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-17332fb737.png"></p>
<p><strong>特性：</strong></p>
<ul>
<li><strong>优雅的界面</strong>：时尚直观的界面，将 Windows 无缝集成到你的 Linux 桌面环境中，让你感觉像原生体验</li>
<li><strong>自动安装</strong>：通过界面进行简单的安装过程 - 选择你的偏好和规格，然后自动处理其余部分</li>
<li><strong>运行任何应用程序</strong>：只要能在 Windows 上运行，就能在 WinBoat 上运行。在 Linux 环境中，像原生操作系统级窗口一样畅享各种 Windows 应用程序。</li>
<li><strong>完整的 Windows 桌面</strong>：在需要时访问完整的 Windows 桌面体验，或运行无缝集成到 Linux 工作流程中的单个应用程序</li>
<li><strong>文件系统集成</strong>：主目录安装在 Windows 中，允许在两个系统之间轻松共享文件，没有任何麻烦</li>
<li><strong>还有更多</strong>：智能卡直通、资源监控以及定期添加的更多功能</li>
</ul>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>WinBoat 是一款 Electron 应用，它允许你使用容器化方法在 Linux 上运行 Windows 应用。Windows 以虚拟机的形式在 Docker 容器内运行，使用&nbsp;<span><span>&nbsp;</span></span><a href="https://github.com/TibixDev/winboat/tree/main/guest_server">WinBoat Guest Server</a><span><span>&nbsp;</span></span>与其通信，从 Windows 中检索所需的数据。为了将应用程序合成为原生操作系统级窗口，将 FreeRDP 与 Windows 的 RemoteApp 协议结合使用。</p>
<p><span>WinBoat 目前处于测试阶段。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-17332fb737.png"></p>
<p><strong>特性：</strong></p>
<ul>
<li><strong>优雅的界面</strong>：时尚直观的界面，将 Windows 无缝集成到你的 Linux 桌面环境中，让你感觉像原生体验</li>
<li><strong>自动安装</strong>：通过界面进行简单的安装过程 - 选择你的偏好和规格，然后自动处理其余部分</li>
<li><strong>运行任何应用程序</strong>：只要能在 Windows 上运行，就能在 WinBoat 上运行。在 Linux 环境中，像原生操作系统级窗口一样畅享各种 Windows 应用程序。</li>
<li><strong>完整的 Windows 桌面</strong>：在需要时访问完整的 Windows 桌面体验，或运行无缝集成到 Linux 工作流程中的单个应用程序</li>
<li><strong>文件系统集成</strong>：主目录安装在 Windows 中，允许在两个系统之间轻松共享文件，没有任何麻烦</li>
<li><strong>还有更多</strong>：智能卡直通、资源监控以及定期添加的更多功能</li>
</ul>]]>
    </description>
    <content:encoded><![CDATA[<p>WinBoat 是一款 Electron 应用，它允许你使用容器化方法在 Linux 上运行 Windows 应用。Windows 以虚拟机的形式在 Docker 容器内运行，使用&nbsp;<span><span>&nbsp;</span></span><a href="https://github.com/TibixDev/winboat/tree/main/guest_server">WinBoat Guest Server</a><span><span>&nbsp;</span></span>与其通信，从 Windows 中检索所需的数据。为了将应用程序合成为原生操作系统级窗口，将 FreeRDP 与 Windows 的 RemoteApp 协议结合使用。</p>
<p><span>WinBoat 目前处于测试阶段。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-17332fb737.png"></p>
<p><strong>特性：</strong></p>
<ul>
<li><strong>优雅的界面</strong>：时尚直观的界面，将 Windows 无缝集成到你的 Linux 桌面环境中，让你感觉像原生体验</li>
<li><strong>自动安装</strong>：通过界面进行简单的安装过程 - 选择你的偏好和规格，然后自动处理其余部分</li>
<li><strong>运行任何应用程序</strong>：只要能在 Windows 上运行，就能在 WinBoat 上运行。在 Linux 环境中，像原生操作系统级窗口一样畅享各种 Windows 应用程序。</li>
<li><strong>完整的 Windows 桌面</strong>：在需要时访问完整的 Windows 桌面体验，或运行无缝集成到 Linux 工作流程中的单个应用程序</li>
<li><strong>文件系统集成</strong>：主目录安装在 Windows 中，允许在两个系统之间轻松共享文件，没有任何麻烦</li>
<li><strong>还有更多</strong>：智能卡直通、资源监控以及定期添加的更多功能</li>
</ul>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-17332fb737.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-17332fb737.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 18:37:46 +0800</pubDate>
  </item><item>
    <title><![CDATA[抖音与 LV-NUS 联合推出 SAIL-VL2 模型]]></title>
    <link>https://www.oschina.net/news/377404</link>
    <itunes:title><![CDATA[抖音与 LV-NUS 联合推出 SAIL-VL2 模型]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>抖音 SAIL 团队与 LV-NUS Lab 联手推出了一款名为 SAIL-VL2 的多模态大模型，并已开源。这个新模型在保持较小参数规模的同时，还在复杂推理任务中超过了许多同类模型，甚至能与更大型的闭源模型相抗衡。</p>
<p>SAIL-VL2的参数设置分为2B 和8B，在106个数据集上实现了性能的突破，尤其在 MMMU、MathVista 等复杂推理基准测试中表现优异。SAIL-VL2在数据、训练及架构设计上进行了三大方面的创新。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3cfa38b868.png"></p>
<p>在架构设计上，SAIL-VL2引入了稀疏混合专家（MoE），以优化性能和计算效率。其视觉编码器 SAIL-ViT 采用渐进式优化，逐步提升视觉 - 语言的对齐能力。这种创新设计使得 SAIL-VL2在推理时仅需激活部分参数，大幅度提升了模型的计算效率。</p>
<p>数据层面上，SAIL-VL2构建了高质量的多模态语料库，通过评分过滤和合成增强手段，确保数据的准确性和多样性。同时，团队还设计了一套渐进式的训练框架，从基础感知逐步过渡到复杂推理，使得模型在不同任务中的表现更加出色。</p>
<p>通过全链路优化，SAIL-VL2在基础模型的性能上取得了显著进展。数据显示，该模型在多项基准测试中脱颖而出，其8B 规模的模型在推理能力上，已然与<span>最新</span>的 GPT-4o 不相上下。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>抖音 SAIL 团队与 LV-NUS Lab 联手推出了一款名为 SAIL-VL2 的多模态大模型，并已开源。这个新模型在保持较小参数规模的同时，还在复杂推理任务中超过了许多同类模型，甚至能与更大型的闭源模型相抗衡。</p>
<p>SAIL-VL2的参数设置分为2B 和8B，在106个数据集上实现了性能的突破，尤其在 MMMU、MathVista 等复杂推理基准测试中表现优异。SAIL-VL2在数据、训练及架构设计上进行了三大方面的创新。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3cfa38b868.png"></p>
<p>在架构设计上，SAIL-VL2引入了稀疏混合专家（MoE），以优化性能和计算效率。其视觉编码器 SAIL-ViT 采用渐进式优化，逐步提升视觉 - 语言的对齐能力。这种创新设计使得 SAIL-VL2在推理时仅需激活部分参数，大幅度提升了模型的计算效率。</p>
<p>数据层面上，SAIL-VL2构建了高质量的多模态语料库，通过评分过滤和合成增强手段，确保数据的准确性和多样性。同时，团队还设计了一套渐进式的训练框架，从基础感知逐步过渡到复杂推理，使得模型在不同任务中的表现更加出色。</p>
<p>通过全链路优化，SAIL-VL2在基础模型的性能上取得了显著进展。数据显示，该模型在多项基准测试中脱颖而出，其8B 规模的模型在推理能力上，已然与<span>最新</span>的 GPT-4o 不相上下。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>抖音 SAIL 团队与 LV-NUS Lab 联手推出了一款名为 SAIL-VL2 的多模态大模型，并已开源。这个新模型在保持较小参数规模的同时，还在复杂推理任务中超过了许多同类模型，甚至能与更大型的闭源模型相抗衡。</p>
<p>SAIL-VL2的参数设置分为2B 和8B，在106个数据集上实现了性能的突破，尤其在 MMMU、MathVista 等复杂推理基准测试中表现优异。SAIL-VL2在数据、训练及架构设计上进行了三大方面的创新。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3cfa38b868.png"></p>
<p>在架构设计上，SAIL-VL2引入了稀疏混合专家（MoE），以优化性能和计算效率。其视觉编码器 SAIL-ViT 采用渐进式优化，逐步提升视觉 - 语言的对齐能力。这种创新设计使得 SAIL-VL2在推理时仅需激活部分参数，大幅度提升了模型的计算效率。</p>
<p>数据层面上，SAIL-VL2构建了高质量的多模态语料库，通过评分过滤和合成增强手段，确保数据的准确性和多样性。同时，团队还设计了一套渐进式的训练框架，从基础感知逐步过渡到复杂推理，使得模型在不同任务中的表现更加出色。</p>
<p>通过全链路优化，SAIL-VL2在基础模型的性能上取得了显著进展。数据显示，该模型在多项基准测试中脱颖而出，其8B 规模的模型在推理能力上，已然与<span>最新</span>的 GPT-4o 不相上下。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3cfa38b868.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3cfa38b868.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 18:13:21 +0800</pubDate>
  </item><item>
    <title><![CDATA[苹果发布全新 FS-DFM 模型，提升长文本生成效率]]></title>
    <link>https://www.oschina.net/news/377398</link>
    <itunes:title><![CDATA[苹果发布全新 FS-DFM 模型，提升长文本生成效率]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>苹果公司与俄亥俄州立大学研究团队联合发布了名为FS-DFM（Few-Step Discrete Flow-Matching）的全新语言模型。该模型在长文本生成方面实现了重大突破，通过三步法优化了迭代机制，使其在文本生成的困惑度和熵等关键指标上优于其他大型模型。</p>
<p><strong>生成速度大幅提升</strong></p>
<p>FS-DFM模型仅需<strong>8轮快速迭代</strong>即可生成高质量长文本，速度较传统扩散模型提升<strong>128倍</strong>，显著缩短了长文本生成的等待时间。</p>
<p><strong>文本质量保持领先</strong></p>
<p>在困惑度（衡量文本准确性和流畅性）和熵（衡量选词置信度）等关键指标上，FS-DFM的表现优于拥有数十亿参数的主流模型（如Dream-7B、LLaDA-8B），且参数量仅为1.7亿至17亿，实现了“小模型大效果”。</p>
<p><strong>技术创新与优化</strong></p>
<ul>
<li><strong>动态迭代预算</strong>：模型可根据任务需求自动调整迭代深度，避免冗余计算。</li>
<li><strong>教师指导机制</strong>：引入高精度“教师模型”引导迭代，确保更新精准且稳定。</li>
<li><strong>稳态收敛策略</strong>：优化迭代步长，加速模型收敛，减少步骤的同时保证质量。</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-65d4f10f40.png"></p>
<p>详情：<em>https://machinelearning.apple.com/research/fs-dfm</em></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>苹果公司与俄亥俄州立大学研究团队联合发布了名为FS-DFM（Few-Step Discrete Flow-Matching）的全新语言模型。该模型在长文本生成方面实现了重大突破，通过三步法优化了迭代机制，使其在文本生成的困惑度和熵等关键指标上优于其他大型模型。</p>
<p><strong>生成速度大幅提升</strong></p>
<p>FS-DFM模型仅需<strong>8轮快速迭代</strong>即可生成高质量长文本，速度较传统扩散模型提升<strong>128倍</strong>，显著缩短了长文本生成的等待时间。</p>
<p><strong>文本质量保持领先</strong></p>
<p>在困惑度（衡量文本准确性和流畅性）和熵（衡量选词置信度）等关键指标上，FS-DFM的表现优于拥有数十亿参数的主流模型（如Dream-7B、LLaDA-8B），且参数量仅为1.7亿至17亿，实现了“小模型大效果”。</p>
<p><strong>技术创新与优化</strong></p>
<ul>
<li><strong>动态迭代预算</strong>：模型可根据任务需求自动调整迭代深度，避免冗余计算。</li>
<li><strong>教师指导机制</strong>：引入高精度“教师模型”引导迭代，确保更新精准且稳定。</li>
<li><strong>稳态收敛策略</strong>：优化迭代步长，加速模型收敛，减少步骤的同时保证质量。</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-65d4f10f40.png"></p>
<p>详情：<em>https://machinelearning.apple.com/research/fs-dfm</em></p>]]>
    </description>
    <content:encoded><![CDATA[<p>苹果公司与俄亥俄州立大学研究团队联合发布了名为FS-DFM（Few-Step Discrete Flow-Matching）的全新语言模型。该模型在长文本生成方面实现了重大突破，通过三步法优化了迭代机制，使其在文本生成的困惑度和熵等关键指标上优于其他大型模型。</p>
<p><strong>生成速度大幅提升</strong></p>
<p>FS-DFM模型仅需<strong>8轮快速迭代</strong>即可生成高质量长文本，速度较传统扩散模型提升<strong>128倍</strong>，显著缩短了长文本生成的等待时间。</p>
<p><strong>文本质量保持领先</strong></p>
<p>在困惑度（衡量文本准确性和流畅性）和熵（衡量选词置信度）等关键指标上，FS-DFM的表现优于拥有数十亿参数的主流模型（如Dream-7B、LLaDA-8B），且参数量仅为1.7亿至17亿，实现了“小模型大效果”。</p>
<p><strong>技术创新与优化</strong></p>
<ul>
<li><strong>动态迭代预算</strong>：模型可根据任务需求自动调整迭代深度，避免冗余计算。</li>
<li><strong>教师指导机制</strong>：引入高精度“教师模型”引导迭代，确保更新精准且稳定。</li>
<li><strong>稳态收敛策略</strong>：优化迭代步长，加速模型收敛，减少步骤的同时保证质量。</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-65d4f10f40.png"></p>
<p>详情：<em>https://machinelearning.apple.com/research/fs-dfm</em></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-65d4f10f40.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-65d4f10f40.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 17:55:17 +0800</pubDate>
  </item><item>
    <title><![CDATA[马斯克收到黄仁勋亲手交付的 AI 超算 DGX Spark]]></title>
    <link>https://www.oschina.net/news/377397</link>
    <itunes:title><![CDATA[马斯克收到黄仁勋亲手交付的 AI 超算 DGX Spark]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>英伟达官方账号在X上发帖称：“为了庆祝DGX Spark从周三开始全球发货，我们的CEO黄仁勋今天在德克萨斯州的星舰基地将首批产品亲手交付给了SpaceX首席工程师埃隆·马斯克。</p>
<p>这次交流与这款新的桌面AI超算的起源——NVIDIA DGX-1超级计算机——有关，因为马斯克是2016年从黄仁勋那里收到的第一批DGX-1的用户之一。”</p>
<p>马斯克回应称：“这是DGX Spark，每瓦特计算能力比DGX-1多100倍。DGX-1是第一个专用的AI计算机，詹森2016年在OpenAI 交付给了我！”</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8c442baba.png"></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>英伟达官方账号在X上发帖称：“为了庆祝DGX Spark从周三开始全球发货，我们的CEO黄仁勋今天在德克萨斯州的星舰基地将首批产品亲手交付给了SpaceX首席工程师埃隆·马斯克。</p>
<p>这次交流与这款新的桌面AI超算的起源——NVIDIA DGX-1超级计算机——有关，因为马斯克是2016年从黄仁勋那里收到的第一批DGX-1的用户之一。”</p>
<p>马斯克回应称：“这是DGX Spark，每瓦特计算能力比DGX-1多100倍。DGX-1是第一个专用的AI计算机，詹森2016年在OpenAI 交付给了我！”</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8c442baba.png"></p>]]>
    </description>
    <content:encoded><![CDATA[<p>英伟达官方账号在X上发帖称：“为了庆祝DGX Spark从周三开始全球发货，我们的CEO黄仁勋今天在德克萨斯州的星舰基地将首批产品亲手交付给了SpaceX首席工程师埃隆·马斯克。</p>
<p>这次交流与这款新的桌面AI超算的起源——NVIDIA DGX-1超级计算机——有关，因为马斯克是2016年从黄仁勋那里收到的第一批DGX-1的用户之一。”</p>
<p>马斯克回应称：“这是DGX Spark，每瓦特计算能力比DGX-1多100倍。DGX-1是第一个专用的AI计算机，詹森2016年在OpenAI 交付给了我！”</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8c442baba.png"></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8c442baba.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8c442baba.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 17:51:54 +0800</pubDate>
  </item><item>
    <title><![CDATA[半年估值翻 3 倍，Cursor 冲刺 270 亿美元]]></title>
    <link>https://www.oschina.net/news/377377</link>
    <itunes:title><![CDATA[半年估值翻 3 倍，Cursor 冲刺 270 亿美元]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>据 The Information 报道，Coatue 和 Accel 正在与知名 AI 编码助手 Cursor 的母公司 Anysphere 商谈一笔至少10亿美元的融资，融资前估值高达270亿美元。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b405eeb0bc.png"></p>
<p>今年6月，Accel 以99亿美元估值参与了上一轮融资，短短几个月后估值几乎翻了三倍。过去两年，Accel 在 Scale AI、Cyera 等项目上都采取了类似的持续加仓策略，精准卡位 AI 基础设施赛道。</p>
<p>尽管 Anysphere 账面上仍有约8亿美元现金储备，但随着其自研大型语言模型驱动 Cursor 核心功能，训练与推理的成本持续攀升，这笔融资几乎是必然选择。</p>
<p>Anysphere 的崛起堪称"教科书式"。公司完成40万美元种子轮后，于2022年4月启动 Cursor 开发。2023年3月，Cursor 上线公测——一款基于 VS Code 打造、支持自然语言编程的新型 IDE。到2023年底，四人团队创造的 ARR 已突破100万美元，日活用户超3万。2024年11月收购代码补全工具 Supermaven 后，模型能力进一步增强。</p>
<p>Cursor 的增长速度堪称离谱。自2023年3月推出以来，仅20个月 ARR 就突破1亿美元，几乎零营销投入。2025年初势能彻底引爆:2月有4000至5000家公司请求接入，3月 ARR 达3亿美元、付费用户超36万，4月日活突破100万、企业客户达1.4万。截至6月，ARR 已超5亿美元。据彭博社报道，预计年底将达10亿美元。Cursor 已成为史上增长最快的 AI SaaS 产品之一，并登上《Fast Company》"全球最具创新力公司"榜单第26位。</p>
<p>Cursor 不只是"自动补全神器"，而是正在变成工程团队的"主力开发环境"。与 GitHub Copilot 等竞品相比，它在上下文理解、可解释性与隐私部署上优势明显。开发者可依赖它完成从代码编写、调试到架构设计的全流程——一句"帮我重构这段逻辑"，就能自动完成优化到文档生成。</p>
<p>Cursor CEO Michael Truell 称："编程将会消失，未来的 IDE 不再是工具，而是一个会写、会跑、会自我优化的智能体。"</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>据 The Information 报道，Coatue 和 Accel 正在与知名 AI 编码助手 Cursor 的母公司 Anysphere 商谈一笔至少10亿美元的融资，融资前估值高达270亿美元。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b405eeb0bc.png"></p>
<p>今年6月，Accel 以99亿美元估值参与了上一轮融资，短短几个月后估值几乎翻了三倍。过去两年，Accel 在 Scale AI、Cyera 等项目上都采取了类似的持续加仓策略，精准卡位 AI 基础设施赛道。</p>
<p>尽管 Anysphere 账面上仍有约8亿美元现金储备，但随着其自研大型语言模型驱动 Cursor 核心功能，训练与推理的成本持续攀升，这笔融资几乎是必然选择。</p>
<p>Anysphere 的崛起堪称"教科书式"。公司完成40万美元种子轮后，于2022年4月启动 Cursor 开发。2023年3月，Cursor 上线公测——一款基于 VS Code 打造、支持自然语言编程的新型 IDE。到2023年底，四人团队创造的 ARR 已突破100万美元，日活用户超3万。2024年11月收购代码补全工具 Supermaven 后，模型能力进一步增强。</p>
<p>Cursor 的增长速度堪称离谱。自2023年3月推出以来，仅20个月 ARR 就突破1亿美元，几乎零营销投入。2025年初势能彻底引爆:2月有4000至5000家公司请求接入，3月 ARR 达3亿美元、付费用户超36万，4月日活突破100万、企业客户达1.4万。截至6月，ARR 已超5亿美元。据彭博社报道，预计年底将达10亿美元。Cursor 已成为史上增长最快的 AI SaaS 产品之一，并登上《Fast Company》"全球最具创新力公司"榜单第26位。</p>
<p>Cursor 不只是"自动补全神器"，而是正在变成工程团队的"主力开发环境"。与 GitHub Copilot 等竞品相比，它在上下文理解、可解释性与隐私部署上优势明显。开发者可依赖它完成从代码编写、调试到架构设计的全流程——一句"帮我重构这段逻辑"，就能自动完成优化到文档生成。</p>
<p>Cursor CEO Michael Truell 称："编程将会消失，未来的 IDE 不再是工具，而是一个会写、会跑、会自我优化的智能体。"</p>]]>
    </description>
    <content:encoded><![CDATA[<p>据 The Information 报道，Coatue 和 Accel 正在与知名 AI 编码助手 Cursor 的母公司 Anysphere 商谈一笔至少10亿美元的融资，融资前估值高达270亿美元。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b405eeb0bc.png"></p>
<p>今年6月，Accel 以99亿美元估值参与了上一轮融资，短短几个月后估值几乎翻了三倍。过去两年，Accel 在 Scale AI、Cyera 等项目上都采取了类似的持续加仓策略，精准卡位 AI 基础设施赛道。</p>
<p>尽管 Anysphere 账面上仍有约8亿美元现金储备，但随着其自研大型语言模型驱动 Cursor 核心功能，训练与推理的成本持续攀升，这笔融资几乎是必然选择。</p>
<p>Anysphere 的崛起堪称"教科书式"。公司完成40万美元种子轮后，于2022年4月启动 Cursor 开发。2023年3月，Cursor 上线公测——一款基于 VS Code 打造、支持自然语言编程的新型 IDE。到2023年底，四人团队创造的 ARR 已突破100万美元，日活用户超3万。2024年11月收购代码补全工具 Supermaven 后，模型能力进一步增强。</p>
<p>Cursor 的增长速度堪称离谱。自2023年3月推出以来，仅20个月 ARR 就突破1亿美元，几乎零营销投入。2025年初势能彻底引爆:2月有4000至5000家公司请求接入，3月 ARR 达3亿美元、付费用户超36万，4月日活突破100万、企业客户达1.4万。截至6月，ARR 已超5亿美元。据彭博社报道，预计年底将达10亿美元。Cursor 已成为史上增长最快的 AI SaaS 产品之一，并登上《Fast Company》"全球最具创新力公司"榜单第26位。</p>
<p>Cursor 不只是"自动补全神器"，而是正在变成工程团队的"主力开发环境"。与 GitHub Copilot 等竞品相比，它在上下文理解、可解释性与隐私部署上优势明显。开发者可依赖它完成从代码编写、调试到架构设计的全流程——一句"帮我重构这段逻辑"，就能自动完成优化到文档生成。</p>
<p>Cursor CEO Michael Truell 称："编程将会消失，未来的 IDE 不再是工具，而是一个会写、会跑、会自我优化的智能体。"</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b405eeb0bc.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b405eeb0bc.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 16:28:29 +0800</pubDate>
  </item><item>
    <title><![CDATA[全球 AI 投资形成“史上最大科技泡沫”：规模是互联网泡沫的 17 倍]]></title>
    <link>https://www.oschina.net/news/377369</link>
    <itunes:title><![CDATA[全球 AI 投资形成“史上最大科技泡沫”：规模是互联网泡沫的 17 倍]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.marketwatch.com%2Fstory%2Fthe-ai-bubble-is-17-times-the-size-of-the-dot-com-frenzy-this-analyst-argues-046e7c5c" target="_blank">据 MarketWatch 报道</a>，独立研究机构 MacroStrategy Partnership 在最新报告中警告，当前全球人工智能投资热潮已形成“史上最大科技泡沫”——其规模被估算为<strong>上世纪互联网泡沫的 17 倍，也是 2008 年次贷危机的 4 倍</strong>。</p>
<p><span>他们使用古典／威克塞尔 (Wicksellian) 资本理论来支撑评估：</span></p>
<ul>
<li> <p>在理想状态下，资本应在借贷成本比名义 GDP 高 2 个百分点时才能视为“均衡配置”。</p> </li>
<li> <p>但是由于极度宽松的利率环境导致资金误配，资本被投入到高风险、边际效益递减的 AI 领域。</p> </li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7e26977d36.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-e8903ede47.png"></p>
<p>分析师 Julien Garran 指出，宽松的利率环境使资本大量涌入 AI 领域，资金被误配到成本高、回报不确定的项目中。他认为 AI 模型正遭遇“边际收益递减”：训练成本快速上升，但性能提升有限；同时模型间差异化不强，缺乏可持续的商业护城河。</p>
<p>报告称，新一代语言模型（LLM）训练成本增大、性能提升幅度收窄，比如 GPT-3 相比 GPT-4 的训练费用增长很大，但性能改善并不成比例。AI 公司的模型并不容易形成差异化壁垒（其他竞争者可能复制或赶超），难以对客户收取高额溢价。</p>
<p>报告还指出，AI 公司普遍面临盈利模式不清、成本过高的问题，许多项目尚未形成稳定营收。虽然 AI 应用与模型研发备受追捧，但很多公司还未建立稳定、可扩展的营收体系，支出很重、回报尚未显现。</p>
<p>MacroStrategy 预测，随着 AI 投资与数据中心支出见顶，全球经济可能面临通缩性调整。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.marketwatch.com%2Fstory%2Fthe-ai-bubble-is-17-times-the-size-of-the-dot-com-frenzy-this-analyst-argues-046e7c5c" target="_blank">据 MarketWatch 报道</a>，独立研究机构 MacroStrategy Partnership 在最新报告中警告，当前全球人工智能投资热潮已形成“史上最大科技泡沫”——其规模被估算为<strong>上世纪互联网泡沫的 17 倍，也是 2008 年次贷危机的 4 倍</strong>。</p>
<p><span>他们使用古典／威克塞尔 (Wicksellian) 资本理论来支撑评估：</span></p>
<ul>
<li> <p>在理想状态下，资本应在借贷成本比名义 GDP 高 2 个百分点时才能视为“均衡配置”。</p> </li>
<li> <p>但是由于极度宽松的利率环境导致资金误配，资本被投入到高风险、边际效益递减的 AI 领域。</p> </li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7e26977d36.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-e8903ede47.png"></p>
<p>分析师 Julien Garran 指出，宽松的利率环境使资本大量涌入 AI 领域，资金被误配到成本高、回报不确定的项目中。他认为 AI 模型正遭遇“边际收益递减”：训练成本快速上升，但性能提升有限；同时模型间差异化不强，缺乏可持续的商业护城河。</p>
<p>报告称，新一代语言模型（LLM）训练成本增大、性能提升幅度收窄，比如 GPT-3 相比 GPT-4 的训练费用增长很大，但性能改善并不成比例。AI 公司的模型并不容易形成差异化壁垒（其他竞争者可能复制或赶超），难以对客户收取高额溢价。</p>
<p>报告还指出，AI 公司普遍面临盈利模式不清、成本过高的问题，许多项目尚未形成稳定营收。虽然 AI 应用与模型研发备受追捧，但很多公司还未建立稳定、可扩展的营收体系，支出很重、回报尚未显现。</p>
<p>MacroStrategy 预测，随着 AI 投资与数据中心支出见顶，全球经济可能面临通缩性调整。</p>]]>
    </description>
    <content:encoded><![CDATA[<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.marketwatch.com%2Fstory%2Fthe-ai-bubble-is-17-times-the-size-of-the-dot-com-frenzy-this-analyst-argues-046e7c5c" target="_blank">据 MarketWatch 报道</a>，独立研究机构 MacroStrategy Partnership 在最新报告中警告，当前全球人工智能投资热潮已形成“史上最大科技泡沫”——其规模被估算为<strong>上世纪互联网泡沫的 17 倍，也是 2008 年次贷危机的 4 倍</strong>。</p>
<p><span>他们使用古典／威克塞尔 (Wicksellian) 资本理论来支撑评估：</span></p>
<ul>
<li> <p>在理想状态下，资本应在借贷成本比名义 GDP 高 2 个百分点时才能视为“均衡配置”。</p> </li>
<li> <p>但是由于极度宽松的利率环境导致资金误配，资本被投入到高风险、边际效益递减的 AI 领域。</p> </li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7e26977d36.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-e8903ede47.png"></p>
<p>分析师 Julien Garran 指出，宽松的利率环境使资本大量涌入 AI 领域，资金被误配到成本高、回报不确定的项目中。他认为 AI 模型正遭遇“边际收益递减”：训练成本快速上升，但性能提升有限；同时模型间差异化不强，缺乏可持续的商业护城河。</p>
<p>报告称，新一代语言模型（LLM）训练成本增大、性能提升幅度收窄，比如 GPT-3 相比 GPT-4 的训练费用增长很大，但性能改善并不成比例。AI 公司的模型并不容易形成差异化壁垒（其他竞争者可能复制或赶超），难以对客户收取高额溢价。</p>
<p>报告还指出，AI 公司普遍面临盈利模式不清、成本过高的问题，许多项目尚未形成稳定营收。虽然 AI 应用与模型研发备受追捧，但很多公司还未建立稳定、可扩展的营收体系，支出很重、回报尚未显现。</p>
<p>MacroStrategy 预测，随着 AI 投资与数据中心支出见顶，全球经济可能面临通缩性调整。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7e26977d36.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7e26977d36.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 15:46:45 +0800</pubDate>
  </item><item>
    <title><![CDATA[腾讯启动青云奖学金，面向 AI 领域硕博生提供价值 50 万元支持]]></title>
    <link>https://www.oschina.net/news/377363</link>
    <itunes:title><![CDATA[腾讯启动青云奖学金，面向 AI 领域硕博生提供价值 50 万元支持]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><span>腾讯<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCTMCNkk9OwBsPSpkNht37A" target="_blank">宣布</a>全面启动青云奖学金。该项目重点关注人工智能领域的基础研究与应用创新，针对中国大陆及港澳台地区院校就读、具有中国国籍的硕士或博士生，希望申请者来自计算机科学、人工智能及其交叉领域，并拥有前瞻性科研视野。项目首期预计评选出15位获奖者，每位获奖者将获得总价值50万元人民币的现金及算力资源支持。</span></p>
<p><span>具体包括：20万元现金奖励，用于支持获奖者的科研活动和个人发展；价值30万元的云异构算力资源。腾讯方面表示，30万元大约可以支持3个月前沿GPU实例24小时不间断使用。除这些奖励外，15名获奖者还将有机会进入腾讯实习或就业。</span></p>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-58731a2eb0.png"></span></p>
<p><span>腾讯招聘全球负责人罗海波表示，腾讯青云奖学金不仅是一份经济上的支持，也是一个连接学术界与产业界的平台。腾讯希望通过这个计划，为青年学者提供更多资源和机会。</span></p>
<p><span>2024年，腾讯研发投入达707亿元，自2018年至今研发投入超过3795亿元。目前，腾讯科技类人才占比已达73%。</span></p>
<p><span>该公司此前曾推出面向全球顶尖技术学子的人才专项“青云计划”。该专项通过提供定制化培养方案、开放核心业务工作机会、解锁前瞻性技术课题和极具竞争力的薪酬，培养未来的科技人才。其招募对象是2024年1月-2026年12月毕业的博士生，2025年1月-2026年12月毕业的本硕生，以及2025年9月以后毕业的本硕和博士生。</span></p>
<p><span>“青云计划”提供双导师制，有专属岗位导师、发展导师和项目顾问提供日常实时业务辅导、定期成长辅导和跨界科研项目交流。还包括线下集中学习、定制化精品内容授课、顶尖实验室参访等定制化培养方案。工作地点包括深圳、北京、上海和西雅图等全球20多个城市。</span></p>
<p><span>在对顶尖人才的招揽过程中，大厂对AI相关人才的争抢颇为激烈。今年6月，腾讯也曾举办算法大赛，提供百万现金奖池，其中冠军团可获得200万奖金；闯入十强，即可获得腾讯核心业务的直通offer。</span></p>
<p><span>5月，京东推出“顶尖青年技术天才计划”，面向全球高校本硕博毕业生及毕业两年内的技术人才开放招募，薪酬不设上限，研究方向涵盖多模态大模型与应用、AI Infra等方向。</span></p>
<p><span>阿里在3到4月也曾开启大规模AI人才校园招聘，主要面向清华、北大、浙大、麻省理工、斯坦福等全球顶尖高校，招募大语言模型等领域技术人才。其头部AI科技人才培养计划——“Bravo 102”，打破了传统的校招体系，面试通过后可反选项目和团队。</span></p>
<p><span>脉脉发布的《2025年AI人才流动报告》显示，今年1-7月，AI新发岗位量同比增长超10倍，简历投递量也暴涨了11倍。但技术人才整体供不应求，平均每5个AI岗位只有2个人才竞争，尤其是搜索算法人才，供需比仅为0.39。</span></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><span>腾讯<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCTMCNkk9OwBsPSpkNht37A" target="_blank">宣布</a>全面启动青云奖学金。该项目重点关注人工智能领域的基础研究与应用创新，针对中国大陆及港澳台地区院校就读、具有中国国籍的硕士或博士生，希望申请者来自计算机科学、人工智能及其交叉领域，并拥有前瞻性科研视野。项目首期预计评选出15位获奖者，每位获奖者将获得总价值50万元人民币的现金及算力资源支持。</span></p>
<p><span>具体包括：20万元现金奖励，用于支持获奖者的科研活动和个人发展；价值30万元的云异构算力资源。腾讯方面表示，30万元大约可以支持3个月前沿GPU实例24小时不间断使用。除这些奖励外，15名获奖者还将有机会进入腾讯实习或就业。</span></p>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-58731a2eb0.png"></span></p>
<p><span>腾讯招聘全球负责人罗海波表示，腾讯青云奖学金不仅是一份经济上的支持，也是一个连接学术界与产业界的平台。腾讯希望通过这个计划，为青年学者提供更多资源和机会。</span></p>
<p><span>2024年，腾讯研发投入达707亿元，自2018年至今研发投入超过3795亿元。目前，腾讯科技类人才占比已达73%。</span></p>
<p><span>该公司此前曾推出面向全球顶尖技术学子的人才专项“青云计划”。该专项通过提供定制化培养方案、开放核心业务工作机会、解锁前瞻性技术课题和极具竞争力的薪酬，培养未来的科技人才。其招募对象是2024年1月-2026年12月毕业的博士生，2025年1月-2026年12月毕业的本硕生，以及2025年9月以后毕业的本硕和博士生。</span></p>
<p><span>“青云计划”提供双导师制，有专属岗位导师、发展导师和项目顾问提供日常实时业务辅导、定期成长辅导和跨界科研项目交流。还包括线下集中学习、定制化精品内容授课、顶尖实验室参访等定制化培养方案。工作地点包括深圳、北京、上海和西雅图等全球20多个城市。</span></p>
<p><span>在对顶尖人才的招揽过程中，大厂对AI相关人才的争抢颇为激烈。今年6月，腾讯也曾举办算法大赛，提供百万现金奖池，其中冠军团可获得200万奖金；闯入十强，即可获得腾讯核心业务的直通offer。</span></p>
<p><span>5月，京东推出“顶尖青年技术天才计划”，面向全球高校本硕博毕业生及毕业两年内的技术人才开放招募，薪酬不设上限，研究方向涵盖多模态大模型与应用、AI Infra等方向。</span></p>
<p><span>阿里在3到4月也曾开启大规模AI人才校园招聘，主要面向清华、北大、浙大、麻省理工、斯坦福等全球顶尖高校，招募大语言模型等领域技术人才。其头部AI科技人才培养计划——“Bravo 102”，打破了传统的校招体系，面试通过后可反选项目和团队。</span></p>
<p><span>脉脉发布的《2025年AI人才流动报告》显示，今年1-7月，AI新发岗位量同比增长超10倍，简历投递量也暴涨了11倍。但技术人才整体供不应求，平均每5个AI岗位只有2个人才竞争，尤其是搜索算法人才，供需比仅为0.39。</span></p>]]>
    </description>
    <content:encoded><![CDATA[<p><span>腾讯<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FCTMCNkk9OwBsPSpkNht37A" target="_blank">宣布</a>全面启动青云奖学金。该项目重点关注人工智能领域的基础研究与应用创新，针对中国大陆及港澳台地区院校就读、具有中国国籍的硕士或博士生，希望申请者来自计算机科学、人工智能及其交叉领域，并拥有前瞻性科研视野。项目首期预计评选出15位获奖者，每位获奖者将获得总价值50万元人民币的现金及算力资源支持。</span></p>
<p><span>具体包括：20万元现金奖励，用于支持获奖者的科研活动和个人发展；价值30万元的云异构算力资源。腾讯方面表示，30万元大约可以支持3个月前沿GPU实例24小时不间断使用。除这些奖励外，15名获奖者还将有机会进入腾讯实习或就业。</span></p>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-58731a2eb0.png"></span></p>
<p><span>腾讯招聘全球负责人罗海波表示，腾讯青云奖学金不仅是一份经济上的支持，也是一个连接学术界与产业界的平台。腾讯希望通过这个计划，为青年学者提供更多资源和机会。</span></p>
<p><span>2024年，腾讯研发投入达707亿元，自2018年至今研发投入超过3795亿元。目前，腾讯科技类人才占比已达73%。</span></p>
<p><span>该公司此前曾推出面向全球顶尖技术学子的人才专项“青云计划”。该专项通过提供定制化培养方案、开放核心业务工作机会、解锁前瞻性技术课题和极具竞争力的薪酬，培养未来的科技人才。其招募对象是2024年1月-2026年12月毕业的博士生，2025年1月-2026年12月毕业的本硕生，以及2025年9月以后毕业的本硕和博士生。</span></p>
<p><span>“青云计划”提供双导师制，有专属岗位导师、发展导师和项目顾问提供日常实时业务辅导、定期成长辅导和跨界科研项目交流。还包括线下集中学习、定制化精品内容授课、顶尖实验室参访等定制化培养方案。工作地点包括深圳、北京、上海和西雅图等全球20多个城市。</span></p>
<p><span>在对顶尖人才的招揽过程中，大厂对AI相关人才的争抢颇为激烈。今年6月，腾讯也曾举办算法大赛，提供百万现金奖池，其中冠军团可获得200万奖金；闯入十强，即可获得腾讯核心业务的直通offer。</span></p>
<p><span>5月，京东推出“顶尖青年技术天才计划”，面向全球高校本硕博毕业生及毕业两年内的技术人才开放招募，薪酬不设上限，研究方向涵盖多模态大模型与应用、AI Infra等方向。</span></p>
<p><span>阿里在3到4月也曾开启大规模AI人才校园招聘，主要面向清华、北大、浙大、麻省理工、斯坦福等全球顶尖高校，招募大语言模型等领域技术人才。其头部AI科技人才培养计划——“Bravo 102”，打破了传统的校招体系，面试通过后可反选项目和团队。</span></p>
<p><span>脉脉发布的《2025年AI人才流动报告》显示，今年1-7月，AI新发岗位量同比增长超10倍，简历投递量也暴涨了11倍。但技术人才整体供不应求，平均每5个AI岗位只有2个人才竞争，尤其是搜索算法人才，供需比仅为0.39。</span></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-58731a2eb0.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-58731a2eb0.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 15:31:33 +0800</pubDate>
  </item><item>
    <title><![CDATA[腾讯优图实验室开源 Youtu-Embedding]]></title>
    <link>https://www.oschina.net/news/377360</link>
    <itunes:title><![CDATA[腾讯优图实验室开源 Youtu-Embedding]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><span>腾讯优图实验室正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F5iZEv9TewfQrkEyebbhq-g" target="_blank">开源</a>&nbsp;Youtu-Embedding，这是一款面向企业级应用打造的通用文本表示模型，可同时胜任文本检索、意图理解、相似度判断、分类聚类等六大主流任务。在信息检索（IR）、语义相似度（STS）、聚类、重排序和分类等一系列广泛的自然语言处理任务上，均展现出卓越的性能。</span></p>
<p><span>模型权重、推理代码及完整的训练框架现已开源，首个模型版本已在HuggingFace上发布，这是一个拥有20亿（2B）参数的通用语义表示模型。</span></p>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e5cba0cfc.png"></span></p>
<p><span>根据介绍，Youtu-Embedding的核心优势包括：</span></p>
<ul>
<li><span>在中文文本嵌入评测基准 CMTEB 上，Youtu-Embedding&nbsp;以 77.46 的高分荣登榜首（截至2025年09月）</span></li>
</ul>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a7284db496.png"></span></p>
<ul>
<li><span>精密的三阶段训练：通过“LLM基础预训练 → 弱监督对齐 → 协同-判别式微调”的训练流程，系统性地将大模型的广博知识转化为专用于嵌入任务的判别能力。</span></li>
<li><span>创新的微调框架：设计了统一数据格式、任务差异化损失函数和动态单任务采样机制，解决了多任务学习中的“负迁移”难题，实现了多任务的稳定协同训练。（该框架在多种基础编码器上进行了验证，保障其通用性和有效性）</span></li>
<li><span>精细化的数据工程：结合了基于LLM的高质量数据合成技术与高效的难负例挖掘策略，为模型训练提供了最坚实的数据基础。</span></li>
</ul>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><span>腾讯优图实验室正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F5iZEv9TewfQrkEyebbhq-g" target="_blank">开源</a>&nbsp;Youtu-Embedding，这是一款面向企业级应用打造的通用文本表示模型，可同时胜任文本检索、意图理解、相似度判断、分类聚类等六大主流任务。在信息检索（IR）、语义相似度（STS）、聚类、重排序和分类等一系列广泛的自然语言处理任务上，均展现出卓越的性能。</span></p>
<p><span>模型权重、推理代码及完整的训练框架现已开源，首个模型版本已在HuggingFace上发布，这是一个拥有20亿（2B）参数的通用语义表示模型。</span></p>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e5cba0cfc.png"></span></p>
<p><span>根据介绍，Youtu-Embedding的核心优势包括：</span></p>
<ul>
<li><span>在中文文本嵌入评测基准 CMTEB 上，Youtu-Embedding&nbsp;以 77.46 的高分荣登榜首（截至2025年09月）</span></li>
</ul>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a7284db496.png"></span></p>
<ul>
<li><span>精密的三阶段训练：通过“LLM基础预训练 → 弱监督对齐 → 协同-判别式微调”的训练流程，系统性地将大模型的广博知识转化为专用于嵌入任务的判别能力。</span></li>
<li><span>创新的微调框架：设计了统一数据格式、任务差异化损失函数和动态单任务采样机制，解决了多任务学习中的“负迁移”难题，实现了多任务的稳定协同训练。（该框架在多种基础编码器上进行了验证，保障其通用性和有效性）</span></li>
<li><span>精细化的数据工程：结合了基于LLM的高质量数据合成技术与高效的难负例挖掘策略，为模型训练提供了最坚实的数据基础。</span></li>
</ul>]]>
    </description>
    <content:encoded><![CDATA[<p><span>腾讯优图实验室正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F5iZEv9TewfQrkEyebbhq-g" target="_blank">开源</a>&nbsp;Youtu-Embedding，这是一款面向企业级应用打造的通用文本表示模型，可同时胜任文本检索、意图理解、相似度判断、分类聚类等六大主流任务。在信息检索（IR）、语义相似度（STS）、聚类、重排序和分类等一系列广泛的自然语言处理任务上，均展现出卓越的性能。</span></p>
<p><span>模型权重、推理代码及完整的训练框架现已开源，首个模型版本已在HuggingFace上发布，这是一个拥有20亿（2B）参数的通用语义表示模型。</span></p>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e5cba0cfc.png"></span></p>
<p><span>根据介绍，Youtu-Embedding的核心优势包括：</span></p>
<ul>
<li><span>在中文文本嵌入评测基准 CMTEB 上，Youtu-Embedding&nbsp;以 77.46 的高分荣登榜首（截至2025年09月）</span></li>
</ul>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a7284db496.png"></span></p>
<ul>
<li><span>精密的三阶段训练：通过“LLM基础预训练 → 弱监督对齐 → 协同-判别式微调”的训练流程，系统性地将大模型的广博知识转化为专用于嵌入任务的判别能力。</span></li>
<li><span>创新的微调框架：设计了统一数据格式、任务差异化损失函数和动态单任务采样机制，解决了多任务学习中的“负迁移”难题，实现了多任务的稳定协同训练。（该框架在多种基础编码器上进行了验证，保障其通用性和有效性）</span></li>
<li><span>精细化的数据工程：结合了基于LLM的高质量数据合成技术与高效的难负例挖掘策略，为模型训练提供了最坚实的数据基础。</span></li>
</ul>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e5cba0cfc.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e5cba0cfc.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 15:12:22 +0800</pubDate>
  </item><item>
    <title><![CDATA[线程池 ThreadPoolExecutor 源码深度解析]]></title>
    <link>https://my.oschina.net/u/5783135/blog/18695444</link>
    <itunes:title><![CDATA[线程池 ThreadPoolExecutor 源码深度解析]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<h1>一、引 言</h1>
<p><strong>为什么进行源码角度的深度解析？</strong></p>
<p>大家在项目中到处都在使用线程池做一些性能接口层次的优化，原先串行的多个远程调用，因为rt过高，通过线程池批量异步优化，从而降低rt。还有像RocketMQ中broker启动时，同时通过ScheduledThreadPoolExecutor线程池执行其他组件的定时任务，每隔一段时间处理相关的任务。线程池广泛的应用在外面各种实际开发场景中，我们很多同学可能在项目里只是简单的copy了一些前人的代码参数并不知道其中的含义，从而导致生产级别的bug。所以本篇文章，旨在帮助还不熟悉或者想要熟悉线程池的同学，分享我自己在学习线程池源码上的一些内容来更简单、快速的掌握线程池。</p>
<h1>二、为什么使用线程池？</h1>
<p>并发编程中，对于常见的操作系统，线程都是执行任务的基本单元，如果每次执行任务时都创建新的线程，任务执行完毕又进行销毁，会出现以下的问题：</p>
<ul>
<li><strong>资源开销</strong>：比如在Linux系统中，频繁的创建和销毁线程，一个是频繁的进行一个系统调用，另外是一些内存和CPU资源调度的占用。虽然有一些写时复制的策略防止lwp的创建时的内存占用，但是实际写入还是会申请系统内存的，何况一些页表等本身就有内存占用。</li>
<li><strong>性能瓶颈</strong>：线程的创建需要系统调用，如果只是简单的计算任务，可能耗时还没创建的rt高，这里反而降低了系统的吞吐量。</li>
<li><strong>缺乏资源管理</strong>：无限制的创建线程会导致内存溢出，java.lang.OutOfMemoryError: unable to create native thread，这里主要因为Java的线程其实Linux中是lwp线程，需要通过JNI进行系统调用创建，每个线程默认需要1MB的栈空间，很容易导致无休止的创建线程导致内存溢出，另外就是频繁的系统调用，导致的上下文切换，占用了过多的CPU，反而起到了相反的作用。</li>
<li><strong>功能受限</strong>：手动管理线程难以实现更高级的功能，如定时任务、周期任务、任务管理、并发任务数的控制等。</li>
</ul>
<p>通过上面的问题，我们其实可以清晰的感知到这些问题都是归拢到资源没有得到合理的分配和控制导致的，线程池出现的核心宗旨其实就是对资源的合理分配和控制。除了线程池，其实更多的也接触过数据库连接池、netty的对象池等池化技术，这些池化思想其实都是为了更好的降低资源的消耗以及更好的进行资源管理。</p>
<h1>三、JDK线程池的架构设计</h1>
<h1>3.1 JUC并发包下的Executor框架的uml类图</h1>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-10e0261f0e.jpg">
</div>
<ul>
<li><strong>Executor</strong>：任务执行的顶层接口，主要是分离任务提交与执行逻辑，支持同步/异步执行，遵循Java内存模型的&nbsp;happen-before规则。</li>
<li><strong>ExecutorService</strong>：继承Executor接口，提供了更完善的生命周期管理能力，通过Future对象提供任务取消、状态查询、结果获取能力实现了任务监控。</li>
<li><strong>AbstractExecutorService</strong>：常见的设计模式为了简化线程池的开发，通常通过父类进行一些基础的默认实现让子类继承。</li>
<li><strong>ScheduledExecutorService</strong>：ExecutorService的扩展接口，支持延迟执行和周期性任务调度。</li>
<li><strong>ThreadPoolExecutor</strong>：是ExecutorService接口最核心和最常用的实现类，它提供了高度可配置的线程池，允许我们精细控制线程池的各种行为。</li>
<li><strong>ScheduledThreadPoolExecutor</strong>：是ScheduledExecutorService接口的实现类，它继承自ThreadPoolExecutor，专门用于处理定时和周期任务。</li>
<li><strong>Executors</strong>：一个静态工厂模式的工具类，提供了一系列静态方法来创建各种常见配置的线程池，newFixedThreadPool(), newCachedThreadPool(),等，简化了创建线程池的使用但是会带来一些问题，很多开发规范里都不建议大家直接使用。JDK内置的线程池如果我们不熟悉里面的参数很有可能导致出乎自己意料的结果，池大小设置、阻塞队列选择等等都是有考究的，这一点后续会进行一些详细说明。生产环境中建议谨慎使用或直接使用ThreadPoolExecutor构造函数自定义。</li>
</ul>
<h1>3.2 ThreadPoolExecutor的参数解析</h1>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2b578a85a8.jpg">
</div>
<ul>
<li><strong>corePoolSize&nbsp;</strong>核心线程数：
<ul>
<li>线程池中还未退出的alive的核心线程数量。</li>
<li>虽然线程处于空闲状态（其实是阻塞在阻塞队列中），除非显示设置了allowCoreThreadTimeOut=true，否则这些线程不会从自己的run方法中退出被回收。</li>
<li>添加新任务时，如果当前工作线程小于coreSize，此时即使存在空闲的core线程，线程池也会通过addWorker方法创建一个新的线程。</li>
<li>&nbsp;</li>
</ul> </li>
<li><strong>maximumPoolSize&nbsp;</strong>最大线程数：
<ul>
<li>线程池可以创建的最大线程数。</li>
<li>如果是有界队列，当队列满时，仍然有任务进来，此时线程池会创建小于最大线程数的线程来完成任务，空闲。</li>
<li>如果是无界队列，那么永远不会出现第二点的情况，除了内存异常，否则会一直保持核心线程数，多余的任务会一直往队列中加入。</li>
</ul> </li>
</ul>
<ul>
<li><strong>keepAliveTime</strong>&nbsp;线程空闲存活时间
<ul>
<li>线程数超过corePoolSize后创建的线程我们理解为非核心线程，对于这类线程，他的回收机制在于我们设置的keepAliveTime,线程会限期阻塞在队列中获取任务，如果超时未获取就会进行清理并退出。</li>
<li>另外如果设置allowCoreThreadTimeOut=true，所谓的核心线程在空闲时间达到keepAliveTime时也会被回收。</li>
</ul> </li>
</ul>
<ul>
<li><strong>unit&nbsp;</strong>时间单位
<ul>
<li>keepAliveTime参数的时间单位，TimeUnit中时分秒等。</li>
<li>&nbsp;</li>
</ul> </li>
<li><strong>workQueue&nbsp;</strong>任务队列
<ul>
<li>阻塞队列，核心线程数满时，新加入的任务，会先添加到阻塞队列中等待线程获取任务并执行。</li>
<li>常用的BlockingQueue实现有：
<ul>
<li>ArrayBlockingQueue：数组实现的先进先出原则的有界阻塞队列，构造方法必须指定容量。</li>
<li>LinkedBlockingQueue：链表实现的阻塞队列，构造传入容量则有界，未传则是无界队列，此时设置的最大线程数其实就不会有作用了。</li>
<li>SynchronousQueue：一个不存储元素的阻塞队列。每个put操作必须等待一个take操作，反之亦然。它相当于一个传递通道，非常适合传递性需求，吞吐量高，但要求maximumPoolSize足够大。</li>
<li>PriorityBlockingQueue：二叉堆实现的优先级阻塞队列，构造时可自行调整排序行为（小顶堆或大顶堆）。</li>
<li>DelayQueue：支持延时的无界阻塞队列，主要用于周期性的任务，我们可以直接通过它来实现一些简单的延迟任务需求，复杂的周期性任务建议使用ScheduledThreadPoolExecutor。</li>
</ul> </li>
</ul> </li>
</ul>
<ul>
<li><strong>threadFactory&nbsp;</strong>线程工厂
<ul>
<li>用于创建新线程的工厂。通过自定义ThreadFactory，我们可以为线程池中的线程设置更有意义的名称、设置守护线程状态、设置线程优先级、指定UncaughtExceptionHandler等。</li>
<li>Executors.defaultThreadFactory()是默认实现。</li>
</ul> </li>
</ul>
<ul>
<li><strong>handler&nbsp;</strong>拒绝策略
<ul>
<li>ThreadPoolExecutor.AbortPolicy：默认的拒绝策略，简单粗暴，当execute中添加woker失败时，直接在当前线程抛出异常。</li>
<li>JDK内置了四种拒绝策略：
<ul>
<li>ThreadPoolExecutor.AbortPolicy：默认的拒绝策略，简单粗暴，当execute中添加woker失败时，直接在当前线程抛出异常。</li>
<li>ThreadPoolExecutor.CallerRunsPolicy：提交任务的线程，直接执行任务。变相的背压机制，可以降低任务往线程中加入。</li>
<li>ThreadPoolExecutor.DiscardPolicy：直接丢弃被拒绝的任务，不做任何通知，需容忍数据丢失。</li>
<li>ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列中最旧的任务，然后重试提交当前任务，需容忍数据丢失。</li>
<li>实现RejectedExecutionHandler接口自定义拒绝策略，在实际生产应用中推荐使用，可以做一些打印观察日志的操作，告警、兜底的相关处理等。</li>
</ul> </li>
</ul> </li>
</ul>
<h1>3.3 运行机制详解</h1>
<p>新任务通过execute()方法提交给ThreadPoolExecutor时，其处理流程如下：</p>
<p><strong>判断核心线程数</strong>：如果当前运行的线程数小于corePoolSize，则创建新线程（即使有空闲的核心线程）来执行任务。</p>
<p><strong>尝试入队</strong>：如果当前运行的线程数大于或等于corePoolSize，则尝试将任务添加到workQueue中。</p>
<ul>
<li>如果workQueue.offer()成功（队列未满），任务入队等待执行。</li>
</ul>
<p><strong>尝试创建非核心线程</strong>：如果workQueue.offer()失败（队列已满）：</p>
<ul>
<li>判断当前运行的线程数是否小于maximumPoolSize；</li>
<li>如果是，则创建新的非核心线程来执行任务。</li>
</ul>
<p><strong>执行拒绝策略：</strong></p>
<p>如果当前运行的线程数也达到了maximumPoolSize（即核心线程和非核心线程都已用尽，且队列也满了），则执行RejectedExecutionHandler所定义的拒绝策略。</p>
<p>参考网络中的经典执行图：</p>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-88df05dbfd.jpg">
</div>
<p>这个图能很好的表明运行原理，但是忽略了很多细节，比如所谓的缓冲执行是在什么条件下去走的呢？直接执行又是什么逻辑下执行呢？最后的任务拒绝又是怎么回事？带着这些疑问点，我们直接来进行一个源码级别的分析：</p>
<h1>execute核心流程的源码分析</h1>
<pre><code>public&nbsp;void&nbsp;execute(Runnable command)&nbsp;{
&nbsp; &nbsp;&nbsp;if&nbsp;(command ==&nbsp;null)
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;NullPointerException();
&nbsp; &nbsp;&nbsp;//线程池状态 高3位表示线程状态 低29位代表线程数量
&nbsp; &nbsp;&nbsp;int&nbsp;c = ctl.get();
&nbsp; &nbsp;&nbsp;//判断当前线程池线程数量是否小于核心线程数
&nbsp; &nbsp;&nbsp;if&nbsp;(workerCountOf(c) &lt; corePoolSize) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//作为核心线程数进行线程的创建，并且创建成功线程会将command的任务执行--》对应图上的直接执行
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(addWorker(command,&nbsp;true))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; c = ctl.get();
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;//创建核心线程失败或者当前线程数量超过核心线程数
&nbsp; &nbsp;&nbsp;//当前线程池是否还在运行状态，尝试将任务添加到阻塞队列 --》对应图上的缓冲执行
&nbsp; &nbsp;&nbsp;//BlockingQueue队列的顶级抽象定义了offer不是进行阻塞添加而是立即返回，添加失败直接返回false，区别于put
&nbsp; &nbsp;&nbsp;if&nbsp;(isRunning(c) &amp;&amp; workQueue.offer(command)) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//重新获取线程池标志位
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;recheck = ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//如果线程此时不在运行状态中，那么将任务删除
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(! isRunning(recheck) &amp;&amp;&nbsp;remove(command))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//删除任务成功，走拒绝策略拒绝掉当前任务
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; reject(command);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(workerCountOf(recheck) ==&nbsp;0)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//如果线程池中的工作线程都没有的时候，这里需要创建一个线程去执行添加到队列中的任务
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//防止因为并发的原因工作线程都被终止掉了，此时任务在阻塞队列里等着，缺没有工作线程
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addWorker(null,&nbsp;false);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;//到这里那就是添加队列失败，或者线程池状态异常，但是这里仍然尝试进行创建一个worker
&nbsp; &nbsp;&nbsp;//如果创建失败，也是走拒绝策略拒绝当前任务
&nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(!addWorker(command,&nbsp;false))
&nbsp; &nbsp; &nbsp; &nbsp; reject(command);
}</code></pre>
<p>接下来我们仔细看看addWorker这个方法具体是在做什么：</p>
<pre><code>//核心逻辑其实就是在无限循环创建一个worker，创建失败直接返回，创建成功，则将worker执行
// 因为worker有thread的成员变量，最终添加worker成功，会启动线程的start方法
//start方法最终会回调到外层的runWorker方法，改方法会不停的从阻塞队列里以阻塞的take方式
//获取任务，除非达到能被终止的条件，此时当前线程会终止
private&nbsp;boolean&nbsp;addWorker(Runnable firstTask,&nbsp;boolean&nbsp;core)&nbsp;{
&nbsp; &nbsp; retry:
&nbsp; &nbsp;&nbsp;for&nbsp;(;;) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;c&nbsp;=&nbsp;ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;rs&nbsp;=&nbsp;runStateOf(c);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Check if queue empty only if necessary.
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(rs &gt;= SHUTDOWN &amp;&amp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ! (rs == SHUTDOWN &amp;&amp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;firstTask ==&nbsp;null&nbsp;&amp;&amp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;! workQueue.isEmpty()))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;false;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(;;) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;wc&nbsp;=&nbsp;workerCountOf(c);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(wc &gt;= CAPACITY ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; wc &gt;= (core ? corePoolSize : maximumPoolSize))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;false;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//不停的重试添加worker的计数，只有添加成功的才会进行后续的worker启动
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(compareAndIncrementWorkerCount(c))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;break&nbsp;retry;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; c = ctl.get(); &nbsp;// Re-read ctl
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//重试期间，如果其他线程导致线程池状态不一致了。重新回到第一个循环进行check判断
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(runStateOf(c) != rs)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;continue&nbsp;retry;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// else CAS failed due to workerCount change; retry inner loop
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;boolean&nbsp;workerStarted&nbsp;=&nbsp;false;
&nbsp; &nbsp;&nbsp;boolean&nbsp;workerAdded&nbsp;=&nbsp;false;
&nbsp; &nbsp;&nbsp;Worker&nbsp;w&nbsp;=&nbsp;null;
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; w =&nbsp;new&nbsp;Worker(firstTask);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;final&nbsp;Thread&nbsp;t&nbsp;=&nbsp;w.thread;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(t !=&nbsp;null) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//这里加锁一个是workers.add时需要加锁，另外是防止其他线程已经在尝试修改线程池状态了
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock&nbsp;mainLock&nbsp;=&nbsp;this.mainLock;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Recheck while holding lock.
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Back out on ThreadFactory failure or if
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// shut down before lock acquired.
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;rs&nbsp;=&nbsp;runStateOf(ctl.get());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(rs &lt; SHUTDOWN ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (rs == SHUTDOWN &amp;&amp; firstTask ==&nbsp;null)) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(t.isAlive())&nbsp;// precheck that t is startable
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;IllegalThreadStateException();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//将worker的引用添加到workers的hashSet中
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; workers.add(w);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;s&nbsp;=&nbsp;workers.size();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//更新线程池此时最大的线程数
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(s &gt; largestPoolSize)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; largestPoolSize = s;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; workerAdded =&nbsp;true;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//如果添加成功，就启动worker中的线程
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(workerAdded) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; t.start();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; workerStarted =&nbsp;true;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//这里添加失败的话，需要把线程池的count数进行--，并且要把worker引用从hashSer中移除
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(! workerStarted)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addWorkerFailed(w);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;return&nbsp;workerStarted;
}</code></pre>
<h1>3.4 线程池的生命周期</h1>
<p>在介绍运行机制原理的源码分析时，其实是有提到线程池状态这个概念的。介绍这个状态其实也是让大家更方便的去管理线程池，比如我们关闭线程池时，怎么去优雅的关闭，使用不同的方法可能会有不同的效果，我们需要根据自己的业务场景去酌情分析、权衡使用。</p>
<pre><code>//线程池的状态和计数采用一个Integer变量设置的
//这里之所以用一个变量来储存状态和数量，其实很有讲究的，因为我们在上面的运行原理上可以看到
//源码中有大量的进行状态以及数量的判断，如果分开采用变量的记录的话，在维护二者一致性方面
//可能就需要加锁的维护成本了，而且计算中都是位移运算也是非常高效的
private&nbsp;final&nbsp;AtomicInteger&nbsp;ctl&nbsp;=&nbsp;new&nbsp;AtomicInteger(ctlOf(RUNNING,&nbsp;0));
//线程池的大小由ctl低29位表示，现成状态由ctl高3位表示
private&nbsp;static&nbsp;final&nbsp;int&nbsp;COUNT_BITS&nbsp;=&nbsp;Integer.SIZE -&nbsp;3;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;CAPACITY&nbsp; &nbsp;=&nbsp;(1&nbsp;&lt;&lt; COUNT_BITS) -&nbsp;1;
// 线程池的状态通过简单的位移就能计算出来，状态只能从低到高流转，不能逆向
private&nbsp;static&nbsp;final&nbsp;int&nbsp;RUNNING&nbsp; &nbsp;&nbsp;=&nbsp;-1&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;SHUTDOWN&nbsp; &nbsp;=&nbsp;&nbsp;0&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;STOP&nbsp; &nbsp; &nbsp; &nbsp;=&nbsp;&nbsp;1&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;TIDYING&nbsp; &nbsp;&nbsp;=&nbsp;&nbsp;2&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;TERMINATED&nbsp;=&nbsp;&nbsp;3&nbsp;&lt;&lt; COUNT_BITS;
// 这里是获取线程状态以及获取线程数量的简单高效的位移方法
private&nbsp;static&nbsp;int&nbsp;runStateOf(int&nbsp;c)&nbsp; &nbsp; &nbsp;{&nbsp;return&nbsp;c &amp; ~CAPACITY; }
private&nbsp;static&nbsp;int&nbsp;workerCountOf(int&nbsp;c)&nbsp; {&nbsp;return&nbsp;c &amp; CAPACITY; }
private&nbsp;static&nbsp;int&nbsp;ctlOf(int&nbsp;rs,&nbsp;int&nbsp;wc)&nbsp;{&nbsp;return&nbsp;rs | wc; }</code></pre>
<p>接下来结合源码详细介绍下线程池的5种状态以及分别有什么不同的表现行为？</p>
<pre><code>先说下结论：
RUNNING&nbsp; &nbsp; &nbsp;这个就是线程池运行中状态，我们可以添加任务也可以处理阻塞队列任务
SHUTDOWN &nbsp; 不能添加新的任务，但是会将阻塞队列中任务执行完毕
STOP &nbsp; &nbsp; &nbsp; 不能添加新的任务，执行中的线程也会被打断，也不会处理阻塞队列的任务
TIDYING &nbsp; &nbsp;所有线程都被终止，并且workCount=0时会被置为的状态
TERMINATED &nbsp; 调用完钩子方法terminated()被置为的状态&nbsp;</code></pre>
<h1>shutdown状态源码分析：</h1>
<pre><code>&nbsp;
&nbsp;//线程池关闭
&nbsp;public&nbsp;void&nbsp;shutdown()&nbsp;{
&nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock&nbsp;mainLock&nbsp;=&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; checkShutdownAccess();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//循环cas设置线程池状态，直到成功或状态已经state&gt;=SHUTDOWN
&nbsp; &nbsp; &nbsp; &nbsp; advanceRunState(SHUTDOWN);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//这个是真正得出结论的地方
&nbsp; &nbsp; &nbsp; &nbsp; interruptIdleWorkers();
&nbsp; &nbsp; &nbsp; &nbsp; onShutdown();&nbsp;// hook for ScheduledThreadPoolExecutor
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
&nbsp; &nbsp; tryTerminate();
}
private&nbsp;void&nbsp;interruptIdleWorkers()&nbsp;{
&nbsp; &nbsp; interruptIdleWorkers(false);
}
//打断空闲的线程，如何判断线程是否空闲还是运行？
private&nbsp;void&nbsp;interruptIdleWorkers(boolean&nbsp;onlyOne)&nbsp;{
&nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock&nbsp;mainLock&nbsp;=&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(Worker w : workers) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Thread&nbsp;t&nbsp;=&nbsp;w.thread;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//worker的线程没有被打断过，并且能获取到worker的aqs独占锁
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!t.isInterrupted() &amp;&amp; w.tryLock()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//打断当前线程，如果线程在阻塞队列中阻塞，此时会被中断
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; t.interrupt();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(SecurityException ignore) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; w.unlock();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(onlyOne)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;break;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
}
</code></pre>
<h1>STOP状态分析</h1>
<pre><code>//循环cas修改线程池状态为stop。打断所有线程，取出阻塞队列的所有任务
public&nbsp;List&lt;Runnable&gt;&nbsp;shutdownNow()&nbsp;{
&nbsp; &nbsp; List&lt;Runnable&gt; tasks;
&nbsp; &nbsp; final ReentrantLock mainLock =&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//检查线程的权限
&nbsp; &nbsp; &nbsp; &nbsp; checkShutdownAccess();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//将状态case为stop
&nbsp; &nbsp; &nbsp; &nbsp; advanceRunState(STOP);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//打断所有worker不管是不是正在执行任务
&nbsp; &nbsp; &nbsp; &nbsp; interruptWorkers();
&nbsp; &nbsp; &nbsp; &nbsp; tasks = drainQueue();
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
&nbsp; &nbsp; tryTerminate();
&nbsp; &nbsp;&nbsp;return&nbsp;tasks;
}
//这里获取锁之后。打断了所有的线程
private&nbsp;void&nbsp;interruptWorkers()&nbsp;{
&nbsp; &nbsp; final ReentrantLock mainLock =&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(Worker w : workers)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; w.interruptIfStarted();
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
}</code></pre>
<h1>TIDYING、TERMINATED&nbsp;状态分析</h1>
<pre><code>//这个方法在每个线程退出时都会进行调用，如果是运行中、或者状态大于等于TIDYING或者shutdown但是队列不为空都
//直接返回，如果不满足以上条件，并且线程数不为0的话，打断一个空闲线程
final&nbsp;void tryTerminate() {
&nbsp; &nbsp;&nbsp;for&nbsp;(;;) {
&nbsp; &nbsp; &nbsp; &nbsp; int c = ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(isRunning(c) ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; runStateAtLeast(c, TIDYING) ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty()))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(workerCountOf(c) !=&nbsp;0) {&nbsp;// Eligible to terminate
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; interruptIdleWorkers(ONLY_ONE);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//此时到这里，状态要么为STOP。要么是shutdown并且队列为空了
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 获取一个锁，尝试cas修改状态为TIDYING
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//调用terminated()的钩子方法，
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//修改线程池为终态TERMINATED，并且唤醒阻塞在termination队列上的线程
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock mainLock =&nbsp;this.mainLock;
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ctl.compareAndSet(c, ctlOf(TIDYING,&nbsp;0))) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; terminated();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ctl.set(ctlOf(TERMINATED,&nbsp;0));
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; termination.signalAll();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// else retry on failed CAS
&nbsp; &nbsp; }
}</code></pre>
<h1>四、JDK内置线程池的问题</h1>
<p>java.util.concurrent.Executors工厂类提供了一些静态方法，方便我们快速创建几种预设配置的线程池：</p>
<ul>
<li>Executors.newFixedThreadPool(int nThreads)：
<ul>
<li>创建一个固定大小的线程池。corePoolSize和maximumPoolSize都等于nThreads。</li>
<li>keepAliveTime为0L（因为线程数不会超过corePoolSize，所以此参数无效，除非allowCoreThreadTimeOut为true）。</li>
<li>使用无界的LinkedBlockingQueue作为工作队列。</li>
<li><strong>问题</strong>：由于使用无界队列，当任务提交速度远大于处理速度时，队列会持续增长，可能导致内存溢出（OOM）。此时maximumPoolSize参数实际上是无效的，线程数永远不会超过nThreads。</li>
</ul> </li>
</ul>
<ul>
<li>Executors.newSingleThreadExecutor()：
<ul>
<li>创建一个只有一个工作线程的线程池。corePoolSize和maximumPoolSize都为1。</li>
<li>同样使用<strong>无界</strong>的LinkedBlockingQueue。</li>
<li>保证所有任务按照提交顺序（FIFO）执行。</li>
<li><strong>问题</strong>：与newFixedThreadPool类似，无界队列可能导致OOM。</li>
</ul> </li>
</ul>
<ul>
<li>Executors.newCachedThreadPool()：
<ul>
<li>创建一个可缓存的线程池。</li>
<li>corePoolSize为0。</li>
<li>maximumPoolSize为Integer.MAX_VALUE (几乎是无界的)。</li>
<li>keepAliveTime为60秒。</li>
<li>使用SynchronousQueue作为工作队列。这种队列不存储元素，任务提交后必须有空闲线程立即接收，否则会创建新线程（如果未达到maximumPoolSize）。</li>
<li><strong>问题</strong>：如果任务提交速度过快，会创建大量线程（理论上可达Integer.MAX_VALUE个），可能耗尽系统资源，导致OOM以及频繁的上下文切换。</li>
</ul> </li>
</ul>
<ul>
<li>Executors.newSingleThreadScheduledExecutor()、Executors.newScheduledThreadPool(int corePoolSize):
<ul>
<li>创建用于调度任务的线程池。</li>
<li>内部使用ScheduledThreadPoolExecutor实现，其任务队列是DelayedWorkQueue (一种特殊的PriorityQueue)。</li>
<li>newSingleThreadScheduledExecutor的corePoolSize为1，maximumPoolSize为Integer.MAX_VALUE（但由于队列是DelayedWorkQueue，通常不会无限增长线程，除非有大量同时到期的任务且处理不过来）。</li>
<li>newScheduledThreadPool可以指定corePoolSize。</li>
<li><strong>问题</strong>：虽然DelayedWorkQueue本身是无界的，但ScheduledThreadPoolExecutor在任务执行逻辑上与普通ThreadPoolExecutor有所不同。主要风险仍然是如果corePoolSize设置不当，且大量任务同时到期并执行缓慢，可能导致任务积压。</li>
</ul> </li>
</ul>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a0860059e7.jpg">
</div>
<p>某一线互联网Java开发手册</p>
<h1>五、线程池中的问题与最佳实践</h1>
<h1>5.1 invokeAll 超时机制无效？</h1>
<p>ExecutorService.invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit)方法会提交一组Callable任务，并等待所有任务完成，或者直到超时。如果超时发生，它会尝试取消（中断）所有尚未完成的任务，然后返回一个List&lt;Future&gt;。</p>
<p><strong>失效场景分析：</strong></p>
<ul>
<li>任务不响应中断（最常见）：任务内部捕获&nbsp;InterruptedException&nbsp;后静默处理，或执行不检查中断状态的阻塞操作（如循环计算）：</li>
</ul>
<pre><code>Callable&lt;String&gt; task = () -&gt; {
&nbsp; &nbsp;&nbsp;while&nbsp;(true) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//缺少此检查将导致超时失效
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(Thread.interrupted())&nbsp;break;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 耗时计算...
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;return&nbsp;"done";
};</code></pre>
<ul>
<li><strong>使用非响应中断的API</strong>：任务调用了不响应&nbsp;interrupt()&nbsp;的第三方库或JNI代码（如某些IO操作）</li>
</ul>
<pre><code>Callable&lt;Integer&gt; task&nbsp;=&nbsp;() -&gt; {
&nbsp; &nbsp;&nbsp;Files.copy(in, path);&nbsp;// 某些NIO操作不响应中断
&nbsp; &nbsp;&nbsp;return&nbsp;1;
};</code></pre>
<ul>
<li><strong>任务依赖外部资源阻塞</strong>：任务因外部资源（如数据库连接、网络请求）阻塞且未设置超时。</li>
</ul>
<pre><code>Callable&lt;Result&gt; task&nbsp;=&nbsp;() -&gt; {
&nbsp; &nbsp;&nbsp;//未设查询超时时间
&nbsp; &nbsp;&nbsp;return&nbsp;jdbcTemplate.query("SELECT * FROM large_table");&nbsp;
};</code></pre>
<ul>
<li><strong>线程池配置缺陷</strong>：核心线程数过大或队列无界，导致&nbsp;invokeAll&nbsp;超时前任务无法全部启动，任务堆积在队列，invokeAll&nbsp;超时后仍有大量任务未执行。</li>
</ul>
<pre><code>new&nbsp;ThreadPoolExecutor(
&nbsp; &nbsp;&nbsp;100,&nbsp;100,&nbsp;// 核心线程数过大
&nbsp; &nbsp; 0L,&nbsp;TimeUnit.MILLISECONDS,
&nbsp; &nbsp;&nbsp;new&nbsp;LinkedBlockingQueue&lt;&gt;()&nbsp;// 无界队列
);</code></pre>
<p><strong>invokeAll超时失效demo:</strong></p>
<pre><code>import&nbsp;java.util.*;
import&nbsp;java.util.concurrent.*;
public&nbsp;class&nbsp;InvokeAllTimeoutDemo&nbsp;{
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;// 模拟耗时任务（可配置是否响应中断）
&nbsp; &nbsp;&nbsp;static&nbsp;class&nbsp;Task&nbsp;implements&nbsp;Callable&lt;String&gt; {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;int&nbsp;id;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;long&nbsp;durationMs;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;boolean&nbsp;respectInterrupt;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; Task(int&nbsp;id,&nbsp;long&nbsp;durationMs,&nbsp;boolean&nbsp;respectInterrupt) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.id = id;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.durationMs = durationMs;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.respectInterrupt = respectInterrupt;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;String&nbsp;call()&nbsp;throws&nbsp;Exception {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.printf("Task %d started%n", id);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;start&nbsp;=&nbsp;System.currentTimeMillis();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 模拟工作（检查中断状态）
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;while&nbsp;(System.currentTimeMillis() - start &lt; durationMs) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(respectInterrupt &amp;&amp; Thread.interrupted()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;InterruptedException("Task "&nbsp;+ id +&nbsp;" interrupted");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 不响应中断的任务会继续执行
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.printf("Task %d completed%n", id);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;"Result-"&nbsp;+ id;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;public&nbsp;static&nbsp;void&nbsp;main(String[] args)&nbsp;throws&nbsp;InterruptedException {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;ExecutorService&nbsp;executor&nbsp;=&nbsp;Executors.newFixedThreadPool(2);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; List&lt;Callable&lt;String&gt;&gt; tasks = Arrays.asList(
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;new&nbsp;Task(1,&nbsp;2000,&nbsp;true), &nbsp;&nbsp;// 2秒，响应中断
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;new&nbsp;Task(2,&nbsp;10000,&nbsp;false) &nbsp;// 10秒，不响应中断
&nbsp; &nbsp; &nbsp; &nbsp; );
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Invoking with 3s timeout...");
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//设置3秒超时
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; List&lt;Future&lt;String&gt;&gt; futures = executor.invokeAll(tasks,&nbsp;3, TimeUnit.SECONDS);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(Future&lt;String&gt; f : futures) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 明确处理取消状态
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(f.isCancelled()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Task was cancelled"); &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Result: "&nbsp;+ f.get(100, TimeUnit.MILLISECONDS));
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(TimeoutException | ExecutionException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Task failed: "&nbsp;+ e.getCause());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.shutdownNow();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Executor shutdown");
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p>当我们使用invokeAll(tasks, timeout)&nbsp;提交多个任务时，如果出现某个任务对中断不响应或者响应不及时，那我们即使设置了超时时间，不响应中断的任务2仍在后台运行（即使调用了&nbsp;shutdownNow()）</p>
<h1>5.2 submit()的异常消失了？</h1>
<p>使用ExecutorService.submit()提交任务时，任务执行过程中如果抛出未捕获的异常（无论是受检异常还是运行时异常），这个异常会被Future的包装类如FutureTask重写的run()方法捕获并封装在返回的Future包装对象的成员变量中。</p>
<ul>
<li><strong>不显示调用</strong>Future.get()，该异常我们就无法感知，好像没有发生过一样。线程池的工作线程本身通常会有一个默认的未捕获异常处理器，可能会打印堆栈到控制台，但你的主业务逻辑不会知道。</li>
<li><strong>显示调用</strong>Future.get()，抛出声明式的ExecutionException，其cause属性才是原始的任务异常。</li>
<li>如果调用Future.get(long timeout, TimeUnit unit)超时，向外抛出声明式的TimeoutException。此时任务可能仍在后台执行，可能错过了内部的异常。</li>
</ul>
<p><strong>submit()异常消失demo:</strong></p>
<pre><code>public&nbsp;class&nbsp;ThreadPoolExceptionDemo&nbsp;{
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;public&nbsp;static&nbsp;void&nbsp;main(String[] args)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 创建单线程线程池（便于观察异常）
&nbsp; &nbsp; &nbsp; &nbsp; ExecutorService executor = Executors.newSingleThreadExecutor();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 场景1：Callable抛出异常（通过Future.get()捕获）
&nbsp; &nbsp; &nbsp; &nbsp; Future&lt;String&gt; future1 = executor.submit(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("[Callable] 开始执行");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.sleep(100);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;RuntimeException("Callable故意抛出的异常");
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Callable结果: "&nbsp;+ future1.get());
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(ExecutionException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.err.println("捕获到Callable异常: "&nbsp;+ e.getCause().getMessage());
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(InterruptedException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.currentThread().interrupt();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 场景2：Runnable抛出异常（同样通过Future.get()捕获）
&nbsp; &nbsp; &nbsp; &nbsp; Future&lt;?&gt; future2 = executor.submit(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("[Runnable] 开始执行");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;IllegalArgumentException("Runnable故意抛出的异常");
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; future2.get();&nbsp;// Runnable成功时返回null
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(ExecutionException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.err.println("捕获到Runnable异常: "&nbsp;+ e.getCause().getMessage());
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(InterruptedException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.currentThread().interrupt();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 场景3：未处理的任务异常（需设置异常处理器）
&nbsp; &nbsp; &nbsp; &nbsp; executor.submit(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("[未捕获的任务] 开始执行");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;IllegalStateException("这个异常会被默认处理器处理");
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; executor.shutdown();
&nbsp; &nbsp; }
}</code></pre>
<h1>5.3 异常处理实践</h1>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-851f74dab6.jpg">
</div>
<ul>
<li><strong>Callable/Runnable catch处理异常：</strong>
<ul>
<li><strong>不要捕获Throwable或Exception然后静默处理（只打日志）</strong>。如果确实需要捕获，请考虑是否应该重新抛出（包装成业务允许的受检异常或运行时异常）。</li>
<li><strong>禁止静默处理&nbsp;</strong>InterruptedException：</li>
<li>在JDK的JUC底层源码中，我们可以看到很多声明了InterruptedException的方法，基本上都是对这类方法catch异常，要么继续往外抛出，或者处理完相关资源后，重置中断状态，<strong>绝对不要静默处理。</strong></li>
<li>如果方法没有声明InterruptedException如Runnable.run()，在catch&nbsp;InterruptedException后最好调用Thread.currentThread().interrupt()来恢复中断标记。</li>
<li><strong>正确处理中断</strong>：callable在耗时的loop任务处理中，如果出现了中断异常，因为Java代码中中断只是一种协作方式，其并没真的终止线程，所以一般都是需要我们进行一个中断标志的传递，如线程池中的shutdownNow()就依赖次机制处理。</li>
</ul> </li>
</ul>
<ul>
<li><strong>submit()执行的任务，谨慎处理Future：</strong>
<ul>
<li>使用带过期时间的future.get(long timeOut)获取结果，并要对该方法进行try cache防止其他异常抛出。</li>
<li>多个任务并行处理时，如果有下个请求依赖上个请求，务必使用get()让主线程等待这一结果执行完成后，流转到下一个异步任务。</li>
</ul> </li>
</ul>
<ul>
<li><strong>实现线程Thread的UncaughtExceptionHandler属性</strong>，在自定义的TheadFactory中通过set方法赋值：execute()方法执行时，对于没有捕获的异常使用线程组的兜底统一处理机制。</li>
</ul>
<pre><code>//自定义当前线程组创建线程的统一异常处理，类似于controller的统一异常处理机制
ThreadFactory&nbsp;myThreadFactory&nbsp;=&nbsp;new&nbsp;ThreadFactory() {
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;AtomicInteger&nbsp;atomicInteger&nbsp;=&nbsp;new&nbsp;AtomicInteger(0);
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;String&nbsp;threadNamePrefix&nbsp;=&nbsp;"myThreadFactory-";
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;public&nbsp;Thread&nbsp;newThread(Runnable r)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Thread&nbsp;t&nbsp;=&nbsp;&nbsp;new&nbsp;Thread(r,threadNamePrefix + atomicInteger.getAndIncrement());
&nbsp; &nbsp; &nbsp; &nbsp; t.setUncaughtExceptionHandler((thread, throwable) -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//异常的统一处理，日志打印、兜底处理、监控、资源释放等
&nbsp; &nbsp; &nbsp; &nbsp; System.err.println("线程["&nbsp;+ thread.getName() +&nbsp;"]异常: "&nbsp;+ throwable);});
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;t;
&nbsp; &nbsp; }
};
//构造方法时使用自定义的线程工厂
ExecutorService&nbsp;executor&nbsp;=&nbsp;new&nbsp;ThreadPoolExecutor(
&nbsp; &nbsp; corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
&nbsp; &nbsp; threadFactory,
&nbsp; &nbsp; handler
);</code></pre>
<ul>
<li><strong>使用自定义线程池时建议重写钩子方法afterExecute(Runnable r, Throwable t)</strong>：这个hook方法是用来解决当前任务线程发生的异常，默认是空实现，我们可以重写他，比如进行兜底的线程继续执行，打印日志记录，以及同步失败使用兜底异步处理等等方式。还要注意释放应用中的资源，比如文件锁的占用等，最好手动释放掉，避免底层操作系统线程对这类资源释放失败导致长期占用，最后只能重启Java进程的尴尬地步。</li>
</ul>
<pre><code>public&nbsp;class&nbsp;MyThreadPoolExecutor&nbsp;extends&nbsp;ThreadPoolExecutor&nbsp;{
&nbsp; &nbsp;&nbsp;public&nbsp;MyThreadPoolExecutor(int&nbsp;corePoolSize,&nbsp;int&nbsp;maximumPoolSize,&nbsp;long&nbsp;keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;afterExecute(Runnable r, Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;//需要特别注意任务是否为submit提交，如果是execute提交的任务，那这里很直接的知道任务是否发生异常以及后续去怎么处理
&nbsp; &nbsp; &nbsp; &nbsp;if(r&nbsp;instanceof&nbsp;Future){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(((Future&lt;?&gt;) r).isDone() || ((Future&lt;?&gt;) r).isCancelled()){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//继续使用主线程完成任务,一般不建议，最好使用兜底方式：例如异步发消息，由后续的消费组统一处理异常的任务
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; &nbsp; &nbsp;}else&nbsp;if( t !=&nbsp;null){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//execute异常处理
&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; }
}
//FutureTask 把run方法进行了重写，并且catch住了异常，所以说afterExecute的t 如果是submit提交的方式
//那么t基本上就是null
public&nbsp;void&nbsp;run()&nbsp;{
&nbsp; &nbsp;&nbsp;//....
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; Callable&lt;V&gt; c = callable;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(c !=&nbsp;null&nbsp;&amp;&amp; state == NEW) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; V result;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;boolean&nbsp;ran;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; result = c.call();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ran =&nbsp;true;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(Throwable ex) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; result =&nbsp;null;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ran =&nbsp;false;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; setException(ex);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ran)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; set(result);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp;//...
}</code></pre>
<p><strong>afterExecute可以借鉴的示例：</strong></p>
<pre><code>import&nbsp;java.util.concurrent.*;
import&nbsp;java.util.concurrent.atomic.*;
import&nbsp;org.slf4j.*;
public&nbsp;class&nbsp;RobustThreadPool&nbsp;extends&nbsp;ThreadPoolExecutor&nbsp;{
&nbsp; &nbsp;&nbsp;private&nbsp;static&nbsp;final&nbsp;Logger&nbsp;logger&nbsp;=&nbsp;LoggerFactory.getLogger(RobustThreadPool.class);
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;AtomicLong&nbsp;failureCounter&nbsp;=&nbsp;new&nbsp;AtomicLong();
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;RetryPolicy retryPolicy;&nbsp;// 重试策略
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;ThreadLocal&lt;Long&gt; startTime =&nbsp;new&nbsp;ThreadLocal&lt;&gt;();
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;public&nbsp;RobustThreadPool(int&nbsp;corePoolSize,&nbsp;int&nbsp;maxPoolSize,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; BlockingQueue&lt;Runnable&gt; workQueue,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; RetryPolicy retryPolicy) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;super(corePoolSize, maxPoolSize,&nbsp;60L, TimeUnit.SECONDS, workQueue);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.retryPolicy = retryPolicy;
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;beforeExecute(Thread t, Runnable r)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; logger.debug("开始执行任务: {}", r);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;afterExecute(Runnable r, Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 1. 异常分类处理
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(r&nbsp;instanceof&nbsp;Future){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(((Future&lt;?&gt;) r).isDone()){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//错误记录以及异常处理
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; failureCounter.incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; handleFailure(r, t, costTime);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; &nbsp; &nbsp;}else&nbsp;if( t !=&nbsp;null){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//execute异常处理
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; failureCounter.incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; handleFailure(r, t, costTime);
&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 2. 资源清理
&nbsp; &nbsp; &nbsp; &nbsp; cleanThreadLocals();
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;void&nbsp;handleFailure(Runnable r, Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 1. 异常类型识别
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(t&nbsp;instanceof&nbsp;OutOfMemoryError) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.error("JVM内存不足，终止任务: {}", t.getMessage());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.exit(1);&nbsp;// 严重错误直接终止
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 2. 可重试异常处理
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(isRetryable(t)) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;retryCount&nbsp;=&nbsp;retryPolicy.getCurrentRetryCount(r);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(retryCount &lt; retryPolicy.getMaxRetries()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.warn("任务第{}次失败，准备重试...",&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryCount +&nbsp;1, t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryPolicy.retry(r,&nbsp;this);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.error("任务超过最大重试次数({})，转入死信队列",&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryPolicy.getMaxRetries(), t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DeadLetterQueue.add(r, t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 3. 不可重试异常
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.error("不可恢复任务失败", t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Metrics.recordFailure(t.getClass());&nbsp;// 上报监控
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;boolean&nbsp;isRetryable(Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;t&nbsp;instanceof&nbsp;IOException ||&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;t&nbsp;instanceof&nbsp;TimeoutException ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(t.getCause() !=&nbsp;null&nbsp;&amp;&amp; isRetryable(t.getCause()));
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;void&nbsp;cleanThreadLocals()&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 清理可能的内存泄漏
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ThreadLocal&lt;?&gt;[] holders = {&nbsp;/* 其他ThreadLocal */};
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(ThreadLocal&lt;?&gt; holder : holders) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; holder.remove();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(Exception e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.warn("清理ThreadLocal失败", e);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;// 重试策略嵌套类
&nbsp; &nbsp;&nbsp;public&nbsp;static&nbsp;class&nbsp;RetryPolicy&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;int&nbsp;maxRetries;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;long&nbsp;retryDelayMs;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;Map&lt;Runnable, AtomicInteger&gt; retryMap =&nbsp;new&nbsp;ConcurrentHashMap&lt;&gt;();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;RetryPolicy(int&nbsp;maxRetries,&nbsp;long&nbsp;retryDelayMs)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.maxRetries = maxRetries;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.retryDelayMs = retryDelayMs;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;retry(Runnable task, Executor executor)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryMap.computeIfAbsent(task, k -&gt;&nbsp;new&nbsp;AtomicInteger()).incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(retryDelayMs &gt;&nbsp;0) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.execute(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.sleep(retryDelayMs);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(InterruptedException ignored) {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.execute(task);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.execute(task);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;int&nbsp;getCurrentRetryCount(Runnable task)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;retryMap.getOrDefault(task,&nbsp;new&nbsp;AtomicInteger()).get();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;int&nbsp;getMaxRetries()&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;maxRetries;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p><strong>异常处理小结</strong>：要特别注意使用future.get()方法时，我们一定要注意设置超时时间，防止主线程无限期的阻塞避免边缘的业务查询影响了主业务造成得不偿失的效果，另外我们需要注意一个点就是submit()方法的提交任务时，afterExecute(Runnable r, Throwable t)中的t恒为null，如果是execute方法提交的任务，那么就是直接获取的任务执行的异常，对于submit提交的任务异常其被封装到了Futrure&nbsp;包装对象中，一般需要我们再次判断任务时执行完毕还是异常或被取消了，如果发生了异常，Future.get()会抛出封装的ExecutionException异常，当然还可能是取消异常以及中断异常。invokeAll和invokeAny我们需要对返回的Future结果检查可能抛出的异常，对于callable&nbsp;前面一再强调了要对InterruptedException不要静默处理，因为线程的中断标记只是一个协作方式，他并没有停止当前线程的运行，我们需要根据自身的场景对发生的中断进行快速响应以及传递中断标志。</p>
<h1>5.4 拒绝策略实践</h1>
<p>先带大家回顾一下策略是如何触发执行的流程：</p>
<pre><code>//添加任务，当不满足条件时会执行拒绝方法reject
public&nbsp;void&nbsp;execute(Runnable command)&nbsp;{
&nbsp; &nbsp;//...
&nbsp; &nbsp;&nbsp;if&nbsp;(isRunning(c) &amp;&amp; workQueue.offer(command)) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;recheck = ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(! isRunning(recheck) &amp;&amp;&nbsp;remove(command))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; reject(command);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(workerCountOf(recheck) ==&nbsp;0)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addWorker(null,&nbsp;false);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(!addWorker(command,&nbsp;false))
&nbsp; &nbsp; &nbsp; &nbsp; reject(command);
}
//这里就是拒绝的入口。handler是有构造方法传入
final&nbsp;void&nbsp;reject(Runnable command)&nbsp;{
&nbsp; &nbsp; handler.rejectedExecution(command,&nbsp;this);
}
public&nbsp;ThreadPoolExecutor(int&nbsp;corePoolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;maximumPoolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;keepAliveTime,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TimeUnit unit,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; BlockingQueue&lt;Runnable&gt; workQueue,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ThreadFactory threadFactory,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; RejectedExecutionHandler handler) {
&nbsp; &nbsp;&nbsp;//....
&nbsp; &nbsp;&nbsp;//指定拒绝策略
&nbsp; &nbsp;&nbsp;this.handler = handler;
}</code></pre>
<p><strong>AbortPolicy</strong>：默认的拒绝策略，简单粗暴，当execute中添加woker失败时，直接在当前线程抛出异常。</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;AbortPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp; &nbsp;//直接抛出RejectedExecutionException
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;RejectedExecutionException("Task "&nbsp;+ r.toString() +
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" rejected from "&nbsp;+
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;e.toString());
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：快速失败，立即暴露系统过载问题、避免任务静默丢失，便于监控系统捕获</p>
<p><strong>缺点</strong>：需要调用方显式处理异常，增加代码复杂度，可能中断主业务流程</p>
<p><strong>适用场景</strong>：适用于那些对任务丢失非常敏感，配合熔断机制使用的快速失败场景</p>
<p><strong>CallerRunsPolicy</strong>：提交任务的线程，直接执行任务</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;CallerRunsPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;&nbsp;//直接在提交任务的线程中执行任务
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!e.isShutdown()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; r.run();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：任务都会被执行，不会丢任务，并且由于主线程执行任务，天然的流量控制，避免了大量的任务进入线程池。</p>
<p><strong>缺点</strong>：调用线程可能被阻塞，导致上游服务雪崩。不适合高并发场景（可能拖垮整个调用链）。</p>
<p><strong>适用场景</strong>：适用于处理能力不高，并且资源过载能够平滑过渡，同时不丢失任务的场景。如：低并发、高可靠性的后台任务（如日志归档）、允许同步执行的批处理系统。</p>
<p><strong>DiscardPolicy</strong>：直接丢弃被拒绝的任务，不做任何通知。</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;DiscardPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;//空实现
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：实现简单，无额外性能开销。避免异常传播影响主流程</p>
<p><strong>缺点</strong>：数据静默丢失，可能会掩盖系统容量问题</p>
<p><strong>适用场景</strong>：边缘业务的监控上报数据，统计类的uv、pv统计任务</p>
<p>DiscardOldestPolicy：丢弃队列中最旧的任务，然后重试提交当前任务。</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;DiscardOldestPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;&nbsp;//丢弃队列中最旧的任务，重试当前任务
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!e.isShutdown()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; e.getQueue().poll();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; e.execute(r);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：优先保证新任务执行，避免队列堆积导致内存溢出。</p>
<p><strong>缺点</strong>：可能丢失关键旧任务、任务执行顺序无法保证。</p>
<p><strong>适用场景</strong>：适用于可容忍部分数据丢失，并且实时性要求高于历史数据的场景，比如：行情推送。</p>
<p>通过上线的介绍，我们可以看到JDK内置策略基本上只使用于简单处理的场景，在生产实践中一般推荐我们自定义拒绝策略，进行相关的业务处理。</p>
<p><strong>1. 自定义RejectedExecutionHandler</strong>：</p>
<pre><code>/**
&nbsp;* 带监控统计的拒绝策略处理器
&nbsp;*/
public&nbsp;class&nbsp;MetricsRejectedExecutionHandler&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;&nbsp;private&nbsp;static&nbsp;final&nbsp;org.slf4j.Logger&nbsp;logger&nbsp;=&nbsp;org.slf4j.LoggerFactory.getLogger(MetricsRejectedExecutionHandler.class);
&nbsp; &nbsp;&nbsp;// 统计被拒绝的任务数量
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;AtomicLong&nbsp;rejectedCount&nbsp;=&nbsp;new&nbsp;AtomicLong(0);
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor executor)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 1. 采集线程池关键指标
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;poolSize&nbsp;=&nbsp;executor.getPoolSize();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;activeThreads&nbsp;=&nbsp;executor.getActiveCount();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;corePoolSize&nbsp;=&nbsp;executor.getCorePoolSize();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;maxPoolSize&nbsp;=&nbsp;executor.getMaximumPoolSize();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;queueSize&nbsp;=&nbsp;executor.getQueue().size();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;completedTasks&nbsp;=&nbsp;executor.getCompletedTaskCount();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 2. 递增拒绝计数器
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;totalRejected&nbsp;=&nbsp;rejectedCount.incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 3. 输出警告日志（包含完整指标）
&nbsp; &nbsp; &nbsp; &nbsp; logger.warn("""
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;任务被拒绝执行！线程池状态:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 活跃线程数/当前线程数: {}/{}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 核心/最大线程数: {}/{}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 队列大小: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 已完成任务数: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 历史拒绝总数: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 被拒绝任务: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; """,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activeThreads, poolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; corePoolSize, maxPoolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; queueSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; completedTasks,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; totalRejected,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; r.getClass().getName());
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 4. 可选：降级处理（如存入数据库等待重试）
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// fallbackToDatabase(r);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 5. 抛出RejectedExecutionException（保持默认行为）
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;RejectedExecutionException("Task "&nbsp;+ r.toString() +&nbsp;" rejected");
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;// 获取累计拒绝次数（用于监控）
&nbsp; &nbsp;&nbsp;public&nbsp;long&nbsp;getRejectedCount()&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;rejectedCount.get();
&nbsp; &nbsp; }
}</code></pre>
<ul>
<li><strong>记录日志并告警</strong>：所有的异常处理中，最常见简单的方式无外乎，先记录个日志，然后有告警系统的进行相关的某书、某信以及短信等的告警信息推送，方便开发人员以及运维人员的及时发现问题并介入处理。</li>
<li><strong>兜底处理机制</strong>：一般常见的就是通过异步的方式提交到MQ，然后统一进行兜底处理。</li>
<li><strong>带超时和重试的拒绝</strong>：可以尝试等待一小段时间，或者重试几次提交，如果仍然失败，再执行最终的拒绝逻辑（如告警、持久化或抛异常）。</li>
<li><strong>动态调整策略</strong>：根据系统的负载或任务类型，动态的执行兜底策略机制，就如前面写的源码示例方式。</li>
</ul>
<p><strong>2. 根据自身业务场景选择合适的拒绝策略：</strong></p>
<ul>
<li><strong>核心业务，不容丢失</strong>：如果任务非常重要，不能丢失，可以考虑：
<ul>
<li>CallerRunsPolicy：调用线程承担任务执行压力，是否可支撑；</li>
<li>自定义策略：尝试持久化到MQ或DB，然后由专门的消费组补偿任务处理；</li>
<li>AbortPolicy：如果希望系统快速失败并由上层进行重试或熔断。</li>
<li>&nbsp;</li>
</ul> </li>
<li><strong>非核心业务，可容忍部分丢失</strong>：
<ul>
<li>DiscardOldestPolicy：新任务更重要时，如行情推送；</li>
<li>DiscardPolicy：边缘业务场景，比如一些pv统计等，丢失了无所谓；</li>
<li>及时的进行监控查看，了解任务的丢失情况。</li>
</ul> </li>
</ul>
<p><strong>3. 结合线程池参数综合考虑：</strong></p>
<ul>
<li>拒绝策略的选择也与线程池的队列类型（有界/无界）、队列容量、maximumPoolSize等参数密切相关。</li>
<li>如果使用无界队列LinkedBlockingQueue的无参构造，只有机器内存不够时才会进行拒绝策略，不过这种极端场景已经不是影响线程池本身，内存不够可能导致Java进程被操作系统直接kill可能。</li>
<li>如果使用有界队列，需要权衡队列的大小，核心场景甚至可以动态追踪阻塞队列大小，以及动态调整队列大小来保证核心业务的正常流转。</li>
</ul>
<ol>
<li><strong>充分测试和监控</strong>：无论选择哪种策略，都必须在压测环境中充分测试其行为，并在线上环境建立完善的监控体系，监控线程池的各项指标（活跃线程数、队列长度、任务完成数、任务拒绝数等）。当拒绝发生时，应有相应的告警通知。</li>
</ol>
<p><strong>拒绝策略小结</strong>：</p>
<p>策略的选择跟我们大多数的系统设计哲学是保持一致的，都是在应对不同的场景中，做出一定的trade off。最好的策略需要根据业务场景、系统容忍度、资源等方面的综合考量，<strong>一个黄金的实践原则</strong>：拒绝事件做好监控告警、根据业务SLA定义策略，是否可丢失，快速失败等，定期的进行压力测试，验证策略的有效性。</p>
<h1>5.5 池隔离实践</h1>
<p><strong>核心思想</strong>：根据任务的资源类型 、优先级和业务特性 ，划分多个独立的线程池，避免不同性质的任务相互干扰。</p>
<p><strong>1. 隔离维度</strong>：</p>
<ul>
<li><strong>资源类型</strong>：CPU密集型 vs &nbsp;I/O密集型任务</li>
<li><strong>执行时长</strong>：短时任务（毫秒级） vs 长时任务（分钟级）</li>
<li><strong>实时性要求</strong>：高实时性 vs 可延迟（最终一致即可）</li>
<li><strong>业务重要性</strong>：支付交易（高优先级） vs 日志清理（低优先级）</li>
<li><strong>是否依赖外部资源</strong>：例如，访问特定数据库、调用特定第三方API的任务可以归为一类。</li>
</ul>
<p><strong>2. 不同业务场景线程池独立使用</strong>：在不同的业务场景下，为自己的特定业务，创建独立的线程池。</p>
<ul>
<li><strong>线程命名</strong>：通过ThreadFactory为每个线程池及其线程设置有意义的名称，例如netty-io-compress-pool-%d，excel-export-pool-%d, 主要方便区别不同的业务场景以及问题排查。</li>
<li><strong>参数调优</strong>：不同的业务场景设置不同的参数。
<ul>
<li>corePoolSize, maximumPoolSize：CPU密集型的计算任务可以设置小点减少上下文的切换，I/O密集型可以较大，在io阻塞等待期间，多去处理其他任务。</li>
<li>阻塞队列blockQueue：选择合适的队列类型，以及设置合理的队列大小。</li>
<li>RejectedExecutionHandler：有内置的四种的策略以及自定义策略选择，一般建议做好日志、监控以及兜底的处理。</li>
<li>&nbsp;</li>
</ul> </li>
</ul>
<p><strong>3. 自定义Executor避免线程池共用</strong></p>
<pre><code>// 创建CPU密集型任务线程池（线程数=CPU核心数）
ExecutorService cpuIntensiveExecutor =&nbsp;new&nbsp;ThreadPoolExecutor(
&nbsp; &nbsp; Runtime.getRuntime().availableProcessors(),&nbsp;// 核心线程数=CPU核心数
&nbsp; &nbsp; Runtime.getRuntime().availableProcessors(),&nbsp;// 最大线程数=CPU核心数
&nbsp; &nbsp;&nbsp;30L, TimeUnit.SECONDS,
&nbsp; &nbsp;&nbsp;new&nbsp;LinkedBlockingQueue&lt;&gt;(500),
&nbsp; &nbsp;&nbsp;new&nbsp;ThreadFactoryBuilder()
&nbsp; &nbsp; &nbsp; &nbsp; .setNameFormat("cpu-pool-%d")
&nbsp; &nbsp; &nbsp; &nbsp; .setPriority(Thread.MAX_PRIORITY)&nbsp;// 提高优先级
&nbsp; &nbsp; &nbsp; &nbsp; .build(),
&nbsp; &nbsp;&nbsp;new&nbsp;ThreadPoolExecutor.AbortPolicy()&nbsp;// 直接拒绝
);
// 使用示例
CompletableFuture.supplyAsync(() -&gt; {
&nbsp; &nbsp;&nbsp;// 矩阵计算等CPU密集型任务
&nbsp; &nbsp;&nbsp;double[][] result = matrixMultiply(largeMatrixA, largeMatrixB);
&nbsp; &nbsp;&nbsp;return&nbsp;result;
}, cpuIntensiveExecutor)
.thenAccept(result -&gt; {
&nbsp; &nbsp; System.out.println("计算结果维度: "&nbsp;+ result.length +&nbsp;"x"&nbsp;+ result[0].length);
});</code></pre>
<p><strong>线程池隔离小结</strong>：</p>
<p>专池专用的本质是通过物理隔离实现：</p>
<ul>
<li>资源保障 ：关键业务独占线程资源</li>
<li>故障隔离 ：避免级联雪崩</li>
<li>性能优化 ：针对任务类型最大化吞吐量</li>
</ul>
<p>最终呈现的效果是像专业厨房的分区（切配区/炒菜区/面点区）一样，让每个线程池专注处理同类任务，提升整体效率和可靠性。</p>
<h1>六、总结</h1>
<p>线程池是Java并发编程的核心组件，通过复用线程减少资源开销，提升系统吞吐量。其核心设计包括线程复用机制 、任务队列和拒绝策略 ，通过ThreadPoolExecutor的参数（核心线程数、最大线程数、队列容量等）实现灵活的资源控制。线程池的生命周期由RUNNING、SHUTDOWN等状态管理，确保任务有序执行或终止。</p>
<p>内置线程池（如Executors.newCachedThreadPool）虽便捷，但存在内存溢出或无界队列堆积的风险，需谨慎选择。invokeAll的超时失效和submit提交任务的异常消失是常见陷阱需通过正确处理中断和检查Future.get()规避。</p>
<p>最佳实践包括：</p>
<ul>
<li>异常处理：通过afterExecute来对发生的异常进行兜底处理，任务细粒度的try catch或UncaughtExceptionH捕获异常处理防止线程崩溃退出；</li>
<li>拒绝策略：根据业务选择拒绝策略或自定义降级逻辑，生产级应用建议尽量自定义处理；</li>
<li>线程隔离 ：按任务类型（CPU/I/O）或优先级划分线程池，避免资源竞争。</li>
</ul>
<p>合理使用线程池能显著提升性能，但需结合业务场景精细调参，确保稳定性和可维护性，希望这篇文章能给大家带来一些生产实践上的指导，减少一些因为不熟悉线程池相关原理生产误用导致的一些问题。</p>
<h4>往期回顾</h4>
<p>1.&nbsp;基于浏览器扩展 API Mock 工具开发探索｜得物技术</p>
<p>2.&nbsp;破解gh-ost变更导致MySQL表膨胀之谜｜得物技术</p>
<p>3.&nbsp;MySQL单表为何别超2000万行？揭秘B+树与16KB页的生死博弈｜得物技术</p>
<p>4.&nbsp;0基础带你精通Java对象序列化--以Hessian为例｜得物技术</p>
<p>5.&nbsp;前端日志回捞系统的性能优化实践｜得物技术</p>
<h4>文 /舍得</h4>
<p>关注得物技术，每周更新技术干货</p>
<p>要是觉得文章对你有帮助的话，欢迎评论转发点赞～</p>
<p>未经得物技术许可严禁转载，否则依法追究法律责任。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<h1>一、引 言</h1>
<p><strong>为什么进行源码角度的深度解析？</strong></p>
<p>大家在项目中到处都在使用线程池做一些性能接口层次的优化，原先串行的多个远程调用，因为rt过高，通过线程池批量异步优化，从而降低rt。还有像RocketMQ中broker启动时，同时通过ScheduledThreadPoolExecutor线程池执行其他组件的定时任务，每隔一段时间处理相关的任务。线程池广泛的应用在外面各种实际开发场景中，我们很多同学可能在项目里只是简单的copy了一些前人的代码参数并不知道其中的含义，从而导致生产级别的bug。所以本篇文章，旨在帮助还不熟悉或者想要熟悉线程池的同学，分享我自己在学习线程池源码上的一些内容来更简单、快速的掌握线程池。</p>
<h1>二、为什么使用线程池？</h1>
<p>并发编程中，对于常见的操作系统，线程都是执行任务的基本单元，如果每次执行任务时都创建新的线程，任务执行完毕又进行销毁，会出现以下的问题：</p>
<ul>
<li><strong>资源开销</strong>：比如在Linux系统中，频繁的创建和销毁线程，一个是频繁的进行一个系统调用，另外是一些内存和CPU资源调度的占用。虽然有一些写时复制的策略防止lwp的创建时的内存占用，但是实际写入还是会申请系统内存的，何况一些页表等本身就有内存占用。</li>
<li><strong>性能瓶颈</strong>：线程的创建需要系统调用，如果只是简单的计算任务，可能耗时还没创建的rt高，这里反而降低了系统的吞吐量。</li>
<li><strong>缺乏资源管理</strong>：无限制的创建线程会导致内存溢出，java.lang.OutOfMemoryError: unable to create native thread，这里主要因为Java的线程其实Linux中是lwp线程，需要通过JNI进行系统调用创建，每个线程默认需要1MB的栈空间，很容易导致无休止的创建线程导致内存溢出，另外就是频繁的系统调用，导致的上下文切换，占用了过多的CPU，反而起到了相反的作用。</li>
<li><strong>功能受限</strong>：手动管理线程难以实现更高级的功能，如定时任务、周期任务、任务管理、并发任务数的控制等。</li>
</ul>
<p>通过上面的问题，我们其实可以清晰的感知到这些问题都是归拢到资源没有得到合理的分配和控制导致的，线程池出现的核心宗旨其实就是对资源的合理分配和控制。除了线程池，其实更多的也接触过数据库连接池、netty的对象池等池化技术，这些池化思想其实都是为了更好的降低资源的消耗以及更好的进行资源管理。</p>
<h1>三、JDK线程池的架构设计</h1>
<h1>3.1 JUC并发包下的Executor框架的uml类图</h1>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-10e0261f0e.jpg">
</div>
<ul>
<li><strong>Executor</strong>：任务执行的顶层接口，主要是分离任务提交与执行逻辑，支持同步/异步执行，遵循Java内存模型的&nbsp;happen-before规则。</li>
<li><strong>ExecutorService</strong>：继承Executor接口，提供了更完善的生命周期管理能力，通过Future对象提供任务取消、状态查询、结果获取能力实现了任务监控。</li>
<li><strong>AbstractExecutorService</strong>：常见的设计模式为了简化线程池的开发，通常通过父类进行一些基础的默认实现让子类继承。</li>
<li><strong>ScheduledExecutorService</strong>：ExecutorService的扩展接口，支持延迟执行和周期性任务调度。</li>
<li><strong>ThreadPoolExecutor</strong>：是ExecutorService接口最核心和最常用的实现类，它提供了高度可配置的线程池，允许我们精细控制线程池的各种行为。</li>
<li><strong>ScheduledThreadPoolExecutor</strong>：是ScheduledExecutorService接口的实现类，它继承自ThreadPoolExecutor，专门用于处理定时和周期任务。</li>
<li><strong>Executors</strong>：一个静态工厂模式的工具类，提供了一系列静态方法来创建各种常见配置的线程池，newFixedThreadPool(), newCachedThreadPool(),等，简化了创建线程池的使用但是会带来一些问题，很多开发规范里都不建议大家直接使用。JDK内置的线程池如果我们不熟悉里面的参数很有可能导致出乎自己意料的结果，池大小设置、阻塞队列选择等等都是有考究的，这一点后续会进行一些详细说明。生产环境中建议谨慎使用或直接使用ThreadPoolExecutor构造函数自定义。</li>
</ul>
<h1>3.2 ThreadPoolExecutor的参数解析</h1>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2b578a85a8.jpg">
</div>
<ul>
<li><strong>corePoolSize&nbsp;</strong>核心线程数：
<ul>
<li>线程池中还未退出的alive的核心线程数量。</li>
<li>虽然线程处于空闲状态（其实是阻塞在阻塞队列中），除非显示设置了allowCoreThreadTimeOut=true，否则这些线程不会从自己的run方法中退出被回收。</li>
<li>添加新任务时，如果当前工作线程小于coreSize，此时即使存在空闲的core线程，线程池也会通过addWorker方法创建一个新的线程。</li>
<li>&nbsp;</li>
</ul> </li>
<li><strong>maximumPoolSize&nbsp;</strong>最大线程数：
<ul>
<li>线程池可以创建的最大线程数。</li>
<li>如果是有界队列，当队列满时，仍然有任务进来，此时线程池会创建小于最大线程数的线程来完成任务，空闲。</li>
<li>如果是无界队列，那么永远不会出现第二点的情况，除了内存异常，否则会一直保持核心线程数，多余的任务会一直往队列中加入。</li>
</ul> </li>
</ul>
<ul>
<li><strong>keepAliveTime</strong>&nbsp;线程空闲存活时间
<ul>
<li>线程数超过corePoolSize后创建的线程我们理解为非核心线程，对于这类线程，他的回收机制在于我们设置的keepAliveTime,线程会限期阻塞在队列中获取任务，如果超时未获取就会进行清理并退出。</li>
<li>另外如果设置allowCoreThreadTimeOut=true，所谓的核心线程在空闲时间达到keepAliveTime时也会被回收。</li>
</ul> </li>
</ul>
<ul>
<li><strong>unit&nbsp;</strong>时间单位
<ul>
<li>keepAliveTime参数的时间单位，TimeUnit中时分秒等。</li>
<li>&nbsp;</li>
</ul> </li>
<li><strong>workQueue&nbsp;</strong>任务队列
<ul>
<li>阻塞队列，核心线程数满时，新加入的任务，会先添加到阻塞队列中等待线程获取任务并执行。</li>
<li>常用的BlockingQueue实现有：
<ul>
<li>ArrayBlockingQueue：数组实现的先进先出原则的有界阻塞队列，构造方法必须指定容量。</li>
<li>LinkedBlockingQueue：链表实现的阻塞队列，构造传入容量则有界，未传则是无界队列，此时设置的最大线程数其实就不会有作用了。</li>
<li>SynchronousQueue：一个不存储元素的阻塞队列。每个put操作必须等待一个take操作，反之亦然。它相当于一个传递通道，非常适合传递性需求，吞吐量高，但要求maximumPoolSize足够大。</li>
<li>PriorityBlockingQueue：二叉堆实现的优先级阻塞队列，构造时可自行调整排序行为（小顶堆或大顶堆）。</li>
<li>DelayQueue：支持延时的无界阻塞队列，主要用于周期性的任务，我们可以直接通过它来实现一些简单的延迟任务需求，复杂的周期性任务建议使用ScheduledThreadPoolExecutor。</li>
</ul> </li>
</ul> </li>
</ul>
<ul>
<li><strong>threadFactory&nbsp;</strong>线程工厂
<ul>
<li>用于创建新线程的工厂。通过自定义ThreadFactory，我们可以为线程池中的线程设置更有意义的名称、设置守护线程状态、设置线程优先级、指定UncaughtExceptionHandler等。</li>
<li>Executors.defaultThreadFactory()是默认实现。</li>
</ul> </li>
</ul>
<ul>
<li><strong>handler&nbsp;</strong>拒绝策略
<ul>
<li>ThreadPoolExecutor.AbortPolicy：默认的拒绝策略，简单粗暴，当execute中添加woker失败时，直接在当前线程抛出异常。</li>
<li>JDK内置了四种拒绝策略：
<ul>
<li>ThreadPoolExecutor.AbortPolicy：默认的拒绝策略，简单粗暴，当execute中添加woker失败时，直接在当前线程抛出异常。</li>
<li>ThreadPoolExecutor.CallerRunsPolicy：提交任务的线程，直接执行任务。变相的背压机制，可以降低任务往线程中加入。</li>
<li>ThreadPoolExecutor.DiscardPolicy：直接丢弃被拒绝的任务，不做任何通知，需容忍数据丢失。</li>
<li>ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列中最旧的任务，然后重试提交当前任务，需容忍数据丢失。</li>
<li>实现RejectedExecutionHandler接口自定义拒绝策略，在实际生产应用中推荐使用，可以做一些打印观察日志的操作，告警、兜底的相关处理等。</li>
</ul> </li>
</ul> </li>
</ul>
<h1>3.3 运行机制详解</h1>
<p>新任务通过execute()方法提交给ThreadPoolExecutor时，其处理流程如下：</p>
<p><strong>判断核心线程数</strong>：如果当前运行的线程数小于corePoolSize，则创建新线程（即使有空闲的核心线程）来执行任务。</p>
<p><strong>尝试入队</strong>：如果当前运行的线程数大于或等于corePoolSize，则尝试将任务添加到workQueue中。</p>
<ul>
<li>如果workQueue.offer()成功（队列未满），任务入队等待执行。</li>
</ul>
<p><strong>尝试创建非核心线程</strong>：如果workQueue.offer()失败（队列已满）：</p>
<ul>
<li>判断当前运行的线程数是否小于maximumPoolSize；</li>
<li>如果是，则创建新的非核心线程来执行任务。</li>
</ul>
<p><strong>执行拒绝策略：</strong></p>
<p>如果当前运行的线程数也达到了maximumPoolSize（即核心线程和非核心线程都已用尽，且队列也满了），则执行RejectedExecutionHandler所定义的拒绝策略。</p>
<p>参考网络中的经典执行图：</p>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-88df05dbfd.jpg">
</div>
<p>这个图能很好的表明运行原理，但是忽略了很多细节，比如所谓的缓冲执行是在什么条件下去走的呢？直接执行又是什么逻辑下执行呢？最后的任务拒绝又是怎么回事？带着这些疑问点，我们直接来进行一个源码级别的分析：</p>
<h1>execute核心流程的源码分析</h1>
<pre><code>public&nbsp;void&nbsp;execute(Runnable command)&nbsp;{
&nbsp; &nbsp;&nbsp;if&nbsp;(command ==&nbsp;null)
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;NullPointerException();
&nbsp; &nbsp;&nbsp;//线程池状态 高3位表示线程状态 低29位代表线程数量
&nbsp; &nbsp;&nbsp;int&nbsp;c = ctl.get();
&nbsp; &nbsp;&nbsp;//判断当前线程池线程数量是否小于核心线程数
&nbsp; &nbsp;&nbsp;if&nbsp;(workerCountOf(c) &lt; corePoolSize) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//作为核心线程数进行线程的创建，并且创建成功线程会将command的任务执行--》对应图上的直接执行
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(addWorker(command,&nbsp;true))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; c = ctl.get();
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;//创建核心线程失败或者当前线程数量超过核心线程数
&nbsp; &nbsp;&nbsp;//当前线程池是否还在运行状态，尝试将任务添加到阻塞队列 --》对应图上的缓冲执行
&nbsp; &nbsp;&nbsp;//BlockingQueue队列的顶级抽象定义了offer不是进行阻塞添加而是立即返回，添加失败直接返回false，区别于put
&nbsp; &nbsp;&nbsp;if&nbsp;(isRunning(c) &amp;&amp; workQueue.offer(command)) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//重新获取线程池标志位
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;recheck = ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//如果线程此时不在运行状态中，那么将任务删除
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(! isRunning(recheck) &amp;&amp;&nbsp;remove(command))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//删除任务成功，走拒绝策略拒绝掉当前任务
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; reject(command);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(workerCountOf(recheck) ==&nbsp;0)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//如果线程池中的工作线程都没有的时候，这里需要创建一个线程去执行添加到队列中的任务
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//防止因为并发的原因工作线程都被终止掉了，此时任务在阻塞队列里等着，缺没有工作线程
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addWorker(null,&nbsp;false);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;//到这里那就是添加队列失败，或者线程池状态异常，但是这里仍然尝试进行创建一个worker
&nbsp; &nbsp;&nbsp;//如果创建失败，也是走拒绝策略拒绝当前任务
&nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(!addWorker(command,&nbsp;false))
&nbsp; &nbsp; &nbsp; &nbsp; reject(command);
}</code></pre>
<p>接下来我们仔细看看addWorker这个方法具体是在做什么：</p>
<pre><code>//核心逻辑其实就是在无限循环创建一个worker，创建失败直接返回，创建成功，则将worker执行
// 因为worker有thread的成员变量，最终添加worker成功，会启动线程的start方法
//start方法最终会回调到外层的runWorker方法，改方法会不停的从阻塞队列里以阻塞的take方式
//获取任务，除非达到能被终止的条件，此时当前线程会终止
private&nbsp;boolean&nbsp;addWorker(Runnable firstTask,&nbsp;boolean&nbsp;core)&nbsp;{
&nbsp; &nbsp; retry:
&nbsp; &nbsp;&nbsp;for&nbsp;(;;) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;c&nbsp;=&nbsp;ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;rs&nbsp;=&nbsp;runStateOf(c);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Check if queue empty only if necessary.
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(rs &gt;= SHUTDOWN &amp;&amp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ! (rs == SHUTDOWN &amp;&amp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;firstTask ==&nbsp;null&nbsp;&amp;&amp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;! workQueue.isEmpty()))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;false;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(;;) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;wc&nbsp;=&nbsp;workerCountOf(c);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(wc &gt;= CAPACITY ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; wc &gt;= (core ? corePoolSize : maximumPoolSize))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;false;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//不停的重试添加worker的计数，只有添加成功的才会进行后续的worker启动
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(compareAndIncrementWorkerCount(c))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;break&nbsp;retry;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; c = ctl.get(); &nbsp;// Re-read ctl
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//重试期间，如果其他线程导致线程池状态不一致了。重新回到第一个循环进行check判断
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(runStateOf(c) != rs)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;continue&nbsp;retry;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// else CAS failed due to workerCount change; retry inner loop
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;boolean&nbsp;workerStarted&nbsp;=&nbsp;false;
&nbsp; &nbsp;&nbsp;boolean&nbsp;workerAdded&nbsp;=&nbsp;false;
&nbsp; &nbsp;&nbsp;Worker&nbsp;w&nbsp;=&nbsp;null;
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; w =&nbsp;new&nbsp;Worker(firstTask);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;final&nbsp;Thread&nbsp;t&nbsp;=&nbsp;w.thread;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(t !=&nbsp;null) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//这里加锁一个是workers.add时需要加锁，另外是防止其他线程已经在尝试修改线程池状态了
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock&nbsp;mainLock&nbsp;=&nbsp;this.mainLock;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Recheck while holding lock.
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Back out on ThreadFactory failure or if
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// shut down before lock acquired.
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;rs&nbsp;=&nbsp;runStateOf(ctl.get());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(rs &lt; SHUTDOWN ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (rs == SHUTDOWN &amp;&amp; firstTask ==&nbsp;null)) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(t.isAlive())&nbsp;// precheck that t is startable
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;IllegalThreadStateException();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//将worker的引用添加到workers的hashSet中
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; workers.add(w);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;s&nbsp;=&nbsp;workers.size();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//更新线程池此时最大的线程数
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(s &gt; largestPoolSize)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; largestPoolSize = s;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; workerAdded =&nbsp;true;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//如果添加成功，就启动worker中的线程
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(workerAdded) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; t.start();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; workerStarted =&nbsp;true;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//这里添加失败的话，需要把线程池的count数进行--，并且要把worker引用从hashSer中移除
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(! workerStarted)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addWorkerFailed(w);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;return&nbsp;workerStarted;
}</code></pre>
<h1>3.4 线程池的生命周期</h1>
<p>在介绍运行机制原理的源码分析时，其实是有提到线程池状态这个概念的。介绍这个状态其实也是让大家更方便的去管理线程池，比如我们关闭线程池时，怎么去优雅的关闭，使用不同的方法可能会有不同的效果，我们需要根据自己的业务场景去酌情分析、权衡使用。</p>
<pre><code>//线程池的状态和计数采用一个Integer变量设置的
//这里之所以用一个变量来储存状态和数量，其实很有讲究的，因为我们在上面的运行原理上可以看到
//源码中有大量的进行状态以及数量的判断，如果分开采用变量的记录的话，在维护二者一致性方面
//可能就需要加锁的维护成本了，而且计算中都是位移运算也是非常高效的
private&nbsp;final&nbsp;AtomicInteger&nbsp;ctl&nbsp;=&nbsp;new&nbsp;AtomicInteger(ctlOf(RUNNING,&nbsp;0));
//线程池的大小由ctl低29位表示，现成状态由ctl高3位表示
private&nbsp;static&nbsp;final&nbsp;int&nbsp;COUNT_BITS&nbsp;=&nbsp;Integer.SIZE -&nbsp;3;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;CAPACITY&nbsp; &nbsp;=&nbsp;(1&nbsp;&lt;&lt; COUNT_BITS) -&nbsp;1;
// 线程池的状态通过简单的位移就能计算出来，状态只能从低到高流转，不能逆向
private&nbsp;static&nbsp;final&nbsp;int&nbsp;RUNNING&nbsp; &nbsp;&nbsp;=&nbsp;-1&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;SHUTDOWN&nbsp; &nbsp;=&nbsp;&nbsp;0&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;STOP&nbsp; &nbsp; &nbsp; &nbsp;=&nbsp;&nbsp;1&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;TIDYING&nbsp; &nbsp;&nbsp;=&nbsp;&nbsp;2&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;TERMINATED&nbsp;=&nbsp;&nbsp;3&nbsp;&lt;&lt; COUNT_BITS;
// 这里是获取线程状态以及获取线程数量的简单高效的位移方法
private&nbsp;static&nbsp;int&nbsp;runStateOf(int&nbsp;c)&nbsp; &nbsp; &nbsp;{&nbsp;return&nbsp;c &amp; ~CAPACITY; }
private&nbsp;static&nbsp;int&nbsp;workerCountOf(int&nbsp;c)&nbsp; {&nbsp;return&nbsp;c &amp; CAPACITY; }
private&nbsp;static&nbsp;int&nbsp;ctlOf(int&nbsp;rs,&nbsp;int&nbsp;wc)&nbsp;{&nbsp;return&nbsp;rs | wc; }</code></pre>
<p>接下来结合源码详细介绍下线程池的5种状态以及分别有什么不同的表现行为？</p>
<pre><code>先说下结论：
RUNNING&nbsp; &nbsp; &nbsp;这个就是线程池运行中状态，我们可以添加任务也可以处理阻塞队列任务
SHUTDOWN &nbsp; 不能添加新的任务，但是会将阻塞队列中任务执行完毕
STOP &nbsp; &nbsp; &nbsp; 不能添加新的任务，执行中的线程也会被打断，也不会处理阻塞队列的任务
TIDYING &nbsp; &nbsp;所有线程都被终止，并且workCount=0时会被置为的状态
TERMINATED &nbsp; 调用完钩子方法terminated()被置为的状态&nbsp;</code></pre>
<h1>shutdown状态源码分析：</h1>
<pre><code>&nbsp;
&nbsp;//线程池关闭
&nbsp;public&nbsp;void&nbsp;shutdown()&nbsp;{
&nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock&nbsp;mainLock&nbsp;=&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; checkShutdownAccess();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//循环cas设置线程池状态，直到成功或状态已经state&gt;=SHUTDOWN
&nbsp; &nbsp; &nbsp; &nbsp; advanceRunState(SHUTDOWN);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//这个是真正得出结论的地方
&nbsp; &nbsp; &nbsp; &nbsp; interruptIdleWorkers();
&nbsp; &nbsp; &nbsp; &nbsp; onShutdown();&nbsp;// hook for ScheduledThreadPoolExecutor
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
&nbsp; &nbsp; tryTerminate();
}
private&nbsp;void&nbsp;interruptIdleWorkers()&nbsp;{
&nbsp; &nbsp; interruptIdleWorkers(false);
}
//打断空闲的线程，如何判断线程是否空闲还是运行？
private&nbsp;void&nbsp;interruptIdleWorkers(boolean&nbsp;onlyOne)&nbsp;{
&nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock&nbsp;mainLock&nbsp;=&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(Worker w : workers) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Thread&nbsp;t&nbsp;=&nbsp;w.thread;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//worker的线程没有被打断过，并且能获取到worker的aqs独占锁
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!t.isInterrupted() &amp;&amp; w.tryLock()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//打断当前线程，如果线程在阻塞队列中阻塞，此时会被中断
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; t.interrupt();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(SecurityException ignore) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; w.unlock();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(onlyOne)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;break;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
}
</code></pre>
<h1>STOP状态分析</h1>
<pre><code>//循环cas修改线程池状态为stop。打断所有线程，取出阻塞队列的所有任务
public&nbsp;List&lt;Runnable&gt;&nbsp;shutdownNow()&nbsp;{
&nbsp; &nbsp; List&lt;Runnable&gt; tasks;
&nbsp; &nbsp; final ReentrantLock mainLock =&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//检查线程的权限
&nbsp; &nbsp; &nbsp; &nbsp; checkShutdownAccess();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//将状态case为stop
&nbsp; &nbsp; &nbsp; &nbsp; advanceRunState(STOP);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//打断所有worker不管是不是正在执行任务
&nbsp; &nbsp; &nbsp; &nbsp; interruptWorkers();
&nbsp; &nbsp; &nbsp; &nbsp; tasks = drainQueue();
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
&nbsp; &nbsp; tryTerminate();
&nbsp; &nbsp;&nbsp;return&nbsp;tasks;
}
//这里获取锁之后。打断了所有的线程
private&nbsp;void&nbsp;interruptWorkers()&nbsp;{
&nbsp; &nbsp; final ReentrantLock mainLock =&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(Worker w : workers)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; w.interruptIfStarted();
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
}</code></pre>
<h1>TIDYING、TERMINATED&nbsp;状态分析</h1>
<pre><code>//这个方法在每个线程退出时都会进行调用，如果是运行中、或者状态大于等于TIDYING或者shutdown但是队列不为空都
//直接返回，如果不满足以上条件，并且线程数不为0的话，打断一个空闲线程
final&nbsp;void tryTerminate() {
&nbsp; &nbsp;&nbsp;for&nbsp;(;;) {
&nbsp; &nbsp; &nbsp; &nbsp; int c = ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(isRunning(c) ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; runStateAtLeast(c, TIDYING) ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty()))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(workerCountOf(c) !=&nbsp;0) {&nbsp;// Eligible to terminate
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; interruptIdleWorkers(ONLY_ONE);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//此时到这里，状态要么为STOP。要么是shutdown并且队列为空了
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 获取一个锁，尝试cas修改状态为TIDYING
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//调用terminated()的钩子方法，
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//修改线程池为终态TERMINATED，并且唤醒阻塞在termination队列上的线程
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock mainLock =&nbsp;this.mainLock;
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ctl.compareAndSet(c, ctlOf(TIDYING,&nbsp;0))) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; terminated();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ctl.set(ctlOf(TERMINATED,&nbsp;0));
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; termination.signalAll();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// else retry on failed CAS
&nbsp; &nbsp; }
}</code></pre>
<h1>四、JDK内置线程池的问题</h1>
<p>java.util.concurrent.Executors工厂类提供了一些静态方法，方便我们快速创建几种预设配置的线程池：</p>
<ul>
<li>Executors.newFixedThreadPool(int nThreads)：
<ul>
<li>创建一个固定大小的线程池。corePoolSize和maximumPoolSize都等于nThreads。</li>
<li>keepAliveTime为0L（因为线程数不会超过corePoolSize，所以此参数无效，除非allowCoreThreadTimeOut为true）。</li>
<li>使用无界的LinkedBlockingQueue作为工作队列。</li>
<li><strong>问题</strong>：由于使用无界队列，当任务提交速度远大于处理速度时，队列会持续增长，可能导致内存溢出（OOM）。此时maximumPoolSize参数实际上是无效的，线程数永远不会超过nThreads。</li>
</ul> </li>
</ul>
<ul>
<li>Executors.newSingleThreadExecutor()：
<ul>
<li>创建一个只有一个工作线程的线程池。corePoolSize和maximumPoolSize都为1。</li>
<li>同样使用<strong>无界</strong>的LinkedBlockingQueue。</li>
<li>保证所有任务按照提交顺序（FIFO）执行。</li>
<li><strong>问题</strong>：与newFixedThreadPool类似，无界队列可能导致OOM。</li>
</ul> </li>
</ul>
<ul>
<li>Executors.newCachedThreadPool()：
<ul>
<li>创建一个可缓存的线程池。</li>
<li>corePoolSize为0。</li>
<li>maximumPoolSize为Integer.MAX_VALUE (几乎是无界的)。</li>
<li>keepAliveTime为60秒。</li>
<li>使用SynchronousQueue作为工作队列。这种队列不存储元素，任务提交后必须有空闲线程立即接收，否则会创建新线程（如果未达到maximumPoolSize）。</li>
<li><strong>问题</strong>：如果任务提交速度过快，会创建大量线程（理论上可达Integer.MAX_VALUE个），可能耗尽系统资源，导致OOM以及频繁的上下文切换。</li>
</ul> </li>
</ul>
<ul>
<li>Executors.newSingleThreadScheduledExecutor()、Executors.newScheduledThreadPool(int corePoolSize):
<ul>
<li>创建用于调度任务的线程池。</li>
<li>内部使用ScheduledThreadPoolExecutor实现，其任务队列是DelayedWorkQueue (一种特殊的PriorityQueue)。</li>
<li>newSingleThreadScheduledExecutor的corePoolSize为1，maximumPoolSize为Integer.MAX_VALUE（但由于队列是DelayedWorkQueue，通常不会无限增长线程，除非有大量同时到期的任务且处理不过来）。</li>
<li>newScheduledThreadPool可以指定corePoolSize。</li>
<li><strong>问题</strong>：虽然DelayedWorkQueue本身是无界的，但ScheduledThreadPoolExecutor在任务执行逻辑上与普通ThreadPoolExecutor有所不同。主要风险仍然是如果corePoolSize设置不当，且大量任务同时到期并执行缓慢，可能导致任务积压。</li>
</ul> </li>
</ul>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a0860059e7.jpg">
</div>
<p>某一线互联网Java开发手册</p>
<h1>五、线程池中的问题与最佳实践</h1>
<h1>5.1 invokeAll 超时机制无效？</h1>
<p>ExecutorService.invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit)方法会提交一组Callable任务，并等待所有任务完成，或者直到超时。如果超时发生，它会尝试取消（中断）所有尚未完成的任务，然后返回一个List&lt;Future&gt;。</p>
<p><strong>失效场景分析：</strong></p>
<ul>
<li>任务不响应中断（最常见）：任务内部捕获&nbsp;InterruptedException&nbsp;后静默处理，或执行不检查中断状态的阻塞操作（如循环计算）：</li>
</ul>
<pre><code>Callable&lt;String&gt; task = () -&gt; {
&nbsp; &nbsp;&nbsp;while&nbsp;(true) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//缺少此检查将导致超时失效
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(Thread.interrupted())&nbsp;break;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 耗时计算...
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;return&nbsp;"done";
};</code></pre>
<ul>
<li><strong>使用非响应中断的API</strong>：任务调用了不响应&nbsp;interrupt()&nbsp;的第三方库或JNI代码（如某些IO操作）</li>
</ul>
<pre><code>Callable&lt;Integer&gt; task&nbsp;=&nbsp;() -&gt; {
&nbsp; &nbsp;&nbsp;Files.copy(in, path);&nbsp;// 某些NIO操作不响应中断
&nbsp; &nbsp;&nbsp;return&nbsp;1;
};</code></pre>
<ul>
<li><strong>任务依赖外部资源阻塞</strong>：任务因外部资源（如数据库连接、网络请求）阻塞且未设置超时。</li>
</ul>
<pre><code>Callable&lt;Result&gt; task&nbsp;=&nbsp;() -&gt; {
&nbsp; &nbsp;&nbsp;//未设查询超时时间
&nbsp; &nbsp;&nbsp;return&nbsp;jdbcTemplate.query("SELECT * FROM large_table");&nbsp;
};</code></pre>
<ul>
<li><strong>线程池配置缺陷</strong>：核心线程数过大或队列无界，导致&nbsp;invokeAll&nbsp;超时前任务无法全部启动，任务堆积在队列，invokeAll&nbsp;超时后仍有大量任务未执行。</li>
</ul>
<pre><code>new&nbsp;ThreadPoolExecutor(
&nbsp; &nbsp;&nbsp;100,&nbsp;100,&nbsp;// 核心线程数过大
&nbsp; &nbsp; 0L,&nbsp;TimeUnit.MILLISECONDS,
&nbsp; &nbsp;&nbsp;new&nbsp;LinkedBlockingQueue&lt;&gt;()&nbsp;// 无界队列
);</code></pre>
<p><strong>invokeAll超时失效demo:</strong></p>
<pre><code>import&nbsp;java.util.*;
import&nbsp;java.util.concurrent.*;
public&nbsp;class&nbsp;InvokeAllTimeoutDemo&nbsp;{
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;// 模拟耗时任务（可配置是否响应中断）
&nbsp; &nbsp;&nbsp;static&nbsp;class&nbsp;Task&nbsp;implements&nbsp;Callable&lt;String&gt; {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;int&nbsp;id;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;long&nbsp;durationMs;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;boolean&nbsp;respectInterrupt;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; Task(int&nbsp;id,&nbsp;long&nbsp;durationMs,&nbsp;boolean&nbsp;respectInterrupt) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.id = id;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.durationMs = durationMs;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.respectInterrupt = respectInterrupt;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;String&nbsp;call()&nbsp;throws&nbsp;Exception {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.printf("Task %d started%n", id);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;start&nbsp;=&nbsp;System.currentTimeMillis();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 模拟工作（检查中断状态）
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;while&nbsp;(System.currentTimeMillis() - start &lt; durationMs) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(respectInterrupt &amp;&amp; Thread.interrupted()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;InterruptedException("Task "&nbsp;+ id +&nbsp;" interrupted");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 不响应中断的任务会继续执行
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.printf("Task %d completed%n", id);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;"Result-"&nbsp;+ id;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;public&nbsp;static&nbsp;void&nbsp;main(String[] args)&nbsp;throws&nbsp;InterruptedException {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;ExecutorService&nbsp;executor&nbsp;=&nbsp;Executors.newFixedThreadPool(2);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; List&lt;Callable&lt;String&gt;&gt; tasks = Arrays.asList(
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;new&nbsp;Task(1,&nbsp;2000,&nbsp;true), &nbsp;&nbsp;// 2秒，响应中断
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;new&nbsp;Task(2,&nbsp;10000,&nbsp;false) &nbsp;// 10秒，不响应中断
&nbsp; &nbsp; &nbsp; &nbsp; );
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Invoking with 3s timeout...");
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//设置3秒超时
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; List&lt;Future&lt;String&gt;&gt; futures = executor.invokeAll(tasks,&nbsp;3, TimeUnit.SECONDS);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(Future&lt;String&gt; f : futures) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 明确处理取消状态
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(f.isCancelled()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Task was cancelled"); &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Result: "&nbsp;+ f.get(100, TimeUnit.MILLISECONDS));
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(TimeoutException | ExecutionException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Task failed: "&nbsp;+ e.getCause());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.shutdownNow();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Executor shutdown");
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p>当我们使用invokeAll(tasks, timeout)&nbsp;提交多个任务时，如果出现某个任务对中断不响应或者响应不及时，那我们即使设置了超时时间，不响应中断的任务2仍在后台运行（即使调用了&nbsp;shutdownNow()）</p>
<h1>5.2 submit()的异常消失了？</h1>
<p>使用ExecutorService.submit()提交任务时，任务执行过程中如果抛出未捕获的异常（无论是受检异常还是运行时异常），这个异常会被Future的包装类如FutureTask重写的run()方法捕获并封装在返回的Future包装对象的成员变量中。</p>
<ul>
<li><strong>不显示调用</strong>Future.get()，该异常我们就无法感知，好像没有发生过一样。线程池的工作线程本身通常会有一个默认的未捕获异常处理器，可能会打印堆栈到控制台，但你的主业务逻辑不会知道。</li>
<li><strong>显示调用</strong>Future.get()，抛出声明式的ExecutionException，其cause属性才是原始的任务异常。</li>
<li>如果调用Future.get(long timeout, TimeUnit unit)超时，向外抛出声明式的TimeoutException。此时任务可能仍在后台执行，可能错过了内部的异常。</li>
</ul>
<p><strong>submit()异常消失demo:</strong></p>
<pre><code>public&nbsp;class&nbsp;ThreadPoolExceptionDemo&nbsp;{
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;public&nbsp;static&nbsp;void&nbsp;main(String[] args)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 创建单线程线程池（便于观察异常）
&nbsp; &nbsp; &nbsp; &nbsp; ExecutorService executor = Executors.newSingleThreadExecutor();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 场景1：Callable抛出异常（通过Future.get()捕获）
&nbsp; &nbsp; &nbsp; &nbsp; Future&lt;String&gt; future1 = executor.submit(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("[Callable] 开始执行");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.sleep(100);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;RuntimeException("Callable故意抛出的异常");
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Callable结果: "&nbsp;+ future1.get());
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(ExecutionException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.err.println("捕获到Callable异常: "&nbsp;+ e.getCause().getMessage());
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(InterruptedException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.currentThread().interrupt();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 场景2：Runnable抛出异常（同样通过Future.get()捕获）
&nbsp; &nbsp; &nbsp; &nbsp; Future&lt;?&gt; future2 = executor.submit(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("[Runnable] 开始执行");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;IllegalArgumentException("Runnable故意抛出的异常");
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; future2.get();&nbsp;// Runnable成功时返回null
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(ExecutionException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.err.println("捕获到Runnable异常: "&nbsp;+ e.getCause().getMessage());
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(InterruptedException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.currentThread().interrupt();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 场景3：未处理的任务异常（需设置异常处理器）
&nbsp; &nbsp; &nbsp; &nbsp; executor.submit(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("[未捕获的任务] 开始执行");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;IllegalStateException("这个异常会被默认处理器处理");
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; executor.shutdown();
&nbsp; &nbsp; }
}</code></pre>
<h1>5.3 异常处理实践</h1>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-851f74dab6.jpg">
</div>
<ul>
<li><strong>Callable/Runnable catch处理异常：</strong>
<ul>
<li><strong>不要捕获Throwable或Exception然后静默处理（只打日志）</strong>。如果确实需要捕获，请考虑是否应该重新抛出（包装成业务允许的受检异常或运行时异常）。</li>
<li><strong>禁止静默处理&nbsp;</strong>InterruptedException：</li>
<li>在JDK的JUC底层源码中，我们可以看到很多声明了InterruptedException的方法，基本上都是对这类方法catch异常，要么继续往外抛出，或者处理完相关资源后，重置中断状态，<strong>绝对不要静默处理。</strong></li>
<li>如果方法没有声明InterruptedException如Runnable.run()，在catch&nbsp;InterruptedException后最好调用Thread.currentThread().interrupt()来恢复中断标记。</li>
<li><strong>正确处理中断</strong>：callable在耗时的loop任务处理中，如果出现了中断异常，因为Java代码中中断只是一种协作方式，其并没真的终止线程，所以一般都是需要我们进行一个中断标志的传递，如线程池中的shutdownNow()就依赖次机制处理。</li>
</ul> </li>
</ul>
<ul>
<li><strong>submit()执行的任务，谨慎处理Future：</strong>
<ul>
<li>使用带过期时间的future.get(long timeOut)获取结果，并要对该方法进行try cache防止其他异常抛出。</li>
<li>多个任务并行处理时，如果有下个请求依赖上个请求，务必使用get()让主线程等待这一结果执行完成后，流转到下一个异步任务。</li>
</ul> </li>
</ul>
<ul>
<li><strong>实现线程Thread的UncaughtExceptionHandler属性</strong>，在自定义的TheadFactory中通过set方法赋值：execute()方法执行时，对于没有捕获的异常使用线程组的兜底统一处理机制。</li>
</ul>
<pre><code>//自定义当前线程组创建线程的统一异常处理，类似于controller的统一异常处理机制
ThreadFactory&nbsp;myThreadFactory&nbsp;=&nbsp;new&nbsp;ThreadFactory() {
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;AtomicInteger&nbsp;atomicInteger&nbsp;=&nbsp;new&nbsp;AtomicInteger(0);
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;String&nbsp;threadNamePrefix&nbsp;=&nbsp;"myThreadFactory-";
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;public&nbsp;Thread&nbsp;newThread(Runnable r)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Thread&nbsp;t&nbsp;=&nbsp;&nbsp;new&nbsp;Thread(r,threadNamePrefix + atomicInteger.getAndIncrement());
&nbsp; &nbsp; &nbsp; &nbsp; t.setUncaughtExceptionHandler((thread, throwable) -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//异常的统一处理，日志打印、兜底处理、监控、资源释放等
&nbsp; &nbsp; &nbsp; &nbsp; System.err.println("线程["&nbsp;+ thread.getName() +&nbsp;"]异常: "&nbsp;+ throwable);});
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;t;
&nbsp; &nbsp; }
};
//构造方法时使用自定义的线程工厂
ExecutorService&nbsp;executor&nbsp;=&nbsp;new&nbsp;ThreadPoolExecutor(
&nbsp; &nbsp; corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
&nbsp; &nbsp; threadFactory,
&nbsp; &nbsp; handler
);</code></pre>
<ul>
<li><strong>使用自定义线程池时建议重写钩子方法afterExecute(Runnable r, Throwable t)</strong>：这个hook方法是用来解决当前任务线程发生的异常，默认是空实现，我们可以重写他，比如进行兜底的线程继续执行，打印日志记录，以及同步失败使用兜底异步处理等等方式。还要注意释放应用中的资源，比如文件锁的占用等，最好手动释放掉，避免底层操作系统线程对这类资源释放失败导致长期占用，最后只能重启Java进程的尴尬地步。</li>
</ul>
<pre><code>public&nbsp;class&nbsp;MyThreadPoolExecutor&nbsp;extends&nbsp;ThreadPoolExecutor&nbsp;{
&nbsp; &nbsp;&nbsp;public&nbsp;MyThreadPoolExecutor(int&nbsp;corePoolSize,&nbsp;int&nbsp;maximumPoolSize,&nbsp;long&nbsp;keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;afterExecute(Runnable r, Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;//需要特别注意任务是否为submit提交，如果是execute提交的任务，那这里很直接的知道任务是否发生异常以及后续去怎么处理
&nbsp; &nbsp; &nbsp; &nbsp;if(r&nbsp;instanceof&nbsp;Future){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(((Future&lt;?&gt;) r).isDone() || ((Future&lt;?&gt;) r).isCancelled()){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//继续使用主线程完成任务,一般不建议，最好使用兜底方式：例如异步发消息，由后续的消费组统一处理异常的任务
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; &nbsp; &nbsp;}else&nbsp;if( t !=&nbsp;null){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//execute异常处理
&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; }
}
//FutureTask 把run方法进行了重写，并且catch住了异常，所以说afterExecute的t 如果是submit提交的方式
//那么t基本上就是null
public&nbsp;void&nbsp;run()&nbsp;{
&nbsp; &nbsp;&nbsp;//....
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; Callable&lt;V&gt; c = callable;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(c !=&nbsp;null&nbsp;&amp;&amp; state == NEW) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; V result;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;boolean&nbsp;ran;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; result = c.call();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ran =&nbsp;true;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(Throwable ex) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; result =&nbsp;null;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ran =&nbsp;false;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; setException(ex);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ran)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; set(result);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp;//...
}</code></pre>
<p><strong>afterExecute可以借鉴的示例：</strong></p>
<pre><code>import&nbsp;java.util.concurrent.*;
import&nbsp;java.util.concurrent.atomic.*;
import&nbsp;org.slf4j.*;
public&nbsp;class&nbsp;RobustThreadPool&nbsp;extends&nbsp;ThreadPoolExecutor&nbsp;{
&nbsp; &nbsp;&nbsp;private&nbsp;static&nbsp;final&nbsp;Logger&nbsp;logger&nbsp;=&nbsp;LoggerFactory.getLogger(RobustThreadPool.class);
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;AtomicLong&nbsp;failureCounter&nbsp;=&nbsp;new&nbsp;AtomicLong();
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;RetryPolicy retryPolicy;&nbsp;// 重试策略
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;ThreadLocal&lt;Long&gt; startTime =&nbsp;new&nbsp;ThreadLocal&lt;&gt;();
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;public&nbsp;RobustThreadPool(int&nbsp;corePoolSize,&nbsp;int&nbsp;maxPoolSize,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; BlockingQueue&lt;Runnable&gt; workQueue,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; RetryPolicy retryPolicy) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;super(corePoolSize, maxPoolSize,&nbsp;60L, TimeUnit.SECONDS, workQueue);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.retryPolicy = retryPolicy;
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;beforeExecute(Thread t, Runnable r)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; logger.debug("开始执行任务: {}", r);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;afterExecute(Runnable r, Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 1. 异常分类处理
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(r&nbsp;instanceof&nbsp;Future){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(((Future&lt;?&gt;) r).isDone()){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//错误记录以及异常处理
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; failureCounter.incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; handleFailure(r, t, costTime);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; &nbsp; &nbsp;}else&nbsp;if( t !=&nbsp;null){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//execute异常处理
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; failureCounter.incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; handleFailure(r, t, costTime);
&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 2. 资源清理
&nbsp; &nbsp; &nbsp; &nbsp; cleanThreadLocals();
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;void&nbsp;handleFailure(Runnable r, Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 1. 异常类型识别
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(t&nbsp;instanceof&nbsp;OutOfMemoryError) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.error("JVM内存不足，终止任务: {}", t.getMessage());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.exit(1);&nbsp;// 严重错误直接终止
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 2. 可重试异常处理
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(isRetryable(t)) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;retryCount&nbsp;=&nbsp;retryPolicy.getCurrentRetryCount(r);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(retryCount &lt; retryPolicy.getMaxRetries()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.warn("任务第{}次失败，准备重试...",&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryCount +&nbsp;1, t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryPolicy.retry(r,&nbsp;this);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.error("任务超过最大重试次数({})，转入死信队列",&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryPolicy.getMaxRetries(), t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DeadLetterQueue.add(r, t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 3. 不可重试异常
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.error("不可恢复任务失败", t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Metrics.recordFailure(t.getClass());&nbsp;// 上报监控
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;boolean&nbsp;isRetryable(Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;t&nbsp;instanceof&nbsp;IOException ||&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;t&nbsp;instanceof&nbsp;TimeoutException ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(t.getCause() !=&nbsp;null&nbsp;&amp;&amp; isRetryable(t.getCause()));
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;void&nbsp;cleanThreadLocals()&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 清理可能的内存泄漏
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ThreadLocal&lt;?&gt;[] holders = {&nbsp;/* 其他ThreadLocal */};
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(ThreadLocal&lt;?&gt; holder : holders) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; holder.remove();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(Exception e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.warn("清理ThreadLocal失败", e);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;// 重试策略嵌套类
&nbsp; &nbsp;&nbsp;public&nbsp;static&nbsp;class&nbsp;RetryPolicy&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;int&nbsp;maxRetries;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;long&nbsp;retryDelayMs;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;Map&lt;Runnable, AtomicInteger&gt; retryMap =&nbsp;new&nbsp;ConcurrentHashMap&lt;&gt;();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;RetryPolicy(int&nbsp;maxRetries,&nbsp;long&nbsp;retryDelayMs)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.maxRetries = maxRetries;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.retryDelayMs = retryDelayMs;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;retry(Runnable task, Executor executor)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryMap.computeIfAbsent(task, k -&gt;&nbsp;new&nbsp;AtomicInteger()).incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(retryDelayMs &gt;&nbsp;0) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.execute(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.sleep(retryDelayMs);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(InterruptedException ignored) {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.execute(task);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.execute(task);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;int&nbsp;getCurrentRetryCount(Runnable task)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;retryMap.getOrDefault(task,&nbsp;new&nbsp;AtomicInteger()).get();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;int&nbsp;getMaxRetries()&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;maxRetries;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p><strong>异常处理小结</strong>：要特别注意使用future.get()方法时，我们一定要注意设置超时时间，防止主线程无限期的阻塞避免边缘的业务查询影响了主业务造成得不偿失的效果，另外我们需要注意一个点就是submit()方法的提交任务时，afterExecute(Runnable r, Throwable t)中的t恒为null，如果是execute方法提交的任务，那么就是直接获取的任务执行的异常，对于submit提交的任务异常其被封装到了Futrure&nbsp;包装对象中，一般需要我们再次判断任务时执行完毕还是异常或被取消了，如果发生了异常，Future.get()会抛出封装的ExecutionException异常，当然还可能是取消异常以及中断异常。invokeAll和invokeAny我们需要对返回的Future结果检查可能抛出的异常，对于callable&nbsp;前面一再强调了要对InterruptedException不要静默处理，因为线程的中断标记只是一个协作方式，他并没有停止当前线程的运行，我们需要根据自身的场景对发生的中断进行快速响应以及传递中断标志。</p>
<h1>5.4 拒绝策略实践</h1>
<p>先带大家回顾一下策略是如何触发执行的流程：</p>
<pre><code>//添加任务，当不满足条件时会执行拒绝方法reject
public&nbsp;void&nbsp;execute(Runnable command)&nbsp;{
&nbsp; &nbsp;//...
&nbsp; &nbsp;&nbsp;if&nbsp;(isRunning(c) &amp;&amp; workQueue.offer(command)) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;recheck = ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(! isRunning(recheck) &amp;&amp;&nbsp;remove(command))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; reject(command);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(workerCountOf(recheck) ==&nbsp;0)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addWorker(null,&nbsp;false);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(!addWorker(command,&nbsp;false))
&nbsp; &nbsp; &nbsp; &nbsp; reject(command);
}
//这里就是拒绝的入口。handler是有构造方法传入
final&nbsp;void&nbsp;reject(Runnable command)&nbsp;{
&nbsp; &nbsp; handler.rejectedExecution(command,&nbsp;this);
}
public&nbsp;ThreadPoolExecutor(int&nbsp;corePoolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;maximumPoolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;keepAliveTime,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TimeUnit unit,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; BlockingQueue&lt;Runnable&gt; workQueue,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ThreadFactory threadFactory,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; RejectedExecutionHandler handler) {
&nbsp; &nbsp;&nbsp;//....
&nbsp; &nbsp;&nbsp;//指定拒绝策略
&nbsp; &nbsp;&nbsp;this.handler = handler;
}</code></pre>
<p><strong>AbortPolicy</strong>：默认的拒绝策略，简单粗暴，当execute中添加woker失败时，直接在当前线程抛出异常。</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;AbortPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp; &nbsp;//直接抛出RejectedExecutionException
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;RejectedExecutionException("Task "&nbsp;+ r.toString() +
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" rejected from "&nbsp;+
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;e.toString());
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：快速失败，立即暴露系统过载问题、避免任务静默丢失，便于监控系统捕获</p>
<p><strong>缺点</strong>：需要调用方显式处理异常，增加代码复杂度，可能中断主业务流程</p>
<p><strong>适用场景</strong>：适用于那些对任务丢失非常敏感，配合熔断机制使用的快速失败场景</p>
<p><strong>CallerRunsPolicy</strong>：提交任务的线程，直接执行任务</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;CallerRunsPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;&nbsp;//直接在提交任务的线程中执行任务
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!e.isShutdown()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; r.run();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：任务都会被执行，不会丢任务，并且由于主线程执行任务，天然的流量控制，避免了大量的任务进入线程池。</p>
<p><strong>缺点</strong>：调用线程可能被阻塞，导致上游服务雪崩。不适合高并发场景（可能拖垮整个调用链）。</p>
<p><strong>适用场景</strong>：适用于处理能力不高，并且资源过载能够平滑过渡，同时不丢失任务的场景。如：低并发、高可靠性的后台任务（如日志归档）、允许同步执行的批处理系统。</p>
<p><strong>DiscardPolicy</strong>：直接丢弃被拒绝的任务，不做任何通知。</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;DiscardPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;//空实现
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：实现简单，无额外性能开销。避免异常传播影响主流程</p>
<p><strong>缺点</strong>：数据静默丢失，可能会掩盖系统容量问题</p>
<p><strong>适用场景</strong>：边缘业务的监控上报数据，统计类的uv、pv统计任务</p>
<p>DiscardOldestPolicy：丢弃队列中最旧的任务，然后重试提交当前任务。</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;DiscardOldestPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;&nbsp;//丢弃队列中最旧的任务，重试当前任务
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!e.isShutdown()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; e.getQueue().poll();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; e.execute(r);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：优先保证新任务执行，避免队列堆积导致内存溢出。</p>
<p><strong>缺点</strong>：可能丢失关键旧任务、任务执行顺序无法保证。</p>
<p><strong>适用场景</strong>：适用于可容忍部分数据丢失，并且实时性要求高于历史数据的场景，比如：行情推送。</p>
<p>通过上线的介绍，我们可以看到JDK内置策略基本上只使用于简单处理的场景，在生产实践中一般推荐我们自定义拒绝策略，进行相关的业务处理。</p>
<p><strong>1. 自定义RejectedExecutionHandler</strong>：</p>
<pre><code>/**
&nbsp;* 带监控统计的拒绝策略处理器
&nbsp;*/
public&nbsp;class&nbsp;MetricsRejectedExecutionHandler&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;&nbsp;private&nbsp;static&nbsp;final&nbsp;org.slf4j.Logger&nbsp;logger&nbsp;=&nbsp;org.slf4j.LoggerFactory.getLogger(MetricsRejectedExecutionHandler.class);
&nbsp; &nbsp;&nbsp;// 统计被拒绝的任务数量
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;AtomicLong&nbsp;rejectedCount&nbsp;=&nbsp;new&nbsp;AtomicLong(0);
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor executor)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 1. 采集线程池关键指标
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;poolSize&nbsp;=&nbsp;executor.getPoolSize();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;activeThreads&nbsp;=&nbsp;executor.getActiveCount();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;corePoolSize&nbsp;=&nbsp;executor.getCorePoolSize();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;maxPoolSize&nbsp;=&nbsp;executor.getMaximumPoolSize();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;queueSize&nbsp;=&nbsp;executor.getQueue().size();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;completedTasks&nbsp;=&nbsp;executor.getCompletedTaskCount();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 2. 递增拒绝计数器
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;totalRejected&nbsp;=&nbsp;rejectedCount.incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 3. 输出警告日志（包含完整指标）
&nbsp; &nbsp; &nbsp; &nbsp; logger.warn("""
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;任务被拒绝执行！线程池状态:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 活跃线程数/当前线程数: {}/{}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 核心/最大线程数: {}/{}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 队列大小: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 已完成任务数: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 历史拒绝总数: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 被拒绝任务: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; """,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activeThreads, poolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; corePoolSize, maxPoolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; queueSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; completedTasks,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; totalRejected,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; r.getClass().getName());
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 4. 可选：降级处理（如存入数据库等待重试）
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// fallbackToDatabase(r);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 5. 抛出RejectedExecutionException（保持默认行为）
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;RejectedExecutionException("Task "&nbsp;+ r.toString() +&nbsp;" rejected");
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;// 获取累计拒绝次数（用于监控）
&nbsp; &nbsp;&nbsp;public&nbsp;long&nbsp;getRejectedCount()&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;rejectedCount.get();
&nbsp; &nbsp; }
}</code></pre>
<ul>
<li><strong>记录日志并告警</strong>：所有的异常处理中，最常见简单的方式无外乎，先记录个日志，然后有告警系统的进行相关的某书、某信以及短信等的告警信息推送，方便开发人员以及运维人员的及时发现问题并介入处理。</li>
<li><strong>兜底处理机制</strong>：一般常见的就是通过异步的方式提交到MQ，然后统一进行兜底处理。</li>
<li><strong>带超时和重试的拒绝</strong>：可以尝试等待一小段时间，或者重试几次提交，如果仍然失败，再执行最终的拒绝逻辑（如告警、持久化或抛异常）。</li>
<li><strong>动态调整策略</strong>：根据系统的负载或任务类型，动态的执行兜底策略机制，就如前面写的源码示例方式。</li>
</ul>
<p><strong>2. 根据自身业务场景选择合适的拒绝策略：</strong></p>
<ul>
<li><strong>核心业务，不容丢失</strong>：如果任务非常重要，不能丢失，可以考虑：
<ul>
<li>CallerRunsPolicy：调用线程承担任务执行压力，是否可支撑；</li>
<li>自定义策略：尝试持久化到MQ或DB，然后由专门的消费组补偿任务处理；</li>
<li>AbortPolicy：如果希望系统快速失败并由上层进行重试或熔断。</li>
<li>&nbsp;</li>
</ul> </li>
<li><strong>非核心业务，可容忍部分丢失</strong>：
<ul>
<li>DiscardOldestPolicy：新任务更重要时，如行情推送；</li>
<li>DiscardPolicy：边缘业务场景，比如一些pv统计等，丢失了无所谓；</li>
<li>及时的进行监控查看，了解任务的丢失情况。</li>
</ul> </li>
</ul>
<p><strong>3. 结合线程池参数综合考虑：</strong></p>
<ul>
<li>拒绝策略的选择也与线程池的队列类型（有界/无界）、队列容量、maximumPoolSize等参数密切相关。</li>
<li>如果使用无界队列LinkedBlockingQueue的无参构造，只有机器内存不够时才会进行拒绝策略，不过这种极端场景已经不是影响线程池本身，内存不够可能导致Java进程被操作系统直接kill可能。</li>
<li>如果使用有界队列，需要权衡队列的大小，核心场景甚至可以动态追踪阻塞队列大小，以及动态调整队列大小来保证核心业务的正常流转。</li>
</ul>
<ol>
<li><strong>充分测试和监控</strong>：无论选择哪种策略，都必须在压测环境中充分测试其行为，并在线上环境建立完善的监控体系，监控线程池的各项指标（活跃线程数、队列长度、任务完成数、任务拒绝数等）。当拒绝发生时，应有相应的告警通知。</li>
</ol>
<p><strong>拒绝策略小结</strong>：</p>
<p>策略的选择跟我们大多数的系统设计哲学是保持一致的，都是在应对不同的场景中，做出一定的trade off。最好的策略需要根据业务场景、系统容忍度、资源等方面的综合考量，<strong>一个黄金的实践原则</strong>：拒绝事件做好监控告警、根据业务SLA定义策略，是否可丢失，快速失败等，定期的进行压力测试，验证策略的有效性。</p>
<h1>5.5 池隔离实践</h1>
<p><strong>核心思想</strong>：根据任务的资源类型 、优先级和业务特性 ，划分多个独立的线程池，避免不同性质的任务相互干扰。</p>
<p><strong>1. 隔离维度</strong>：</p>
<ul>
<li><strong>资源类型</strong>：CPU密集型 vs &nbsp;I/O密集型任务</li>
<li><strong>执行时长</strong>：短时任务（毫秒级） vs 长时任务（分钟级）</li>
<li><strong>实时性要求</strong>：高实时性 vs 可延迟（最终一致即可）</li>
<li><strong>业务重要性</strong>：支付交易（高优先级） vs 日志清理（低优先级）</li>
<li><strong>是否依赖外部资源</strong>：例如，访问特定数据库、调用特定第三方API的任务可以归为一类。</li>
</ul>
<p><strong>2. 不同业务场景线程池独立使用</strong>：在不同的业务场景下，为自己的特定业务，创建独立的线程池。</p>
<ul>
<li><strong>线程命名</strong>：通过ThreadFactory为每个线程池及其线程设置有意义的名称，例如netty-io-compress-pool-%d，excel-export-pool-%d, 主要方便区别不同的业务场景以及问题排查。</li>
<li><strong>参数调优</strong>：不同的业务场景设置不同的参数。
<ul>
<li>corePoolSize, maximumPoolSize：CPU密集型的计算任务可以设置小点减少上下文的切换，I/O密集型可以较大，在io阻塞等待期间，多去处理其他任务。</li>
<li>阻塞队列blockQueue：选择合适的队列类型，以及设置合理的队列大小。</li>
<li>RejectedExecutionHandler：有内置的四种的策略以及自定义策略选择，一般建议做好日志、监控以及兜底的处理。</li>
<li>&nbsp;</li>
</ul> </li>
</ul>
<p><strong>3. 自定义Executor避免线程池共用</strong></p>
<pre><code>// 创建CPU密集型任务线程池（线程数=CPU核心数）
ExecutorService cpuIntensiveExecutor =&nbsp;new&nbsp;ThreadPoolExecutor(
&nbsp; &nbsp; Runtime.getRuntime().availableProcessors(),&nbsp;// 核心线程数=CPU核心数
&nbsp; &nbsp; Runtime.getRuntime().availableProcessors(),&nbsp;// 最大线程数=CPU核心数
&nbsp; &nbsp;&nbsp;30L, TimeUnit.SECONDS,
&nbsp; &nbsp;&nbsp;new&nbsp;LinkedBlockingQueue&lt;&gt;(500),
&nbsp; &nbsp;&nbsp;new&nbsp;ThreadFactoryBuilder()
&nbsp; &nbsp; &nbsp; &nbsp; .setNameFormat("cpu-pool-%d")
&nbsp; &nbsp; &nbsp; &nbsp; .setPriority(Thread.MAX_PRIORITY)&nbsp;// 提高优先级
&nbsp; &nbsp; &nbsp; &nbsp; .build(),
&nbsp; &nbsp;&nbsp;new&nbsp;ThreadPoolExecutor.AbortPolicy()&nbsp;// 直接拒绝
);
// 使用示例
CompletableFuture.supplyAsync(() -&gt; {
&nbsp; &nbsp;&nbsp;// 矩阵计算等CPU密集型任务
&nbsp; &nbsp;&nbsp;double[][] result = matrixMultiply(largeMatrixA, largeMatrixB);
&nbsp; &nbsp;&nbsp;return&nbsp;result;
}, cpuIntensiveExecutor)
.thenAccept(result -&gt; {
&nbsp; &nbsp; System.out.println("计算结果维度: "&nbsp;+ result.length +&nbsp;"x"&nbsp;+ result[0].length);
});</code></pre>
<p><strong>线程池隔离小结</strong>：</p>
<p>专池专用的本质是通过物理隔离实现：</p>
<ul>
<li>资源保障 ：关键业务独占线程资源</li>
<li>故障隔离 ：避免级联雪崩</li>
<li>性能优化 ：针对任务类型最大化吞吐量</li>
</ul>
<p>最终呈现的效果是像专业厨房的分区（切配区/炒菜区/面点区）一样，让每个线程池专注处理同类任务，提升整体效率和可靠性。</p>
<h1>六、总结</h1>
<p>线程池是Java并发编程的核心组件，通过复用线程减少资源开销，提升系统吞吐量。其核心设计包括线程复用机制 、任务队列和拒绝策略 ，通过ThreadPoolExecutor的参数（核心线程数、最大线程数、队列容量等）实现灵活的资源控制。线程池的生命周期由RUNNING、SHUTDOWN等状态管理，确保任务有序执行或终止。</p>
<p>内置线程池（如Executors.newCachedThreadPool）虽便捷，但存在内存溢出或无界队列堆积的风险，需谨慎选择。invokeAll的超时失效和submit提交任务的异常消失是常见陷阱需通过正确处理中断和检查Future.get()规避。</p>
<p>最佳实践包括：</p>
<ul>
<li>异常处理：通过afterExecute来对发生的异常进行兜底处理，任务细粒度的try catch或UncaughtExceptionH捕获异常处理防止线程崩溃退出；</li>
<li>拒绝策略：根据业务选择拒绝策略或自定义降级逻辑，生产级应用建议尽量自定义处理；</li>
<li>线程隔离 ：按任务类型（CPU/I/O）或优先级划分线程池，避免资源竞争。</li>
</ul>
<p>合理使用线程池能显著提升性能，但需结合业务场景精细调参，确保稳定性和可维护性，希望这篇文章能给大家带来一些生产实践上的指导，减少一些因为不熟悉线程池相关原理生产误用导致的一些问题。</p>
<h4>往期回顾</h4>
<p>1.&nbsp;基于浏览器扩展 API Mock 工具开发探索｜得物技术</p>
<p>2.&nbsp;破解gh-ost变更导致MySQL表膨胀之谜｜得物技术</p>
<p>3.&nbsp;MySQL单表为何别超2000万行？揭秘B+树与16KB页的生死博弈｜得物技术</p>
<p>4.&nbsp;0基础带你精通Java对象序列化--以Hessian为例｜得物技术</p>
<p>5.&nbsp;前端日志回捞系统的性能优化实践｜得物技术</p>
<h4>文 /舍得</h4>
<p>关注得物技术，每周更新技术干货</p>
<p>要是觉得文章对你有帮助的话，欢迎评论转发点赞～</p>
<p>未经得物技术许可严禁转载，否则依法追究法律责任。</p>]]>
    </description>
    <content:encoded><![CDATA[<h1>一、引 言</h1>
<p><strong>为什么进行源码角度的深度解析？</strong></p>
<p>大家在项目中到处都在使用线程池做一些性能接口层次的优化，原先串行的多个远程调用，因为rt过高，通过线程池批量异步优化，从而降低rt。还有像RocketMQ中broker启动时，同时通过ScheduledThreadPoolExecutor线程池执行其他组件的定时任务，每隔一段时间处理相关的任务。线程池广泛的应用在外面各种实际开发场景中，我们很多同学可能在项目里只是简单的copy了一些前人的代码参数并不知道其中的含义，从而导致生产级别的bug。所以本篇文章，旨在帮助还不熟悉或者想要熟悉线程池的同学，分享我自己在学习线程池源码上的一些内容来更简单、快速的掌握线程池。</p>
<h1>二、为什么使用线程池？</h1>
<p>并发编程中，对于常见的操作系统，线程都是执行任务的基本单元，如果每次执行任务时都创建新的线程，任务执行完毕又进行销毁，会出现以下的问题：</p>
<ul>
<li><strong>资源开销</strong>：比如在Linux系统中，频繁的创建和销毁线程，一个是频繁的进行一个系统调用，另外是一些内存和CPU资源调度的占用。虽然有一些写时复制的策略防止lwp的创建时的内存占用，但是实际写入还是会申请系统内存的，何况一些页表等本身就有内存占用。</li>
<li><strong>性能瓶颈</strong>：线程的创建需要系统调用，如果只是简单的计算任务，可能耗时还没创建的rt高，这里反而降低了系统的吞吐量。</li>
<li><strong>缺乏资源管理</strong>：无限制的创建线程会导致内存溢出，java.lang.OutOfMemoryError: unable to create native thread，这里主要因为Java的线程其实Linux中是lwp线程，需要通过JNI进行系统调用创建，每个线程默认需要1MB的栈空间，很容易导致无休止的创建线程导致内存溢出，另外就是频繁的系统调用，导致的上下文切换，占用了过多的CPU，反而起到了相反的作用。</li>
<li><strong>功能受限</strong>：手动管理线程难以实现更高级的功能，如定时任务、周期任务、任务管理、并发任务数的控制等。</li>
</ul>
<p>通过上面的问题，我们其实可以清晰的感知到这些问题都是归拢到资源没有得到合理的分配和控制导致的，线程池出现的核心宗旨其实就是对资源的合理分配和控制。除了线程池，其实更多的也接触过数据库连接池、netty的对象池等池化技术，这些池化思想其实都是为了更好的降低资源的消耗以及更好的进行资源管理。</p>
<h1>三、JDK线程池的架构设计</h1>
<h1>3.1 JUC并发包下的Executor框架的uml类图</h1>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-10e0261f0e.jpg">
</div>
<ul>
<li><strong>Executor</strong>：任务执行的顶层接口，主要是分离任务提交与执行逻辑，支持同步/异步执行，遵循Java内存模型的&nbsp;happen-before规则。</li>
<li><strong>ExecutorService</strong>：继承Executor接口，提供了更完善的生命周期管理能力，通过Future对象提供任务取消、状态查询、结果获取能力实现了任务监控。</li>
<li><strong>AbstractExecutorService</strong>：常见的设计模式为了简化线程池的开发，通常通过父类进行一些基础的默认实现让子类继承。</li>
<li><strong>ScheduledExecutorService</strong>：ExecutorService的扩展接口，支持延迟执行和周期性任务调度。</li>
<li><strong>ThreadPoolExecutor</strong>：是ExecutorService接口最核心和最常用的实现类，它提供了高度可配置的线程池，允许我们精细控制线程池的各种行为。</li>
<li><strong>ScheduledThreadPoolExecutor</strong>：是ScheduledExecutorService接口的实现类，它继承自ThreadPoolExecutor，专门用于处理定时和周期任务。</li>
<li><strong>Executors</strong>：一个静态工厂模式的工具类，提供了一系列静态方法来创建各种常见配置的线程池，newFixedThreadPool(), newCachedThreadPool(),等，简化了创建线程池的使用但是会带来一些问题，很多开发规范里都不建议大家直接使用。JDK内置的线程池如果我们不熟悉里面的参数很有可能导致出乎自己意料的结果，池大小设置、阻塞队列选择等等都是有考究的，这一点后续会进行一些详细说明。生产环境中建议谨慎使用或直接使用ThreadPoolExecutor构造函数自定义。</li>
</ul>
<h1>3.2 ThreadPoolExecutor的参数解析</h1>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2b578a85a8.jpg">
</div>
<ul>
<li><strong>corePoolSize&nbsp;</strong>核心线程数：
<ul>
<li>线程池中还未退出的alive的核心线程数量。</li>
<li>虽然线程处于空闲状态（其实是阻塞在阻塞队列中），除非显示设置了allowCoreThreadTimeOut=true，否则这些线程不会从自己的run方法中退出被回收。</li>
<li>添加新任务时，如果当前工作线程小于coreSize，此时即使存在空闲的core线程，线程池也会通过addWorker方法创建一个新的线程。</li>
<li>&nbsp;</li>
</ul> </li>
<li><strong>maximumPoolSize&nbsp;</strong>最大线程数：
<ul>
<li>线程池可以创建的最大线程数。</li>
<li>如果是有界队列，当队列满时，仍然有任务进来，此时线程池会创建小于最大线程数的线程来完成任务，空闲。</li>
<li>如果是无界队列，那么永远不会出现第二点的情况，除了内存异常，否则会一直保持核心线程数，多余的任务会一直往队列中加入。</li>
</ul> </li>
</ul>
<ul>
<li><strong>keepAliveTime</strong>&nbsp;线程空闲存活时间
<ul>
<li>线程数超过corePoolSize后创建的线程我们理解为非核心线程，对于这类线程，他的回收机制在于我们设置的keepAliveTime,线程会限期阻塞在队列中获取任务，如果超时未获取就会进行清理并退出。</li>
<li>另外如果设置allowCoreThreadTimeOut=true，所谓的核心线程在空闲时间达到keepAliveTime时也会被回收。</li>
</ul> </li>
</ul>
<ul>
<li><strong>unit&nbsp;</strong>时间单位
<ul>
<li>keepAliveTime参数的时间单位，TimeUnit中时分秒等。</li>
<li>&nbsp;</li>
</ul> </li>
<li><strong>workQueue&nbsp;</strong>任务队列
<ul>
<li>阻塞队列，核心线程数满时，新加入的任务，会先添加到阻塞队列中等待线程获取任务并执行。</li>
<li>常用的BlockingQueue实现有：
<ul>
<li>ArrayBlockingQueue：数组实现的先进先出原则的有界阻塞队列，构造方法必须指定容量。</li>
<li>LinkedBlockingQueue：链表实现的阻塞队列，构造传入容量则有界，未传则是无界队列，此时设置的最大线程数其实就不会有作用了。</li>
<li>SynchronousQueue：一个不存储元素的阻塞队列。每个put操作必须等待一个take操作，反之亦然。它相当于一个传递通道，非常适合传递性需求，吞吐量高，但要求maximumPoolSize足够大。</li>
<li>PriorityBlockingQueue：二叉堆实现的优先级阻塞队列，构造时可自行调整排序行为（小顶堆或大顶堆）。</li>
<li>DelayQueue：支持延时的无界阻塞队列，主要用于周期性的任务，我们可以直接通过它来实现一些简单的延迟任务需求，复杂的周期性任务建议使用ScheduledThreadPoolExecutor。</li>
</ul> </li>
</ul> </li>
</ul>
<ul>
<li><strong>threadFactory&nbsp;</strong>线程工厂
<ul>
<li>用于创建新线程的工厂。通过自定义ThreadFactory，我们可以为线程池中的线程设置更有意义的名称、设置守护线程状态、设置线程优先级、指定UncaughtExceptionHandler等。</li>
<li>Executors.defaultThreadFactory()是默认实现。</li>
</ul> </li>
</ul>
<ul>
<li><strong>handler&nbsp;</strong>拒绝策略
<ul>
<li>ThreadPoolExecutor.AbortPolicy：默认的拒绝策略，简单粗暴，当execute中添加woker失败时，直接在当前线程抛出异常。</li>
<li>JDK内置了四种拒绝策略：
<ul>
<li>ThreadPoolExecutor.AbortPolicy：默认的拒绝策略，简单粗暴，当execute中添加woker失败时，直接在当前线程抛出异常。</li>
<li>ThreadPoolExecutor.CallerRunsPolicy：提交任务的线程，直接执行任务。变相的背压机制，可以降低任务往线程中加入。</li>
<li>ThreadPoolExecutor.DiscardPolicy：直接丢弃被拒绝的任务，不做任何通知，需容忍数据丢失。</li>
<li>ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列中最旧的任务，然后重试提交当前任务，需容忍数据丢失。</li>
<li>实现RejectedExecutionHandler接口自定义拒绝策略，在实际生产应用中推荐使用，可以做一些打印观察日志的操作，告警、兜底的相关处理等。</li>
</ul> </li>
</ul> </li>
</ul>
<h1>3.3 运行机制详解</h1>
<p>新任务通过execute()方法提交给ThreadPoolExecutor时，其处理流程如下：</p>
<p><strong>判断核心线程数</strong>：如果当前运行的线程数小于corePoolSize，则创建新线程（即使有空闲的核心线程）来执行任务。</p>
<p><strong>尝试入队</strong>：如果当前运行的线程数大于或等于corePoolSize，则尝试将任务添加到workQueue中。</p>
<ul>
<li>如果workQueue.offer()成功（队列未满），任务入队等待执行。</li>
</ul>
<p><strong>尝试创建非核心线程</strong>：如果workQueue.offer()失败（队列已满）：</p>
<ul>
<li>判断当前运行的线程数是否小于maximumPoolSize；</li>
<li>如果是，则创建新的非核心线程来执行任务。</li>
</ul>
<p><strong>执行拒绝策略：</strong></p>
<p>如果当前运行的线程数也达到了maximumPoolSize（即核心线程和非核心线程都已用尽，且队列也满了），则执行RejectedExecutionHandler所定义的拒绝策略。</p>
<p>参考网络中的经典执行图：</p>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-88df05dbfd.jpg">
</div>
<p>这个图能很好的表明运行原理，但是忽略了很多细节，比如所谓的缓冲执行是在什么条件下去走的呢？直接执行又是什么逻辑下执行呢？最后的任务拒绝又是怎么回事？带着这些疑问点，我们直接来进行一个源码级别的分析：</p>
<h1>execute核心流程的源码分析</h1>
<pre><code>public&nbsp;void&nbsp;execute(Runnable command)&nbsp;{
&nbsp; &nbsp;&nbsp;if&nbsp;(command ==&nbsp;null)
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;NullPointerException();
&nbsp; &nbsp;&nbsp;//线程池状态 高3位表示线程状态 低29位代表线程数量
&nbsp; &nbsp;&nbsp;int&nbsp;c = ctl.get();
&nbsp; &nbsp;&nbsp;//判断当前线程池线程数量是否小于核心线程数
&nbsp; &nbsp;&nbsp;if&nbsp;(workerCountOf(c) &lt; corePoolSize) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//作为核心线程数进行线程的创建，并且创建成功线程会将command的任务执行--》对应图上的直接执行
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(addWorker(command,&nbsp;true))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; c = ctl.get();
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;//创建核心线程失败或者当前线程数量超过核心线程数
&nbsp; &nbsp;&nbsp;//当前线程池是否还在运行状态，尝试将任务添加到阻塞队列 --》对应图上的缓冲执行
&nbsp; &nbsp;&nbsp;//BlockingQueue队列的顶级抽象定义了offer不是进行阻塞添加而是立即返回，添加失败直接返回false，区别于put
&nbsp; &nbsp;&nbsp;if&nbsp;(isRunning(c) &amp;&amp; workQueue.offer(command)) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//重新获取线程池标志位
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;recheck = ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//如果线程此时不在运行状态中，那么将任务删除
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(! isRunning(recheck) &amp;&amp;&nbsp;remove(command))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//删除任务成功，走拒绝策略拒绝掉当前任务
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; reject(command);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(workerCountOf(recheck) ==&nbsp;0)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//如果线程池中的工作线程都没有的时候，这里需要创建一个线程去执行添加到队列中的任务
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//防止因为并发的原因工作线程都被终止掉了，此时任务在阻塞队列里等着，缺没有工作线程
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addWorker(null,&nbsp;false);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;//到这里那就是添加队列失败，或者线程池状态异常，但是这里仍然尝试进行创建一个worker
&nbsp; &nbsp;&nbsp;//如果创建失败，也是走拒绝策略拒绝当前任务
&nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(!addWorker(command,&nbsp;false))
&nbsp; &nbsp; &nbsp; &nbsp; reject(command);
}</code></pre>
<p>接下来我们仔细看看addWorker这个方法具体是在做什么：</p>
<pre><code>//核心逻辑其实就是在无限循环创建一个worker，创建失败直接返回，创建成功，则将worker执行
// 因为worker有thread的成员变量，最终添加worker成功，会启动线程的start方法
//start方法最终会回调到外层的runWorker方法，改方法会不停的从阻塞队列里以阻塞的take方式
//获取任务，除非达到能被终止的条件，此时当前线程会终止
private&nbsp;boolean&nbsp;addWorker(Runnable firstTask,&nbsp;boolean&nbsp;core)&nbsp;{
&nbsp; &nbsp; retry:
&nbsp; &nbsp;&nbsp;for&nbsp;(;;) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;c&nbsp;=&nbsp;ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;rs&nbsp;=&nbsp;runStateOf(c);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Check if queue empty only if necessary.
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(rs &gt;= SHUTDOWN &amp;&amp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ! (rs == SHUTDOWN &amp;&amp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;firstTask ==&nbsp;null&nbsp;&amp;&amp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;! workQueue.isEmpty()))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;false;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(;;) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;wc&nbsp;=&nbsp;workerCountOf(c);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(wc &gt;= CAPACITY ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; wc &gt;= (core ? corePoolSize : maximumPoolSize))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;false;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//不停的重试添加worker的计数，只有添加成功的才会进行后续的worker启动
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(compareAndIncrementWorkerCount(c))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;break&nbsp;retry;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; c = ctl.get(); &nbsp;// Re-read ctl
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//重试期间，如果其他线程导致线程池状态不一致了。重新回到第一个循环进行check判断
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(runStateOf(c) != rs)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;continue&nbsp;retry;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// else CAS failed due to workerCount change; retry inner loop
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;boolean&nbsp;workerStarted&nbsp;=&nbsp;false;
&nbsp; &nbsp;&nbsp;boolean&nbsp;workerAdded&nbsp;=&nbsp;false;
&nbsp; &nbsp;&nbsp;Worker&nbsp;w&nbsp;=&nbsp;null;
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; w =&nbsp;new&nbsp;Worker(firstTask);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;final&nbsp;Thread&nbsp;t&nbsp;=&nbsp;w.thread;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(t !=&nbsp;null) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//这里加锁一个是workers.add时需要加锁，另外是防止其他线程已经在尝试修改线程池状态了
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock&nbsp;mainLock&nbsp;=&nbsp;this.mainLock;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Recheck while holding lock.
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Back out on ThreadFactory failure or if
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// shut down before lock acquired.
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;rs&nbsp;=&nbsp;runStateOf(ctl.get());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(rs &lt; SHUTDOWN ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (rs == SHUTDOWN &amp;&amp; firstTask ==&nbsp;null)) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(t.isAlive())&nbsp;// precheck that t is startable
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;IllegalThreadStateException();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//将worker的引用添加到workers的hashSet中
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; workers.add(w);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;s&nbsp;=&nbsp;workers.size();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//更新线程池此时最大的线程数
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(s &gt; largestPoolSize)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; largestPoolSize = s;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; workerAdded =&nbsp;true;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//如果添加成功，就启动worker中的线程
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(workerAdded) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; t.start();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; workerStarted =&nbsp;true;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//这里添加失败的话，需要把线程池的count数进行--，并且要把worker引用从hashSer中移除
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(! workerStarted)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addWorkerFailed(w);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;return&nbsp;workerStarted;
}</code></pre>
<h1>3.4 线程池的生命周期</h1>
<p>在介绍运行机制原理的源码分析时，其实是有提到线程池状态这个概念的。介绍这个状态其实也是让大家更方便的去管理线程池，比如我们关闭线程池时，怎么去优雅的关闭，使用不同的方法可能会有不同的效果，我们需要根据自己的业务场景去酌情分析、权衡使用。</p>
<pre><code>//线程池的状态和计数采用一个Integer变量设置的
//这里之所以用一个变量来储存状态和数量，其实很有讲究的，因为我们在上面的运行原理上可以看到
//源码中有大量的进行状态以及数量的判断，如果分开采用变量的记录的话，在维护二者一致性方面
//可能就需要加锁的维护成本了，而且计算中都是位移运算也是非常高效的
private&nbsp;final&nbsp;AtomicInteger&nbsp;ctl&nbsp;=&nbsp;new&nbsp;AtomicInteger(ctlOf(RUNNING,&nbsp;0));
//线程池的大小由ctl低29位表示，现成状态由ctl高3位表示
private&nbsp;static&nbsp;final&nbsp;int&nbsp;COUNT_BITS&nbsp;=&nbsp;Integer.SIZE -&nbsp;3;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;CAPACITY&nbsp; &nbsp;=&nbsp;(1&nbsp;&lt;&lt; COUNT_BITS) -&nbsp;1;
// 线程池的状态通过简单的位移就能计算出来，状态只能从低到高流转，不能逆向
private&nbsp;static&nbsp;final&nbsp;int&nbsp;RUNNING&nbsp; &nbsp;&nbsp;=&nbsp;-1&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;SHUTDOWN&nbsp; &nbsp;=&nbsp;&nbsp;0&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;STOP&nbsp; &nbsp; &nbsp; &nbsp;=&nbsp;&nbsp;1&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;TIDYING&nbsp; &nbsp;&nbsp;=&nbsp;&nbsp;2&nbsp;&lt;&lt; COUNT_BITS;
private&nbsp;static&nbsp;final&nbsp;int&nbsp;TERMINATED&nbsp;=&nbsp;&nbsp;3&nbsp;&lt;&lt; COUNT_BITS;
// 这里是获取线程状态以及获取线程数量的简单高效的位移方法
private&nbsp;static&nbsp;int&nbsp;runStateOf(int&nbsp;c)&nbsp; &nbsp; &nbsp;{&nbsp;return&nbsp;c &amp; ~CAPACITY; }
private&nbsp;static&nbsp;int&nbsp;workerCountOf(int&nbsp;c)&nbsp; {&nbsp;return&nbsp;c &amp; CAPACITY; }
private&nbsp;static&nbsp;int&nbsp;ctlOf(int&nbsp;rs,&nbsp;int&nbsp;wc)&nbsp;{&nbsp;return&nbsp;rs | wc; }</code></pre>
<p>接下来结合源码详细介绍下线程池的5种状态以及分别有什么不同的表现行为？</p>
<pre><code>先说下结论：
RUNNING&nbsp; &nbsp; &nbsp;这个就是线程池运行中状态，我们可以添加任务也可以处理阻塞队列任务
SHUTDOWN &nbsp; 不能添加新的任务，但是会将阻塞队列中任务执行完毕
STOP &nbsp; &nbsp; &nbsp; 不能添加新的任务，执行中的线程也会被打断，也不会处理阻塞队列的任务
TIDYING &nbsp; &nbsp;所有线程都被终止，并且workCount=0时会被置为的状态
TERMINATED &nbsp; 调用完钩子方法terminated()被置为的状态&nbsp;</code></pre>
<h1>shutdown状态源码分析：</h1>
<pre><code>&nbsp;
&nbsp;//线程池关闭
&nbsp;public&nbsp;void&nbsp;shutdown()&nbsp;{
&nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock&nbsp;mainLock&nbsp;=&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; checkShutdownAccess();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//循环cas设置线程池状态，直到成功或状态已经state&gt;=SHUTDOWN
&nbsp; &nbsp; &nbsp; &nbsp; advanceRunState(SHUTDOWN);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//这个是真正得出结论的地方
&nbsp; &nbsp; &nbsp; &nbsp; interruptIdleWorkers();
&nbsp; &nbsp; &nbsp; &nbsp; onShutdown();&nbsp;// hook for ScheduledThreadPoolExecutor
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
&nbsp; &nbsp; tryTerminate();
}
private&nbsp;void&nbsp;interruptIdleWorkers()&nbsp;{
&nbsp; &nbsp; interruptIdleWorkers(false);
}
//打断空闲的线程，如何判断线程是否空闲还是运行？
private&nbsp;void&nbsp;interruptIdleWorkers(boolean&nbsp;onlyOne)&nbsp;{
&nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock&nbsp;mainLock&nbsp;=&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(Worker w : workers) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Thread&nbsp;t&nbsp;=&nbsp;w.thread;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//worker的线程没有被打断过，并且能获取到worker的aqs独占锁
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!t.isInterrupted() &amp;&amp; w.tryLock()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//打断当前线程，如果线程在阻塞队列中阻塞，此时会被中断
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; t.interrupt();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(SecurityException ignore) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; w.unlock();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(onlyOne)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;break;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
}
</code></pre>
<h1>STOP状态分析</h1>
<pre><code>//循环cas修改线程池状态为stop。打断所有线程，取出阻塞队列的所有任务
public&nbsp;List&lt;Runnable&gt;&nbsp;shutdownNow()&nbsp;{
&nbsp; &nbsp; List&lt;Runnable&gt; tasks;
&nbsp; &nbsp; final ReentrantLock mainLock =&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//检查线程的权限
&nbsp; &nbsp; &nbsp; &nbsp; checkShutdownAccess();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//将状态case为stop
&nbsp; &nbsp; &nbsp; &nbsp; advanceRunState(STOP);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//打断所有worker不管是不是正在执行任务
&nbsp; &nbsp; &nbsp; &nbsp; interruptWorkers();
&nbsp; &nbsp; &nbsp; &nbsp; tasks = drainQueue();
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
&nbsp; &nbsp; tryTerminate();
&nbsp; &nbsp;&nbsp;return&nbsp;tasks;
}
//这里获取锁之后。打断了所有的线程
private&nbsp;void&nbsp;interruptWorkers()&nbsp;{
&nbsp; &nbsp; final ReentrantLock mainLock =&nbsp;this.mainLock;
&nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(Worker w : workers)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; w.interruptIfStarted();
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; }
}</code></pre>
<h1>TIDYING、TERMINATED&nbsp;状态分析</h1>
<pre><code>//这个方法在每个线程退出时都会进行调用，如果是运行中、或者状态大于等于TIDYING或者shutdown但是队列不为空都
//直接返回，如果不满足以上条件，并且线程数不为0的话，打断一个空闲线程
final&nbsp;void tryTerminate() {
&nbsp; &nbsp;&nbsp;for&nbsp;(;;) {
&nbsp; &nbsp; &nbsp; &nbsp; int c = ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(isRunning(c) ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; runStateAtLeast(c, TIDYING) ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty()))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(workerCountOf(c) !=&nbsp;0) {&nbsp;// Eligible to terminate
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; interruptIdleWorkers(ONLY_ONE);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//此时到这里，状态要么为STOP。要么是shutdown并且队列为空了
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 获取一个锁，尝试cas修改状态为TIDYING
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//调用terminated()的钩子方法，
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//修改线程池为终态TERMINATED，并且唤醒阻塞在termination队列上的线程
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;final&nbsp;ReentrantLock mainLock =&nbsp;this.mainLock;
&nbsp; &nbsp; &nbsp; &nbsp; mainLock.lock();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ctl.compareAndSet(c, ctlOf(TIDYING,&nbsp;0))) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; terminated();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ctl.set(ctlOf(TERMINATED,&nbsp;0));
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; termination.signalAll();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mainLock.unlock();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// else retry on failed CAS
&nbsp; &nbsp; }
}</code></pre>
<h1>四、JDK内置线程池的问题</h1>
<p>java.util.concurrent.Executors工厂类提供了一些静态方法，方便我们快速创建几种预设配置的线程池：</p>
<ul>
<li>Executors.newFixedThreadPool(int nThreads)：
<ul>
<li>创建一个固定大小的线程池。corePoolSize和maximumPoolSize都等于nThreads。</li>
<li>keepAliveTime为0L（因为线程数不会超过corePoolSize，所以此参数无效，除非allowCoreThreadTimeOut为true）。</li>
<li>使用无界的LinkedBlockingQueue作为工作队列。</li>
<li><strong>问题</strong>：由于使用无界队列，当任务提交速度远大于处理速度时，队列会持续增长，可能导致内存溢出（OOM）。此时maximumPoolSize参数实际上是无效的，线程数永远不会超过nThreads。</li>
</ul> </li>
</ul>
<ul>
<li>Executors.newSingleThreadExecutor()：
<ul>
<li>创建一个只有一个工作线程的线程池。corePoolSize和maximumPoolSize都为1。</li>
<li>同样使用<strong>无界</strong>的LinkedBlockingQueue。</li>
<li>保证所有任务按照提交顺序（FIFO）执行。</li>
<li><strong>问题</strong>：与newFixedThreadPool类似，无界队列可能导致OOM。</li>
</ul> </li>
</ul>
<ul>
<li>Executors.newCachedThreadPool()：
<ul>
<li>创建一个可缓存的线程池。</li>
<li>corePoolSize为0。</li>
<li>maximumPoolSize为Integer.MAX_VALUE (几乎是无界的)。</li>
<li>keepAliveTime为60秒。</li>
<li>使用SynchronousQueue作为工作队列。这种队列不存储元素，任务提交后必须有空闲线程立即接收，否则会创建新线程（如果未达到maximumPoolSize）。</li>
<li><strong>问题</strong>：如果任务提交速度过快，会创建大量线程（理论上可达Integer.MAX_VALUE个），可能耗尽系统资源，导致OOM以及频繁的上下文切换。</li>
</ul> </li>
</ul>
<ul>
<li>Executors.newSingleThreadScheduledExecutor()、Executors.newScheduledThreadPool(int corePoolSize):
<ul>
<li>创建用于调度任务的线程池。</li>
<li>内部使用ScheduledThreadPoolExecutor实现，其任务队列是DelayedWorkQueue (一种特殊的PriorityQueue)。</li>
<li>newSingleThreadScheduledExecutor的corePoolSize为1，maximumPoolSize为Integer.MAX_VALUE（但由于队列是DelayedWorkQueue，通常不会无限增长线程，除非有大量同时到期的任务且处理不过来）。</li>
<li>newScheduledThreadPool可以指定corePoolSize。</li>
<li><strong>问题</strong>：虽然DelayedWorkQueue本身是无界的，但ScheduledThreadPoolExecutor在任务执行逻辑上与普通ThreadPoolExecutor有所不同。主要风险仍然是如果corePoolSize设置不当，且大量任务同时到期并执行缓慢，可能导致任务积压。</li>
</ul> </li>
</ul>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a0860059e7.jpg">
</div>
<p>某一线互联网Java开发手册</p>
<h1>五、线程池中的问题与最佳实践</h1>
<h1>5.1 invokeAll 超时机制无效？</h1>
<p>ExecutorService.invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit)方法会提交一组Callable任务，并等待所有任务完成，或者直到超时。如果超时发生，它会尝试取消（中断）所有尚未完成的任务，然后返回一个List&lt;Future&gt;。</p>
<p><strong>失效场景分析：</strong></p>
<ul>
<li>任务不响应中断（最常见）：任务内部捕获&nbsp;InterruptedException&nbsp;后静默处理，或执行不检查中断状态的阻塞操作（如循环计算）：</li>
</ul>
<pre><code>Callable&lt;String&gt; task = () -&gt; {
&nbsp; &nbsp;&nbsp;while&nbsp;(true) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//缺少此检查将导致超时失效
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(Thread.interrupted())&nbsp;break;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 耗时计算...
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;return&nbsp;"done";
};</code></pre>
<ul>
<li><strong>使用非响应中断的API</strong>：任务调用了不响应&nbsp;interrupt()&nbsp;的第三方库或JNI代码（如某些IO操作）</li>
</ul>
<pre><code>Callable&lt;Integer&gt; task&nbsp;=&nbsp;() -&gt; {
&nbsp; &nbsp;&nbsp;Files.copy(in, path);&nbsp;// 某些NIO操作不响应中断
&nbsp; &nbsp;&nbsp;return&nbsp;1;
};</code></pre>
<ul>
<li><strong>任务依赖外部资源阻塞</strong>：任务因外部资源（如数据库连接、网络请求）阻塞且未设置超时。</li>
</ul>
<pre><code>Callable&lt;Result&gt; task&nbsp;=&nbsp;() -&gt; {
&nbsp; &nbsp;&nbsp;//未设查询超时时间
&nbsp; &nbsp;&nbsp;return&nbsp;jdbcTemplate.query("SELECT * FROM large_table");&nbsp;
};</code></pre>
<ul>
<li><strong>线程池配置缺陷</strong>：核心线程数过大或队列无界，导致&nbsp;invokeAll&nbsp;超时前任务无法全部启动，任务堆积在队列，invokeAll&nbsp;超时后仍有大量任务未执行。</li>
</ul>
<pre><code>new&nbsp;ThreadPoolExecutor(
&nbsp; &nbsp;&nbsp;100,&nbsp;100,&nbsp;// 核心线程数过大
&nbsp; &nbsp; 0L,&nbsp;TimeUnit.MILLISECONDS,
&nbsp; &nbsp;&nbsp;new&nbsp;LinkedBlockingQueue&lt;&gt;()&nbsp;// 无界队列
);</code></pre>
<p><strong>invokeAll超时失效demo:</strong></p>
<pre><code>import&nbsp;java.util.*;
import&nbsp;java.util.concurrent.*;
public&nbsp;class&nbsp;InvokeAllTimeoutDemo&nbsp;{
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;// 模拟耗时任务（可配置是否响应中断）
&nbsp; &nbsp;&nbsp;static&nbsp;class&nbsp;Task&nbsp;implements&nbsp;Callable&lt;String&gt; {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;int&nbsp;id;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;long&nbsp;durationMs;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;boolean&nbsp;respectInterrupt;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; Task(int&nbsp;id,&nbsp;long&nbsp;durationMs,&nbsp;boolean&nbsp;respectInterrupt) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.id = id;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.durationMs = durationMs;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.respectInterrupt = respectInterrupt;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;String&nbsp;call()&nbsp;throws&nbsp;Exception {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.printf("Task %d started%n", id);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;start&nbsp;=&nbsp;System.currentTimeMillis();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 模拟工作（检查中断状态）
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;while&nbsp;(System.currentTimeMillis() - start &lt; durationMs) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(respectInterrupt &amp;&amp; Thread.interrupted()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;InterruptedException("Task "&nbsp;+ id +&nbsp;" interrupted");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 不响应中断的任务会继续执行
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.printf("Task %d completed%n", id);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;"Result-"&nbsp;+ id;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;public&nbsp;static&nbsp;void&nbsp;main(String[] args)&nbsp;throws&nbsp;InterruptedException {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;ExecutorService&nbsp;executor&nbsp;=&nbsp;Executors.newFixedThreadPool(2);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; List&lt;Callable&lt;String&gt;&gt; tasks = Arrays.asList(
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;new&nbsp;Task(1,&nbsp;2000,&nbsp;true), &nbsp;&nbsp;// 2秒，响应中断
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;new&nbsp;Task(2,&nbsp;10000,&nbsp;false) &nbsp;// 10秒，不响应中断
&nbsp; &nbsp; &nbsp; &nbsp; );
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Invoking with 3s timeout...");
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//设置3秒超时
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; List&lt;Future&lt;String&gt;&gt; futures = executor.invokeAll(tasks,&nbsp;3, TimeUnit.SECONDS);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(Future&lt;String&gt; f : futures) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 明确处理取消状态
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(f.isCancelled()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Task was cancelled"); &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Result: "&nbsp;+ f.get(100, TimeUnit.MILLISECONDS));
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(TimeoutException | ExecutionException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Task failed: "&nbsp;+ e.getCause());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.shutdownNow();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Executor shutdown");
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p>当我们使用invokeAll(tasks, timeout)&nbsp;提交多个任务时，如果出现某个任务对中断不响应或者响应不及时，那我们即使设置了超时时间，不响应中断的任务2仍在后台运行（即使调用了&nbsp;shutdownNow()）</p>
<h1>5.2 submit()的异常消失了？</h1>
<p>使用ExecutorService.submit()提交任务时，任务执行过程中如果抛出未捕获的异常（无论是受检异常还是运行时异常），这个异常会被Future的包装类如FutureTask重写的run()方法捕获并封装在返回的Future包装对象的成员变量中。</p>
<ul>
<li><strong>不显示调用</strong>Future.get()，该异常我们就无法感知，好像没有发生过一样。线程池的工作线程本身通常会有一个默认的未捕获异常处理器，可能会打印堆栈到控制台，但你的主业务逻辑不会知道。</li>
<li><strong>显示调用</strong>Future.get()，抛出声明式的ExecutionException，其cause属性才是原始的任务异常。</li>
<li>如果调用Future.get(long timeout, TimeUnit unit)超时，向外抛出声明式的TimeoutException。此时任务可能仍在后台执行，可能错过了内部的异常。</li>
</ul>
<p><strong>submit()异常消失demo:</strong></p>
<pre><code>public&nbsp;class&nbsp;ThreadPoolExceptionDemo&nbsp;{
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;public&nbsp;static&nbsp;void&nbsp;main(String[] args)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 创建单线程线程池（便于观察异常）
&nbsp; &nbsp; &nbsp; &nbsp; ExecutorService executor = Executors.newSingleThreadExecutor();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 场景1：Callable抛出异常（通过Future.get()捕获）
&nbsp; &nbsp; &nbsp; &nbsp; Future&lt;String&gt; future1 = executor.submit(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("[Callable] 开始执行");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.sleep(100);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;RuntimeException("Callable故意抛出的异常");
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("Callable结果: "&nbsp;+ future1.get());
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(ExecutionException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.err.println("捕获到Callable异常: "&nbsp;+ e.getCause().getMessage());
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(InterruptedException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.currentThread().interrupt();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 场景2：Runnable抛出异常（同样通过Future.get()捕获）
&nbsp; &nbsp; &nbsp; &nbsp; Future&lt;?&gt; future2 = executor.submit(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("[Runnable] 开始执行");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;IllegalArgumentException("Runnable故意抛出的异常");
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; future2.get();&nbsp;// Runnable成功时返回null
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(ExecutionException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.err.println("捕获到Runnable异常: "&nbsp;+ e.getCause().getMessage());
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(InterruptedException e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.currentThread().interrupt();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 场景3：未处理的任务异常（需设置异常处理器）
&nbsp; &nbsp; &nbsp; &nbsp; executor.submit(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("[未捕获的任务] 开始执行");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;IllegalStateException("这个异常会被默认处理器处理");
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; executor.shutdown();
&nbsp; &nbsp; }
}</code></pre>
<h1>5.3 异常处理实践</h1>
<div>
<img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-851f74dab6.jpg">
</div>
<ul>
<li><strong>Callable/Runnable catch处理异常：</strong>
<ul>
<li><strong>不要捕获Throwable或Exception然后静默处理（只打日志）</strong>。如果确实需要捕获，请考虑是否应该重新抛出（包装成业务允许的受检异常或运行时异常）。</li>
<li><strong>禁止静默处理&nbsp;</strong>InterruptedException：</li>
<li>在JDK的JUC底层源码中，我们可以看到很多声明了InterruptedException的方法，基本上都是对这类方法catch异常，要么继续往外抛出，或者处理完相关资源后，重置中断状态，<strong>绝对不要静默处理。</strong></li>
<li>如果方法没有声明InterruptedException如Runnable.run()，在catch&nbsp;InterruptedException后最好调用Thread.currentThread().interrupt()来恢复中断标记。</li>
<li><strong>正确处理中断</strong>：callable在耗时的loop任务处理中，如果出现了中断异常，因为Java代码中中断只是一种协作方式，其并没真的终止线程，所以一般都是需要我们进行一个中断标志的传递，如线程池中的shutdownNow()就依赖次机制处理。</li>
</ul> </li>
</ul>
<ul>
<li><strong>submit()执行的任务，谨慎处理Future：</strong>
<ul>
<li>使用带过期时间的future.get(long timeOut)获取结果，并要对该方法进行try cache防止其他异常抛出。</li>
<li>多个任务并行处理时，如果有下个请求依赖上个请求，务必使用get()让主线程等待这一结果执行完成后，流转到下一个异步任务。</li>
</ul> </li>
</ul>
<ul>
<li><strong>实现线程Thread的UncaughtExceptionHandler属性</strong>，在自定义的TheadFactory中通过set方法赋值：execute()方法执行时，对于没有捕获的异常使用线程组的兜底统一处理机制。</li>
</ul>
<pre><code>//自定义当前线程组创建线程的统一异常处理，类似于controller的统一异常处理机制
ThreadFactory&nbsp;myThreadFactory&nbsp;=&nbsp;new&nbsp;ThreadFactory() {
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;AtomicInteger&nbsp;atomicInteger&nbsp;=&nbsp;new&nbsp;AtomicInteger(0);
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;String&nbsp;threadNamePrefix&nbsp;=&nbsp;"myThreadFactory-";
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;public&nbsp;Thread&nbsp;newThread(Runnable r)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Thread&nbsp;t&nbsp;=&nbsp;&nbsp;new&nbsp;Thread(r,threadNamePrefix + atomicInteger.getAndIncrement());
&nbsp; &nbsp; &nbsp; &nbsp; t.setUncaughtExceptionHandler((thread, throwable) -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//异常的统一处理，日志打印、兜底处理、监控、资源释放等
&nbsp; &nbsp; &nbsp; &nbsp; System.err.println("线程["&nbsp;+ thread.getName() +&nbsp;"]异常: "&nbsp;+ throwable);});
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;t;
&nbsp; &nbsp; }
};
//构造方法时使用自定义的线程工厂
ExecutorService&nbsp;executor&nbsp;=&nbsp;new&nbsp;ThreadPoolExecutor(
&nbsp; &nbsp; corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
&nbsp; &nbsp; threadFactory,
&nbsp; &nbsp; handler
);</code></pre>
<ul>
<li><strong>使用自定义线程池时建议重写钩子方法afterExecute(Runnable r, Throwable t)</strong>：这个hook方法是用来解决当前任务线程发生的异常，默认是空实现，我们可以重写他，比如进行兜底的线程继续执行，打印日志记录，以及同步失败使用兜底异步处理等等方式。还要注意释放应用中的资源，比如文件锁的占用等，最好手动释放掉，避免底层操作系统线程对这类资源释放失败导致长期占用，最后只能重启Java进程的尴尬地步。</li>
</ul>
<pre><code>public&nbsp;class&nbsp;MyThreadPoolExecutor&nbsp;extends&nbsp;ThreadPoolExecutor&nbsp;{
&nbsp; &nbsp;&nbsp;public&nbsp;MyThreadPoolExecutor(int&nbsp;corePoolSize,&nbsp;int&nbsp;maximumPoolSize,&nbsp;long&nbsp;keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;afterExecute(Runnable r, Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;//需要特别注意任务是否为submit提交，如果是execute提交的任务，那这里很直接的知道任务是否发生异常以及后续去怎么处理
&nbsp; &nbsp; &nbsp; &nbsp;if(r&nbsp;instanceof&nbsp;Future){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(((Future&lt;?&gt;) r).isDone() || ((Future&lt;?&gt;) r).isCancelled()){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//继续使用主线程完成任务,一般不建议，最好使用兜底方式：例如异步发消息，由后续的消费组统一处理异常的任务
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; &nbsp; &nbsp;}else&nbsp;if( t !=&nbsp;null){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//execute异常处理
&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; }
}
//FutureTask 把run方法进行了重写，并且catch住了异常，所以说afterExecute的t 如果是submit提交的方式
//那么t基本上就是null
public&nbsp;void&nbsp;run()&nbsp;{
&nbsp; &nbsp;&nbsp;//....
&nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; Callable&lt;V&gt; c = callable;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(c !=&nbsp;null&nbsp;&amp;&amp; state == NEW) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; V result;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;boolean&nbsp;ran;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; result = c.call();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ran =&nbsp;true;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(Throwable ex) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; result =&nbsp;null;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ran =&nbsp;false;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; setException(ex);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ran)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; set(result);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }&nbsp;finally&nbsp;{
&nbsp; &nbsp; &nbsp;//...
}</code></pre>
<p><strong>afterExecute可以借鉴的示例：</strong></p>
<pre><code>import&nbsp;java.util.concurrent.*;
import&nbsp;java.util.concurrent.atomic.*;
import&nbsp;org.slf4j.*;
public&nbsp;class&nbsp;RobustThreadPool&nbsp;extends&nbsp;ThreadPoolExecutor&nbsp;{
&nbsp; &nbsp;&nbsp;private&nbsp;static&nbsp;final&nbsp;Logger&nbsp;logger&nbsp;=&nbsp;LoggerFactory.getLogger(RobustThreadPool.class);
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;AtomicLong&nbsp;failureCounter&nbsp;=&nbsp;new&nbsp;AtomicLong();
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;RetryPolicy retryPolicy;&nbsp;// 重试策略
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;ThreadLocal&lt;Long&gt; startTime =&nbsp;new&nbsp;ThreadLocal&lt;&gt;();
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;public&nbsp;RobustThreadPool(int&nbsp;corePoolSize,&nbsp;int&nbsp;maxPoolSize,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; BlockingQueue&lt;Runnable&gt; workQueue,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; RetryPolicy retryPolicy) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;super(corePoolSize, maxPoolSize,&nbsp;60L, TimeUnit.SECONDS, workQueue);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.retryPolicy = retryPolicy;
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;beforeExecute(Thread t, Runnable r)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; logger.debug("开始执行任务: {}", r);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;afterExecute(Runnable r, Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 1. 异常分类处理
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(r&nbsp;instanceof&nbsp;Future){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(((Future&lt;?&gt;) r).isDone()){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;//错误记录以及异常处理
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; failureCounter.incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; handleFailure(r, t, costTime);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; &nbsp; &nbsp;}else&nbsp;if( t !=&nbsp;null){
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;//execute异常处理
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; failureCounter.incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; handleFailure(r, t, costTime);
&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 2. 资源清理
&nbsp; &nbsp; &nbsp; &nbsp; cleanThreadLocals();
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;void&nbsp;handleFailure(Runnable r, Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 1. 异常类型识别
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(t&nbsp;instanceof&nbsp;OutOfMemoryError) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.error("JVM内存不足，终止任务: {}", t.getMessage());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.exit(1);&nbsp;// 严重错误直接终止
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 2. 可重试异常处理
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(isRetryable(t)) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;retryCount&nbsp;=&nbsp;retryPolicy.getCurrentRetryCount(r);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(retryCount &lt; retryPolicy.getMaxRetries()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.warn("任务第{}次失败，准备重试...",&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryCount +&nbsp;1, t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryPolicy.retry(r,&nbsp;this);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.error("任务超过最大重试次数({})，转入死信队列",&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryPolicy.getMaxRetries(), t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DeadLetterQueue.add(r, t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 3. 不可重试异常
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.error("不可恢复任务失败", t);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Metrics.recordFailure(t.getClass());&nbsp;// 上报监控
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;boolean&nbsp;isRetryable(Throwable t)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;t&nbsp;instanceof&nbsp;IOException ||&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;t&nbsp;instanceof&nbsp;TimeoutException ||
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(t.getCause() !=&nbsp;null&nbsp;&amp;&amp; isRetryable(t.getCause()));
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;private&nbsp;void&nbsp;cleanThreadLocals()&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 清理可能的内存泄漏
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ThreadLocal&lt;?&gt;[] holders = {&nbsp;/* 其他ThreadLocal */};
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(ThreadLocal&lt;?&gt; holder : holders) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; holder.remove();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(Exception e) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.warn("清理ThreadLocal失败", e);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;// 重试策略嵌套类
&nbsp; &nbsp;&nbsp;public&nbsp;static&nbsp;class&nbsp;RetryPolicy&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;int&nbsp;maxRetries;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;long&nbsp;retryDelayMs;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;Map&lt;Runnable, AtomicInteger&gt; retryMap =&nbsp;new&nbsp;ConcurrentHashMap&lt;&gt;();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;RetryPolicy(int&nbsp;maxRetries,&nbsp;long&nbsp;retryDelayMs)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.maxRetries = maxRetries;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;this.retryDelayMs = retryDelayMs;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;retry(Runnable task, Executor executor)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retryMap.computeIfAbsent(task, k -&gt;&nbsp;new&nbsp;AtomicInteger()).incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(retryDelayMs &gt;&nbsp;0) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.execute(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;try&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Thread.sleep(retryDelayMs);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;catch&nbsp;(InterruptedException ignored) {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.execute(task);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; executor.execute(task);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;int&nbsp;getCurrentRetryCount(Runnable task)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;retryMap.getOrDefault(task,&nbsp;new&nbsp;AtomicInteger()).get();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;public&nbsp;int&nbsp;getMaxRetries()&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;maxRetries;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p><strong>异常处理小结</strong>：要特别注意使用future.get()方法时，我们一定要注意设置超时时间，防止主线程无限期的阻塞避免边缘的业务查询影响了主业务造成得不偿失的效果，另外我们需要注意一个点就是submit()方法的提交任务时，afterExecute(Runnable r, Throwable t)中的t恒为null，如果是execute方法提交的任务，那么就是直接获取的任务执行的异常，对于submit提交的任务异常其被封装到了Futrure&nbsp;包装对象中，一般需要我们再次判断任务时执行完毕还是异常或被取消了，如果发生了异常，Future.get()会抛出封装的ExecutionException异常，当然还可能是取消异常以及中断异常。invokeAll和invokeAny我们需要对返回的Future结果检查可能抛出的异常，对于callable&nbsp;前面一再强调了要对InterruptedException不要静默处理，因为线程的中断标记只是一个协作方式，他并没有停止当前线程的运行，我们需要根据自身的场景对发生的中断进行快速响应以及传递中断标志。</p>
<h1>5.4 拒绝策略实践</h1>
<p>先带大家回顾一下策略是如何触发执行的流程：</p>
<pre><code>//添加任务，当不满足条件时会执行拒绝方法reject
public&nbsp;void&nbsp;execute(Runnable command)&nbsp;{
&nbsp; &nbsp;//...
&nbsp; &nbsp;&nbsp;if&nbsp;(isRunning(c) &amp;&amp; workQueue.offer(command)) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;recheck = ctl.get();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(! isRunning(recheck) &amp;&amp;&nbsp;remove(command))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; reject(command);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(workerCountOf(recheck) ==&nbsp;0)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addWorker(null,&nbsp;false);
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;else&nbsp;if&nbsp;(!addWorker(command,&nbsp;false))
&nbsp; &nbsp; &nbsp; &nbsp; reject(command);
}
//这里就是拒绝的入口。handler是有构造方法传入
final&nbsp;void&nbsp;reject(Runnable command)&nbsp;{
&nbsp; &nbsp; handler.rejectedExecution(command,&nbsp;this);
}
public&nbsp;ThreadPoolExecutor(int&nbsp;corePoolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;maximumPoolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;keepAliveTime,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TimeUnit unit,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; BlockingQueue&lt;Runnable&gt; workQueue,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ThreadFactory threadFactory,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; RejectedExecutionHandler handler) {
&nbsp; &nbsp;&nbsp;//....
&nbsp; &nbsp;&nbsp;//指定拒绝策略
&nbsp; &nbsp;&nbsp;this.handler = handler;
}</code></pre>
<p><strong>AbortPolicy</strong>：默认的拒绝策略，简单粗暴，当execute中添加woker失败时，直接在当前线程抛出异常。</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;AbortPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp; &nbsp;//直接抛出RejectedExecutionException
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;RejectedExecutionException("Task "&nbsp;+ r.toString() +
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" rejected from "&nbsp;+
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;e.toString());
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：快速失败，立即暴露系统过载问题、避免任务静默丢失，便于监控系统捕获</p>
<p><strong>缺点</strong>：需要调用方显式处理异常，增加代码复杂度，可能中断主业务流程</p>
<p><strong>适用场景</strong>：适用于那些对任务丢失非常敏感，配合熔断机制使用的快速失败场景</p>
<p><strong>CallerRunsPolicy</strong>：提交任务的线程，直接执行任务</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;CallerRunsPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;&nbsp;//直接在提交任务的线程中执行任务
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!e.isShutdown()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; r.run();
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：任务都会被执行，不会丢任务，并且由于主线程执行任务，天然的流量控制，避免了大量的任务进入线程池。</p>
<p><strong>缺点</strong>：调用线程可能被阻塞，导致上游服务雪崩。不适合高并发场景（可能拖垮整个调用链）。</p>
<p><strong>适用场景</strong>：适用于处理能力不高，并且资源过载能够平滑过渡，同时不丢失任务的场景。如：低并发、高可靠性的后台任务（如日志归档）、允许同步执行的批处理系统。</p>
<p><strong>DiscardPolicy</strong>：直接丢弃被拒绝的任务，不做任何通知。</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;DiscardPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;//空实现
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：实现简单，无额外性能开销。避免异常传播影响主流程</p>
<p><strong>缺点</strong>：数据静默丢失，可能会掩盖系统容量问题</p>
<p><strong>适用场景</strong>：边缘业务的监控上报数据，统计类的uv、pv统计任务</p>
<p>DiscardOldestPolicy：丢弃队列中最旧的任务，然后重试提交当前任务。</p>
<pre><code>public&nbsp;static&nbsp;class&nbsp;DiscardOldestPolicy&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;&nbsp;//丢弃队列中最旧的任务，重试当前任务
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor e) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!e.isShutdown()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; e.getQueue().poll();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; e.execute(r);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
}</code></pre>
<p><strong>优点</strong>：优先保证新任务执行，避免队列堆积导致内存溢出。</p>
<p><strong>缺点</strong>：可能丢失关键旧任务、任务执行顺序无法保证。</p>
<p><strong>适用场景</strong>：适用于可容忍部分数据丢失，并且实时性要求高于历史数据的场景，比如：行情推送。</p>
<p>通过上线的介绍，我们可以看到JDK内置策略基本上只使用于简单处理的场景，在生产实践中一般推荐我们自定义拒绝策略，进行相关的业务处理。</p>
<p><strong>1. 自定义RejectedExecutionHandler</strong>：</p>
<pre><code>/**
&nbsp;* 带监控统计的拒绝策略处理器
&nbsp;*/
public&nbsp;class&nbsp;MetricsRejectedExecutionHandler&nbsp;implements&nbsp;RejectedExecutionHandler&nbsp;{
&nbsp; &nbsp;&nbsp;private&nbsp;static&nbsp;final&nbsp;org.slf4j.Logger&nbsp;logger&nbsp;=&nbsp;org.slf4j.LoggerFactory.getLogger(MetricsRejectedExecutionHandler.class);
&nbsp; &nbsp;&nbsp;// 统计被拒绝的任务数量
&nbsp; &nbsp;&nbsp;private&nbsp;final&nbsp;AtomicLong&nbsp;rejectedCount&nbsp;=&nbsp;new&nbsp;AtomicLong(0);
&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;public&nbsp;void&nbsp;rejectedExecution(Runnable r, ThreadPoolExecutor executor)&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 1. 采集线程池关键指标
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;poolSize&nbsp;=&nbsp;executor.getPoolSize();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;activeThreads&nbsp;=&nbsp;executor.getActiveCount();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;corePoolSize&nbsp;=&nbsp;executor.getCorePoolSize();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;maxPoolSize&nbsp;=&nbsp;executor.getMaximumPoolSize();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;int&nbsp;queueSize&nbsp;=&nbsp;executor.getQueue().size();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;completedTasks&nbsp;=&nbsp;executor.getCompletedTaskCount();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 2. 递增拒绝计数器
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;long&nbsp;totalRejected&nbsp;=&nbsp;rejectedCount.incrementAndGet();
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 3. 输出警告日志（包含完整指标）
&nbsp; &nbsp; &nbsp; &nbsp; logger.warn("""
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;任务被拒绝执行！线程池状态:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 活跃线程数/当前线程数: {}/{}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 核心/最大线程数: {}/{}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 队列大小: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 已完成任务数: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 历史拒绝总数: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |- 被拒绝任务: {}
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; """,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; activeThreads, poolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; corePoolSize, maxPoolSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; queueSize,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; completedTasks,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; totalRejected,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; r.getClass().getName());
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 4. 可选：降级处理（如存入数据库等待重试）
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// fallbackToDatabase(r);
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 5. 抛出RejectedExecutionException（保持默认行为）
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;throw&nbsp;new&nbsp;RejectedExecutionException("Task "&nbsp;+ r.toString() +&nbsp;" rejected");
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp;&nbsp;// 获取累计拒绝次数（用于监控）
&nbsp; &nbsp;&nbsp;public&nbsp;long&nbsp;getRejectedCount()&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;rejectedCount.get();
&nbsp; &nbsp; }
}</code></pre>
<ul>
<li><strong>记录日志并告警</strong>：所有的异常处理中，最常见简单的方式无外乎，先记录个日志，然后有告警系统的进行相关的某书、某信以及短信等的告警信息推送，方便开发人员以及运维人员的及时发现问题并介入处理。</li>
<li><strong>兜底处理机制</strong>：一般常见的就是通过异步的方式提交到MQ，然后统一进行兜底处理。</li>
<li><strong>带超时和重试的拒绝</strong>：可以尝试等待一小段时间，或者重试几次提交，如果仍然失败，再执行最终的拒绝逻辑（如告警、持久化或抛异常）。</li>
<li><strong>动态调整策略</strong>：根据系统的负载或任务类型，动态的执行兜底策略机制，就如前面写的源码示例方式。</li>
</ul>
<p><strong>2. 根据自身业务场景选择合适的拒绝策略：</strong></p>
<ul>
<li><strong>核心业务，不容丢失</strong>：如果任务非常重要，不能丢失，可以考虑：
<ul>
<li>CallerRunsPolicy：调用线程承担任务执行压力，是否可支撑；</li>
<li>自定义策略：尝试持久化到MQ或DB，然后由专门的消费组补偿任务处理；</li>
<li>AbortPolicy：如果希望系统快速失败并由上层进行重试或熔断。</li>
<li>&nbsp;</li>
</ul> </li>
<li><strong>非核心业务，可容忍部分丢失</strong>：
<ul>
<li>DiscardOldestPolicy：新任务更重要时，如行情推送；</li>
<li>DiscardPolicy：边缘业务场景，比如一些pv统计等，丢失了无所谓；</li>
<li>及时的进行监控查看，了解任务的丢失情况。</li>
</ul> </li>
</ul>
<p><strong>3. 结合线程池参数综合考虑：</strong></p>
<ul>
<li>拒绝策略的选择也与线程池的队列类型（有界/无界）、队列容量、maximumPoolSize等参数密切相关。</li>
<li>如果使用无界队列LinkedBlockingQueue的无参构造，只有机器内存不够时才会进行拒绝策略，不过这种极端场景已经不是影响线程池本身，内存不够可能导致Java进程被操作系统直接kill可能。</li>
<li>如果使用有界队列，需要权衡队列的大小，核心场景甚至可以动态追踪阻塞队列大小，以及动态调整队列大小来保证核心业务的正常流转。</li>
</ul>
<ol>
<li><strong>充分测试和监控</strong>：无论选择哪种策略，都必须在压测环境中充分测试其行为，并在线上环境建立完善的监控体系，监控线程池的各项指标（活跃线程数、队列长度、任务完成数、任务拒绝数等）。当拒绝发生时，应有相应的告警通知。</li>
</ol>
<p><strong>拒绝策略小结</strong>：</p>
<p>策略的选择跟我们大多数的系统设计哲学是保持一致的，都是在应对不同的场景中，做出一定的trade off。最好的策略需要根据业务场景、系统容忍度、资源等方面的综合考量，<strong>一个黄金的实践原则</strong>：拒绝事件做好监控告警、根据业务SLA定义策略，是否可丢失，快速失败等，定期的进行压力测试，验证策略的有效性。</p>
<h1>5.5 池隔离实践</h1>
<p><strong>核心思想</strong>：根据任务的资源类型 、优先级和业务特性 ，划分多个独立的线程池，避免不同性质的任务相互干扰。</p>
<p><strong>1. 隔离维度</strong>：</p>
<ul>
<li><strong>资源类型</strong>：CPU密集型 vs &nbsp;I/O密集型任务</li>
<li><strong>执行时长</strong>：短时任务（毫秒级） vs 长时任务（分钟级）</li>
<li><strong>实时性要求</strong>：高实时性 vs 可延迟（最终一致即可）</li>
<li><strong>业务重要性</strong>：支付交易（高优先级） vs 日志清理（低优先级）</li>
<li><strong>是否依赖外部资源</strong>：例如，访问特定数据库、调用特定第三方API的任务可以归为一类。</li>
</ul>
<p><strong>2. 不同业务场景线程池独立使用</strong>：在不同的业务场景下，为自己的特定业务，创建独立的线程池。</p>
<ul>
<li><strong>线程命名</strong>：通过ThreadFactory为每个线程池及其线程设置有意义的名称，例如netty-io-compress-pool-%d，excel-export-pool-%d, 主要方便区别不同的业务场景以及问题排查。</li>
<li><strong>参数调优</strong>：不同的业务场景设置不同的参数。
<ul>
<li>corePoolSize, maximumPoolSize：CPU密集型的计算任务可以设置小点减少上下文的切换，I/O密集型可以较大，在io阻塞等待期间，多去处理其他任务。</li>
<li>阻塞队列blockQueue：选择合适的队列类型，以及设置合理的队列大小。</li>
<li>RejectedExecutionHandler：有内置的四种的策略以及自定义策略选择，一般建议做好日志、监控以及兜底的处理。</li>
<li>&nbsp;</li>
</ul> </li>
</ul>
<p><strong>3. 自定义Executor避免线程池共用</strong></p>
<pre><code>// 创建CPU密集型任务线程池（线程数=CPU核心数）
ExecutorService cpuIntensiveExecutor =&nbsp;new&nbsp;ThreadPoolExecutor(
&nbsp; &nbsp; Runtime.getRuntime().availableProcessors(),&nbsp;// 核心线程数=CPU核心数
&nbsp; &nbsp; Runtime.getRuntime().availableProcessors(),&nbsp;// 最大线程数=CPU核心数
&nbsp; &nbsp;&nbsp;30L, TimeUnit.SECONDS,
&nbsp; &nbsp;&nbsp;new&nbsp;LinkedBlockingQueue&lt;&gt;(500),
&nbsp; &nbsp;&nbsp;new&nbsp;ThreadFactoryBuilder()
&nbsp; &nbsp; &nbsp; &nbsp; .setNameFormat("cpu-pool-%d")
&nbsp; &nbsp; &nbsp; &nbsp; .setPriority(Thread.MAX_PRIORITY)&nbsp;// 提高优先级
&nbsp; &nbsp; &nbsp; &nbsp; .build(),
&nbsp; &nbsp;&nbsp;new&nbsp;ThreadPoolExecutor.AbortPolicy()&nbsp;// 直接拒绝
);
// 使用示例
CompletableFuture.supplyAsync(() -&gt; {
&nbsp; &nbsp;&nbsp;// 矩阵计算等CPU密集型任务
&nbsp; &nbsp;&nbsp;double[][] result = matrixMultiply(largeMatrixA, largeMatrixB);
&nbsp; &nbsp;&nbsp;return&nbsp;result;
}, cpuIntensiveExecutor)
.thenAccept(result -&gt; {
&nbsp; &nbsp; System.out.println("计算结果维度: "&nbsp;+ result.length +&nbsp;"x"&nbsp;+ result[0].length);
});</code></pre>
<p><strong>线程池隔离小结</strong>：</p>
<p>专池专用的本质是通过物理隔离实现：</p>
<ul>
<li>资源保障 ：关键业务独占线程资源</li>
<li>故障隔离 ：避免级联雪崩</li>
<li>性能优化 ：针对任务类型最大化吞吐量</li>
</ul>
<p>最终呈现的效果是像专业厨房的分区（切配区/炒菜区/面点区）一样，让每个线程池专注处理同类任务，提升整体效率和可靠性。</p>
<h1>六、总结</h1>
<p>线程池是Java并发编程的核心组件，通过复用线程减少资源开销，提升系统吞吐量。其核心设计包括线程复用机制 、任务队列和拒绝策略 ，通过ThreadPoolExecutor的参数（核心线程数、最大线程数、队列容量等）实现灵活的资源控制。线程池的生命周期由RUNNING、SHUTDOWN等状态管理，确保任务有序执行或终止。</p>
<p>内置线程池（如Executors.newCachedThreadPool）虽便捷，但存在内存溢出或无界队列堆积的风险，需谨慎选择。invokeAll的超时失效和submit提交任务的异常消失是常见陷阱需通过正确处理中断和检查Future.get()规避。</p>
<p>最佳实践包括：</p>
<ul>
<li>异常处理：通过afterExecute来对发生的异常进行兜底处理，任务细粒度的try catch或UncaughtExceptionH捕获异常处理防止线程崩溃退出；</li>
<li>拒绝策略：根据业务选择拒绝策略或自定义降级逻辑，生产级应用建议尽量自定义处理；</li>
<li>线程隔离 ：按任务类型（CPU/I/O）或优先级划分线程池，避免资源竞争。</li>
</ul>
<p>合理使用线程池能显著提升性能，但需结合业务场景精细调参，确保稳定性和可维护性，希望这篇文章能给大家带来一些生产实践上的指导，减少一些因为不熟悉线程池相关原理生产误用导致的一些问题。</p>
<h4>往期回顾</h4>
<p>1.&nbsp;基于浏览器扩展 API Mock 工具开发探索｜得物技术</p>
<p>2.&nbsp;破解gh-ost变更导致MySQL表膨胀之谜｜得物技术</p>
<p>3.&nbsp;MySQL单表为何别超2000万行？揭秘B+树与16KB页的生死博弈｜得物技术</p>
<p>4.&nbsp;0基础带你精通Java对象序列化--以Hessian为例｜得物技术</p>
<p>5.&nbsp;前端日志回捞系统的性能优化实践｜得物技术</p>
<h4>文 /舍得</h4>
<p>关注得物技术，每周更新技术干货</p>
<p>要是觉得文章对你有帮助的话，欢迎评论转发点赞～</p>
<p>未经得物技术许可严禁转载，否则依法追究法律责任。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-10e0261f0e.jpg"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-10e0261f0e.jpg" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 14:59:05 +0800</pubDate>
  </item><item>
    <title><![CDATA[苹果将在 ICCV 2025 展示多项前沿视觉研究成果]]></title>
    <link>https://www.oschina.net/news/377355/apple-at-iccv-2025</link>
    <itunes:title><![CDATA[苹果将在 ICCV 2025 展示多项前沿视觉研究成果]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>苹果<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmachinelearning.apple.com%2Fupdates%2Fapple-at-iccv-2025" target="_blank">宣布</a>将携多篇论文亮相<strong> 2025 年国际计算机视觉大会（ICCV 2025）</strong>，展现其在<strong>多模态 AI、图像生成与视频理解</strong>等领域的研究实力。大会将于 10 月 19 日至 23 日在夏威夷檀香山举行。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a28fbc2140.png"></p>
<p>今年，苹果共有<strong>八篇论文</strong> 入选大会，涵盖从文本生成视频的评估方法、三维空间理解、多模态模型的扩展规律，到图像生成与编辑的统一扩散模型等前沿方向。其中包括：</p>
<ul>
<li> <p><strong>ETVA</strong>：一种评估文本与视频对齐度的新方法；</p> </li>
<li> <p><strong>MM-Spatial</strong>：探索多模态大模型的 3D 空间理解能力；</p> </li>
<li> <p><strong>Scaling Laws for Native Multimodal Models</strong>：研究多模态模型的扩展规律；</p> </li>
<li> <p><strong>Stable Diffusion Models are Secretly Good at Visual In-Context Learning</strong>：揭示稳定扩散模型在视觉上下文学习中的潜力；</p> </li>
<li> <p><strong>UniVG</strong>：面向统一图像生成与编辑的通用扩散模型；</p> </li>
<li> <p>以及面向交互式智能体评测的 <strong>UINavBench</strong> 等研究。</p> </li>
</ul>
<p>此外，苹果应用研究经理 <strong>Dr. C. Thomas</strong> 将在大会的工业视觉检测专题中发表主旨演讲。苹果研究团队成员 <strong>Patricia Vitoria Carrera</strong> 与 <strong>Tanya Glozman</strong> 也将参与 “女性在计算机视觉”<em>（<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsites.google.com%2Fview%2Fwicv-iccv-2025" target="_blank">Women in Computer Vision (WiCV) Workshop</a>）</em>工作坊担任导师，支持学术多元化。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>苹果<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmachinelearning.apple.com%2Fupdates%2Fapple-at-iccv-2025" target="_blank">宣布</a>将携多篇论文亮相<strong> 2025 年国际计算机视觉大会（ICCV 2025）</strong>，展现其在<strong>多模态 AI、图像生成与视频理解</strong>等领域的研究实力。大会将于 10 月 19 日至 23 日在夏威夷檀香山举行。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a28fbc2140.png"></p>
<p>今年，苹果共有<strong>八篇论文</strong> 入选大会，涵盖从文本生成视频的评估方法、三维空间理解、多模态模型的扩展规律，到图像生成与编辑的统一扩散模型等前沿方向。其中包括：</p>
<ul>
<li> <p><strong>ETVA</strong>：一种评估文本与视频对齐度的新方法；</p> </li>
<li> <p><strong>MM-Spatial</strong>：探索多模态大模型的 3D 空间理解能力；</p> </li>
<li> <p><strong>Scaling Laws for Native Multimodal Models</strong>：研究多模态模型的扩展规律；</p> </li>
<li> <p><strong>Stable Diffusion Models are Secretly Good at Visual In-Context Learning</strong>：揭示稳定扩散模型在视觉上下文学习中的潜力；</p> </li>
<li> <p><strong>UniVG</strong>：面向统一图像生成与编辑的通用扩散模型；</p> </li>
<li> <p>以及面向交互式智能体评测的 <strong>UINavBench</strong> 等研究。</p> </li>
</ul>
<p>此外，苹果应用研究经理 <strong>Dr. C. Thomas</strong> 将在大会的工业视觉检测专题中发表主旨演讲。苹果研究团队成员 <strong>Patricia Vitoria Carrera</strong> 与 <strong>Tanya Glozman</strong> 也将参与 “女性在计算机视觉”<em>（<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsites.google.com%2Fview%2Fwicv-iccv-2025" target="_blank">Women in Computer Vision (WiCV) Workshop</a>）</em>工作坊担任导师，支持学术多元化。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>苹果<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmachinelearning.apple.com%2Fupdates%2Fapple-at-iccv-2025" target="_blank">宣布</a>将携多篇论文亮相<strong> 2025 年国际计算机视觉大会（ICCV 2025）</strong>，展现其在<strong>多模态 AI、图像生成与视频理解</strong>等领域的研究实力。大会将于 10 月 19 日至 23 日在夏威夷檀香山举行。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a28fbc2140.png"></p>
<p>今年，苹果共有<strong>八篇论文</strong> 入选大会，涵盖从文本生成视频的评估方法、三维空间理解、多模态模型的扩展规律，到图像生成与编辑的统一扩散模型等前沿方向。其中包括：</p>
<ul>
<li> <p><strong>ETVA</strong>：一种评估文本与视频对齐度的新方法；</p> </li>
<li> <p><strong>MM-Spatial</strong>：探索多模态大模型的 3D 空间理解能力；</p> </li>
<li> <p><strong>Scaling Laws for Native Multimodal Models</strong>：研究多模态模型的扩展规律；</p> </li>
<li> <p><strong>Stable Diffusion Models are Secretly Good at Visual In-Context Learning</strong>：揭示稳定扩散模型在视觉上下文学习中的潜力；</p> </li>
<li> <p><strong>UniVG</strong>：面向统一图像生成与编辑的通用扩散模型；</p> </li>
<li> <p>以及面向交互式智能体评测的 <strong>UINavBench</strong> 等研究。</p> </li>
</ul>
<p>此外，苹果应用研究经理 <strong>Dr. C. Thomas</strong> 将在大会的工业视觉检测专题中发表主旨演讲。苹果研究团队成员 <strong>Patricia Vitoria Carrera</strong> 与 <strong>Tanya Glozman</strong> 也将参与 “女性在计算机视觉”<em>（<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsites.google.com%2Fview%2Fwicv-iccv-2025" target="_blank">Women in Computer Vision (WiCV) Workshop</a>）</em>工作坊担任导师，支持学术多元化。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a28fbc2140.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a28fbc2140.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 14:48:30 +0800</pubDate>
  </item><item>
    <title><![CDATA[微软发布首款自主开发图像生成模型 MAI-Image-1]]></title>
    <link>https://www.oschina.net/news/377352/microsoft-ai-mai-image-1-debuting-in-the-top-10-on-lmarena</link>
    <itunes:title><![CDATA[微软发布首款自主开发图像生成模型 MAI-Image-1]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>微软正式推出其首个完全自主研发的图像生成模型 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmicrosoft.ai%2Fnews%2Fintroducing-mai-image-1-debuting-in-the-top-10-on-lmarena%2F" target="_blank">MAI-Image-1</a>，并在知名 AI 模型评测平台 LMArena 上首次亮相就进入了文本到图像模型的 Top 10 排行（<em>https://lmarena.ai/leaderboard/text-to-image</em>）。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-0bba5f7d39.png"></p>
<p>据介绍，MAI-Image-1 专注于高保真、真实感图像生成，能够在光照、细节和构图等方面展现出强大表现，同时兼顾生成速度与效率。微软称该模型的目标是让创作者能以更自然的方式，将文字想法快速转化为视觉作品。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-20979b94b0.png"></p>
<p>该模型目前已在 LMArena 上开放测试，微软计划根据社区反馈持续优化，并逐步将其集成到 Copilot、Bing Image Creator 等产品中。微软强调，MAI-Image-1 的开发遵循负责任的 AI 原则，确保内容生成的安全与合规。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>微软正式推出其首个完全自主研发的图像生成模型 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmicrosoft.ai%2Fnews%2Fintroducing-mai-image-1-debuting-in-the-top-10-on-lmarena%2F" target="_blank">MAI-Image-1</a>，并在知名 AI 模型评测平台 LMArena 上首次亮相就进入了文本到图像模型的 Top 10 排行（<em>https://lmarena.ai/leaderboard/text-to-image</em>）。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-0bba5f7d39.png"></p>
<p>据介绍，MAI-Image-1 专注于高保真、真实感图像生成，能够在光照、细节和构图等方面展现出强大表现，同时兼顾生成速度与效率。微软称该模型的目标是让创作者能以更自然的方式，将文字想法快速转化为视觉作品。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-20979b94b0.png"></p>
<p>该模型目前已在 LMArena 上开放测试，微软计划根据社区反馈持续优化，并逐步将其集成到 Copilot、Bing Image Creator 等产品中。微软强调，MAI-Image-1 的开发遵循负责任的 AI 原则，确保内容生成的安全与合规。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>微软正式推出其首个完全自主研发的图像生成模型 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmicrosoft.ai%2Fnews%2Fintroducing-mai-image-1-debuting-in-the-top-10-on-lmarena%2F" target="_blank">MAI-Image-1</a>，并在知名 AI 模型评测平台 LMArena 上首次亮相就进入了文本到图像模型的 Top 10 排行（<em>https://lmarena.ai/leaderboard/text-to-image</em>）。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-0bba5f7d39.png"></p>
<p>据介绍，MAI-Image-1 专注于高保真、真实感图像生成，能够在光照、细节和构图等方面展现出强大表现，同时兼顾生成速度与效率。微软称该模型的目标是让创作者能以更自然的方式，将文字想法快速转化为视觉作品。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-20979b94b0.png"></p>
<p>该模型目前已在 LMArena 上开放测试，微软计划根据社区反馈持续优化，并逐步将其集成到 Copilot、Bing Image Creator 等产品中。微软强调，MAI-Image-1 的开发遵循负责任的 AI 原则，确保内容生成的安全与合规。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-0bba5f7d39.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-0bba5f7d39.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 14:40:07 +0800</pubDate>
  </item><item>
    <title><![CDATA[中国农业大学发布神农大模型 3.0]]></title>
    <link>https://www.oschina.net/news/377346</link>
    <itunes:title><![CDATA[中国农业大学发布神农大模型 3.0]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><span>中国农业大学发布神农大模型3.0。这是目前全国覆盖农业学科和场景最全的大模型，具备农业知识问答、生产决策推理等功能，标志着我国农业人工智能发展迈入新阶段。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a7bf8ea13a.png"></p>
<p><span>此前，神农大模型1.0主要实现农业知识问答，神农大模型2.0主要具备多模态能力并向农业应用延伸。此次发布的神农大模型3.0则聚焦36个农业智能体，模型以“小体积、高智能、低成本”为核心突破，推出32B、7B、1B三个版本，通过动态稀疏机制与增量压缩技术实现模型算力缩小50%，同时关键任务性能损失低于1%，全面推动农业AI从“能用”向“好用、普惠”跃升。</span></p>
<p><span>团队同步推出“神农大模型智能体平台”，构建覆盖农业全链路的轻量级、可部署、可协同的AI应用生态。平台深度融合神农大模型3.0，兼容主流通用大模型，支持智能体在端、边、云环境中跨平台灵活部署。现已上线智慧育种、智慧种植、智慧养殖、遥感气象、智慧教育等六大类共36个智能体，提供低代码开发工具，支持快速构建与迭代智能体，兼容30多种编程语言开发的应用程序接入，并遵循MCP服务协议实现知识库与结构化数据的便捷接入。智能体间支持相互调用与通信，形成任务协同网络，推动农业AI从“单点智能”走向“系统智能”。</span></p>
<p><span>同时，三款专用一体机业同步推出——“神农-启航”（适配1B模型）、“神农-拓疆”（适配7B模型）与“神农-至臻”（适配32B模型）。一体机支持完全断网运行，具备“开箱即用”特性，确保农业数据不出本地，大幅提升数据隐私与作业安全性。其三防设计（防水、防尘、防震）适配田间地头、温室大棚、加工车间等恶劣环境，为农业用户提供软硬一体、自主可控的轻量化AI解决方案。</span></p>
<p><span>目前，神农大模型已在北京周边、内蒙古、黑龙江等地区推广应用，服务用户超10万人。</span></p>
<p><span>中国农大介绍，大模型以“神农”命名，是对神农氏这一农业之神和医药之祖的致敬，也是对中华优秀传统农耕文化的传承和弘扬。未来，学校还将持续对大模型开展研发升级，让它在更大范围内普惠落地。</span></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><span>中国农业大学发布神农大模型3.0。这是目前全国覆盖农业学科和场景最全的大模型，具备农业知识问答、生产决策推理等功能，标志着我国农业人工智能发展迈入新阶段。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a7bf8ea13a.png"></p>
<p><span>此前，神农大模型1.0主要实现农业知识问答，神农大模型2.0主要具备多模态能力并向农业应用延伸。此次发布的神农大模型3.0则聚焦36个农业智能体，模型以“小体积、高智能、低成本”为核心突破，推出32B、7B、1B三个版本，通过动态稀疏机制与增量压缩技术实现模型算力缩小50%，同时关键任务性能损失低于1%，全面推动农业AI从“能用”向“好用、普惠”跃升。</span></p>
<p><span>团队同步推出“神农大模型智能体平台”，构建覆盖农业全链路的轻量级、可部署、可协同的AI应用生态。平台深度融合神农大模型3.0，兼容主流通用大模型，支持智能体在端、边、云环境中跨平台灵活部署。现已上线智慧育种、智慧种植、智慧养殖、遥感气象、智慧教育等六大类共36个智能体，提供低代码开发工具，支持快速构建与迭代智能体，兼容30多种编程语言开发的应用程序接入，并遵循MCP服务协议实现知识库与结构化数据的便捷接入。智能体间支持相互调用与通信，形成任务协同网络，推动农业AI从“单点智能”走向“系统智能”。</span></p>
<p><span>同时，三款专用一体机业同步推出——“神农-启航”（适配1B模型）、“神农-拓疆”（适配7B模型）与“神农-至臻”（适配32B模型）。一体机支持完全断网运行，具备“开箱即用”特性，确保农业数据不出本地，大幅提升数据隐私与作业安全性。其三防设计（防水、防尘、防震）适配田间地头、温室大棚、加工车间等恶劣环境，为农业用户提供软硬一体、自主可控的轻量化AI解决方案。</span></p>
<p><span>目前，神农大模型已在北京周边、内蒙古、黑龙江等地区推广应用，服务用户超10万人。</span></p>
<p><span>中国农大介绍，大模型以“神农”命名，是对神农氏这一农业之神和医药之祖的致敬，也是对中华优秀传统农耕文化的传承和弘扬。未来，学校还将持续对大模型开展研发升级，让它在更大范围内普惠落地。</span></p>]]>
    </description>
    <content:encoded><![CDATA[<p><span>中国农业大学发布神农大模型3.0。这是目前全国覆盖农业学科和场景最全的大模型，具备农业知识问答、生产决策推理等功能，标志着我国农业人工智能发展迈入新阶段。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a7bf8ea13a.png"></p>
<p><span>此前，神农大模型1.0主要实现农业知识问答，神农大模型2.0主要具备多模态能力并向农业应用延伸。此次发布的神农大模型3.0则聚焦36个农业智能体，模型以“小体积、高智能、低成本”为核心突破，推出32B、7B、1B三个版本，通过动态稀疏机制与增量压缩技术实现模型算力缩小50%，同时关键任务性能损失低于1%，全面推动农业AI从“能用”向“好用、普惠”跃升。</span></p>
<p><span>团队同步推出“神农大模型智能体平台”，构建覆盖农业全链路的轻量级、可部署、可协同的AI应用生态。平台深度融合神农大模型3.0，兼容主流通用大模型，支持智能体在端、边、云环境中跨平台灵活部署。现已上线智慧育种、智慧种植、智慧养殖、遥感气象、智慧教育等六大类共36个智能体，提供低代码开发工具，支持快速构建与迭代智能体，兼容30多种编程语言开发的应用程序接入，并遵循MCP服务协议实现知识库与结构化数据的便捷接入。智能体间支持相互调用与通信，形成任务协同网络，推动农业AI从“单点智能”走向“系统智能”。</span></p>
<p><span>同时，三款专用一体机业同步推出——“神农-启航”（适配1B模型）、“神农-拓疆”（适配7B模型）与“神农-至臻”（适配32B模型）。一体机支持完全断网运行，具备“开箱即用”特性，确保农业数据不出本地，大幅提升数据隐私与作业安全性。其三防设计（防水、防尘、防震）适配田间地头、温室大棚、加工车间等恶劣环境，为农业用户提供软硬一体、自主可控的轻量化AI解决方案。</span></p>
<p><span>目前，神农大模型已在北京周边、内蒙古、黑龙江等地区推广应用，服务用户超10万人。</span></p>
<p><span>中国农大介绍，大模型以“神农”命名，是对神农氏这一农业之神和医药之祖的致敬，也是对中华优秀传统农耕文化的传承和弘扬。未来，学校还将持续对大模型开展研发升级，让它在更大范围内普惠落地。</span></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a7bf8ea13a.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a7bf8ea13a.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 14:20:20 +0800</pubDate>
  </item><item>
    <title><![CDATA[团结引擎 1.7.1 preview 发布]]></title>
    <link>https://www.oschina.net/news/377344/unity-tuanjie-1-7-1-preview</link>
    <itunes:title><![CDATA[团结引擎 1.7.1 preview 发布]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>团结引擎 1.7.1 preview 已发布！本次技术更新涵盖渲染（Rendering）、实时动态全局光照系统（TuanjieGI）、团结粒子系统（Particle System）、团结动画系统（Animation）、ShaderGraph、Content Pipeline、 小游戏（MiniGame) 以及 Platform 几大方向。</p>
<h3><strong>渲染（Rendering）</strong></h3>
<p>在1.7.1 preview 版本中做了如下更新：</p>
<h4><strong>虚拟几何体功能支持通用渲染管线（URP）：</strong></h4>
<p>在 Tuanjie 1.7.1 preview 中，虚拟几何体功能进一步拓展了对团结引擎已有渲染管线的支持。重新设计模型的导入设置 UI，为开发者带来更优的模型导入使用体验。优化虚拟几何体执行效率，提升渲染性能。</p>
<ul>
<li>虚拟几何体功能现已支持团结引擎的通用渲染管线（URP），为其带来了同屏实时流畅渲染上亿三角形的能力。</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-6c864d82f1.png"></p>
<ul>
<li>使用虚拟几何体的渲染方式优化阴影绘制过程，使用 GPU 剔除，以 Multi View 的方式绘制光源所有的 Cascade，大幅减少阴影绘制所需的 Draw Call 数量。</li>
<li>进一步优化虚拟几何体的 CPU 和 GPU 执行效率，提升渲染性能。提供固定压缩比例的 Cluster 数据编码方式，获取更好的 GPU 性能。</li>
<li>重新设计的模型导入设置 UI，为开发者提供了更方便快捷管理的跨平台设置调整虚拟几何体导入参数的工作流。</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7fec8d849a.png"></p>
<p>更多使用指南参考 Tuanjie 虚拟几何体手册： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2Ftuanjiemanual%2FManual%2FVirtualGeometry.html" target="_blank">https://docs.unity.cn/cn/tuanjiemanual/Manual/VirtualGeometry.html</a></p>
<h3><strong>实时动态全局光照（TuanjieGI）</strong></h3>
<p>在 Tuanjie 1.7.1 preview 中对实时动态全局光照系统做了以下功能改进：</p>
<ul>
<li><strong>新增 TuanjieGI 参数调节功能，通过 Volume Profile 提供灵活的参数配置管理，使开发者能够精细化调整全局或局部光照效果。</strong>开发者可自由调节直接光、天光及自发光在反弹后的颜色系数，并对最终计算出的间接光进行优化。借助该功能，开发者能够便捷地控制间接光照强度及颜色，打造理想的光线氛围</li>
<li><strong>新增可视化调试器，便于开发者调试实时光照效果</strong>，可通过 Window &gt; Analysis &gt; Rendering Debugger &gt; TuanjieGI 快速体验。</li>
<li><strong>新增 Screen Tracing 功能</strong>，利用屏幕空间信息模拟光线追踪效果，可通过 Volume Profile 控制开启或关闭。</li>
<li>支持 4K 及更高分辨率输出。</li>
<li>优化渲染性能并降低显存占用。</li>
</ul>
<p>获取 TuanjieGI 官方 Demo：</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpackages.cdn.unity.cn%2Fgi%2FTuanjieGI_Demo_Tower_171.zip" target="_blank">https://packages.cdn.unity.cn/gi/TuanjieGI_Demo_Tower_171.zip</a></p>
</blockquote>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcnb.cool%2Ftuanjie%2FTuanjieGI_Demo_Tower" target="_blank">https://cnb.cool/tuanjie/TuanjieGI_Demo_Tower</a></p>
</blockquote>
<p>更多使用指南参考 TuanjieGI 手册：</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2Ftuanjiemanual%2FManual%2FTuanjieGI.html" target="_blank">https://docs.unity.cn/cn/tuanjiemanual/Manual/TuanjieGI.html</a></p>
</blockquote>
<h3><strong>团结动画系统 (Animation）</strong></h3>
<p>在 Tuanjie 1.7.1 preview 中，团结动画系统在 1.7.0_preview 推出的 AnimGraph 和 IK&amp;Retargeter 模块的基础上持续进行升级、优化，主要包括：</p>
<ul>
<li><strong>全新推出 RigGraph（骨骼程序化绑定编辑器）</strong>，通过可视化界面进行模型骨骼绑定，支持一键生成高级骨骼绑定方案或自定义控制器进行个性化模型绑定</li>
<li><strong>升级 AnimGraph</strong>，包括添加反射 Missing 检测、Node Binding 等功能和 ApplyAdditive Node、Constraints Node、Mirror Node 等节点，提升使用体验；</li>
<li><strong>IK&amp;Retargeter 中新增基于 PBD 的 Fullbody IK 功能</strong>，同时修复了一系列 preview 版本中的异常，在前一版本基础上优化、提升了 Editor 的使用。</li>
</ul>
<p>更多使用指南参考 Tuanjie Animation 用户手册：</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2Ftuanjiemanual%2FManual%2FAnimation-whats-new.html" target="_blank">https://docs.unity.cn/cn/tuanjiemanual/Manual/Animation-whats-new.html</a></p>
</blockquote>
<h3><strong>团结粒子系统（Particle System)</strong></h3>
<p>在 Tuanjie 1.7.1 preview 中，优化了 Infinity 粒子系统的性能。按以下步骤即可快速体验：</p>
<p>1.打开或新建一个 URP 项目（未来将支持 HDRP 管线）。</p>
<p>2.打开 <strong>Package Manager</strong>，点击左上角的 <strong>+</strong> 图标，选择 <strong>Add package by name</strong>，输入 cn.tuanjie.infinity 并安装。</p>
<p>3.在 <strong>Hierarchy</strong> 面板中右键单击，从 <strong>Effects</strong> 折叠菜单中选择 <strong>Infinity Particle System</strong>，即可向场景添加粒子系统。其操作方式与内置粒子系统保持一致。</p>
<p>4.通过菜单 <strong>Tools &gt; Convert Particle System to Infinity</strong>，可轻松迁移现有粒子系统资产。</p>
<p>更多使用指南参阅文档 :</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2FPackages-cn%2Fcn.tuanjie.infinity%401.2%2Fmanual%2F" target="_blank">https://docs.unity.cn/cn/Packages-cn/cn.tuanjie.infinity@1.2/manual/</a></p>
</blockquote>
<h3><strong>Shader Graph</strong></h3>
<p>在本次更新中进行了如下优化：</p>
<h4><strong>易用性优化</strong></h4>
<p>在最新版本的团结引擎 Shader Graph 中对 <strong>Portal Nodes（入口/出口节点）</strong> 的快捷创建功能进行了优化，让开发者的工作流更加高效、直观。</p>
<ul>
<li>一键生成多个 Get Var Node，自动贴近端口</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c28a5b5f1e.gif"></p>
<ul>
<li>新建 Group 智能命名：框选若干与 Register 节点相连的节点，右键选择 Group Selection 或直接按 Ctrl + G，新建的 Group 框顶部会自动显示其中 Register 节点的变量名集合，省去手动修改。</li>
</ul>
<h4><strong>Scalable Lit 支持 Custom Pass</strong></h4>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c9dd92f793.png"></p>
<p>在新版本中，用户可以在 Scalable Lit 单个 Shader Graph中定义和配置多个 Pass。每个自定义 Pass 拥有独立的顶点和片段着色器，并可配置其渲染状态（如混合模式、深度测试等）。上图展示了使用多 Pass 可实现一些特定的渲染效果。</p>
<h4><strong>支持URP Stencil Test</strong></h4>
<p>现在可在 Shader Graph 中直接使用和配置 Stencil Buffer。方便快速实现游戏中常用的遮挡描边/透视效果。</p>
<h3><strong>Content Pipeline</strong></h3>
<p>在 Tuaniie 1.7.1 preview 版本中，深入理解开发者对 <strong>Instant Asset</strong> 功能模块的体验和建议，并进行了多项升级：</p>
<ul>
<li> <p><strong>新增 Instant Asset 场景资产构建与编辑器模拟运行</strong>：支持场景资产构建能力，同时支持了在编辑器中直接模拟运行测试，加速项目开发迭代流程；</p> </li>
<li> <p><strong>新增 Instant Asset 资源表管理接口与确定性资产构建</strong>：支持资源表合并、Diff 接口和确定性资产构建机制，确保增量构建精准高效，完美适配热更新等第三方解决方案，轻松应对复杂项目兼容挑战。</p> </li>
</ul>
<h3><strong>小游戏（MiniGame）</strong></h3>
<p>在 Tuanjie 1.7.1 preview 版本中，团结引擎小游戏平台<strong>新增了对 TapTap 子平台的支持。</strong>此次更新进一步扩展了平台发布能力，对 TapTap 子平台的构建支持，使开发者能够更便捷地发布游戏至 TapTap 平台，触达更广泛的玩家群体。</p>
<h3><strong>Platform</strong></h3>
<h4><strong>OpenHarmony 平台</strong></h4>
<ul>
<li>更完善的 UAAL 模式，从 1.7.1 的版本开始，导出工程的 ETS 代码会被封装到 har 包里面去，并且 Default 的模式不再提供，UAAL 成为 Default 模式。模块集成到原生应用更加的便捷。</li>
<li>新增蓝牙键盘鼠标支持，可在 PC/平板中使用该能力。</li>
<li>Webview 接口新增 registerJavaScriptProxy API 和 border 属性相关 API。</li>
<li>新增 Strip Engine Code 支持，进一步降低包体占用。</li>
<li>开发工具上，除了此前支持的 ARM 架构 DevEco Studio 的 Emulator，完善了 x64 架构适配支持。目前 DevEco Studio 的 Emulator 可以在 Window 跟 Mac 上均可流畅使用。</li>
</ul>
<p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.unity.cn%2Fprojects%2F68e72ddcedbc2a001ec6b74e" target="_blank">查看官方公告</a>。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>团结引擎 1.7.1 preview 已发布！本次技术更新涵盖渲染（Rendering）、实时动态全局光照系统（TuanjieGI）、团结粒子系统（Particle System）、团结动画系统（Animation）、ShaderGraph、Content Pipeline、 小游戏（MiniGame) 以及 Platform 几大方向。</p>
<h3><strong>渲染（Rendering）</strong></h3>
<p>在1.7.1 preview 版本中做了如下更新：</p>
<h4><strong>虚拟几何体功能支持通用渲染管线（URP）：</strong></h4>
<p>在 Tuanjie 1.7.1 preview 中，虚拟几何体功能进一步拓展了对团结引擎已有渲染管线的支持。重新设计模型的导入设置 UI，为开发者带来更优的模型导入使用体验。优化虚拟几何体执行效率，提升渲染性能。</p>
<ul>
<li>虚拟几何体功能现已支持团结引擎的通用渲染管线（URP），为其带来了同屏实时流畅渲染上亿三角形的能力。</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-6c864d82f1.png"></p>
<ul>
<li>使用虚拟几何体的渲染方式优化阴影绘制过程，使用 GPU 剔除，以 Multi View 的方式绘制光源所有的 Cascade，大幅减少阴影绘制所需的 Draw Call 数量。</li>
<li>进一步优化虚拟几何体的 CPU 和 GPU 执行效率，提升渲染性能。提供固定压缩比例的 Cluster 数据编码方式，获取更好的 GPU 性能。</li>
<li>重新设计的模型导入设置 UI，为开发者提供了更方便快捷管理的跨平台设置调整虚拟几何体导入参数的工作流。</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7fec8d849a.png"></p>
<p>更多使用指南参考 Tuanjie 虚拟几何体手册： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2Ftuanjiemanual%2FManual%2FVirtualGeometry.html" target="_blank">https://docs.unity.cn/cn/tuanjiemanual/Manual/VirtualGeometry.html</a></p>
<h3><strong>实时动态全局光照（TuanjieGI）</strong></h3>
<p>在 Tuanjie 1.7.1 preview 中对实时动态全局光照系统做了以下功能改进：</p>
<ul>
<li><strong>新增 TuanjieGI 参数调节功能，通过 Volume Profile 提供灵活的参数配置管理，使开发者能够精细化调整全局或局部光照效果。</strong>开发者可自由调节直接光、天光及自发光在反弹后的颜色系数，并对最终计算出的间接光进行优化。借助该功能，开发者能够便捷地控制间接光照强度及颜色，打造理想的光线氛围</li>
<li><strong>新增可视化调试器，便于开发者调试实时光照效果</strong>，可通过 Window &gt; Analysis &gt; Rendering Debugger &gt; TuanjieGI 快速体验。</li>
<li><strong>新增 Screen Tracing 功能</strong>，利用屏幕空间信息模拟光线追踪效果，可通过 Volume Profile 控制开启或关闭。</li>
<li>支持 4K 及更高分辨率输出。</li>
<li>优化渲染性能并降低显存占用。</li>
</ul>
<p>获取 TuanjieGI 官方 Demo：</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpackages.cdn.unity.cn%2Fgi%2FTuanjieGI_Demo_Tower_171.zip" target="_blank">https://packages.cdn.unity.cn/gi/TuanjieGI_Demo_Tower_171.zip</a></p>
</blockquote>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcnb.cool%2Ftuanjie%2FTuanjieGI_Demo_Tower" target="_blank">https://cnb.cool/tuanjie/TuanjieGI_Demo_Tower</a></p>
</blockquote>
<p>更多使用指南参考 TuanjieGI 手册：</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2Ftuanjiemanual%2FManual%2FTuanjieGI.html" target="_blank">https://docs.unity.cn/cn/tuanjiemanual/Manual/TuanjieGI.html</a></p>
</blockquote>
<h3><strong>团结动画系统 (Animation）</strong></h3>
<p>在 Tuanjie 1.7.1 preview 中，团结动画系统在 1.7.0_preview 推出的 AnimGraph 和 IK&amp;Retargeter 模块的基础上持续进行升级、优化，主要包括：</p>
<ul>
<li><strong>全新推出 RigGraph（骨骼程序化绑定编辑器）</strong>，通过可视化界面进行模型骨骼绑定，支持一键生成高级骨骼绑定方案或自定义控制器进行个性化模型绑定</li>
<li><strong>升级 AnimGraph</strong>，包括添加反射 Missing 检测、Node Binding 等功能和 ApplyAdditive Node、Constraints Node、Mirror Node 等节点，提升使用体验；</li>
<li><strong>IK&amp;Retargeter 中新增基于 PBD 的 Fullbody IK 功能</strong>，同时修复了一系列 preview 版本中的异常，在前一版本基础上优化、提升了 Editor 的使用。</li>
</ul>
<p>更多使用指南参考 Tuanjie Animation 用户手册：</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2Ftuanjiemanual%2FManual%2FAnimation-whats-new.html" target="_blank">https://docs.unity.cn/cn/tuanjiemanual/Manual/Animation-whats-new.html</a></p>
</blockquote>
<h3><strong>团结粒子系统（Particle System)</strong></h3>
<p>在 Tuanjie 1.7.1 preview 中，优化了 Infinity 粒子系统的性能。按以下步骤即可快速体验：</p>
<p>1.打开或新建一个 URP 项目（未来将支持 HDRP 管线）。</p>
<p>2.打开 <strong>Package Manager</strong>，点击左上角的 <strong>+</strong> 图标，选择 <strong>Add package by name</strong>，输入 cn.tuanjie.infinity 并安装。</p>
<p>3.在 <strong>Hierarchy</strong> 面板中右键单击，从 <strong>Effects</strong> 折叠菜单中选择 <strong>Infinity Particle System</strong>，即可向场景添加粒子系统。其操作方式与内置粒子系统保持一致。</p>
<p>4.通过菜单 <strong>Tools &gt; Convert Particle System to Infinity</strong>，可轻松迁移现有粒子系统资产。</p>
<p>更多使用指南参阅文档 :</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2FPackages-cn%2Fcn.tuanjie.infinity%401.2%2Fmanual%2F" target="_blank">https://docs.unity.cn/cn/Packages-cn/cn.tuanjie.infinity@1.2/manual/</a></p>
</blockquote>
<h3><strong>Shader Graph</strong></h3>
<p>在本次更新中进行了如下优化：</p>
<h4><strong>易用性优化</strong></h4>
<p>在最新版本的团结引擎 Shader Graph 中对 <strong>Portal Nodes（入口/出口节点）</strong> 的快捷创建功能进行了优化，让开发者的工作流更加高效、直观。</p>
<ul>
<li>一键生成多个 Get Var Node，自动贴近端口</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c28a5b5f1e.gif"></p>
<ul>
<li>新建 Group 智能命名：框选若干与 Register 节点相连的节点，右键选择 Group Selection 或直接按 Ctrl + G，新建的 Group 框顶部会自动显示其中 Register 节点的变量名集合，省去手动修改。</li>
</ul>
<h4><strong>Scalable Lit 支持 Custom Pass</strong></h4>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c9dd92f793.png"></p>
<p>在新版本中，用户可以在 Scalable Lit 单个 Shader Graph中定义和配置多个 Pass。每个自定义 Pass 拥有独立的顶点和片段着色器，并可配置其渲染状态（如混合模式、深度测试等）。上图展示了使用多 Pass 可实现一些特定的渲染效果。</p>
<h4><strong>支持URP Stencil Test</strong></h4>
<p>现在可在 Shader Graph 中直接使用和配置 Stencil Buffer。方便快速实现游戏中常用的遮挡描边/透视效果。</p>
<h3><strong>Content Pipeline</strong></h3>
<p>在 Tuaniie 1.7.1 preview 版本中，深入理解开发者对 <strong>Instant Asset</strong> 功能模块的体验和建议，并进行了多项升级：</p>
<ul>
<li> <p><strong>新增 Instant Asset 场景资产构建与编辑器模拟运行</strong>：支持场景资产构建能力，同时支持了在编辑器中直接模拟运行测试，加速项目开发迭代流程；</p> </li>
<li> <p><strong>新增 Instant Asset 资源表管理接口与确定性资产构建</strong>：支持资源表合并、Diff 接口和确定性资产构建机制，确保增量构建精准高效，完美适配热更新等第三方解决方案，轻松应对复杂项目兼容挑战。</p> </li>
</ul>
<h3><strong>小游戏（MiniGame）</strong></h3>
<p>在 Tuanjie 1.7.1 preview 版本中，团结引擎小游戏平台<strong>新增了对 TapTap 子平台的支持。</strong>此次更新进一步扩展了平台发布能力，对 TapTap 子平台的构建支持，使开发者能够更便捷地发布游戏至 TapTap 平台，触达更广泛的玩家群体。</p>
<h3><strong>Platform</strong></h3>
<h4><strong>OpenHarmony 平台</strong></h4>
<ul>
<li>更完善的 UAAL 模式，从 1.7.1 的版本开始，导出工程的 ETS 代码会被封装到 har 包里面去，并且 Default 的模式不再提供，UAAL 成为 Default 模式。模块集成到原生应用更加的便捷。</li>
<li>新增蓝牙键盘鼠标支持，可在 PC/平板中使用该能力。</li>
<li>Webview 接口新增 registerJavaScriptProxy API 和 border 属性相关 API。</li>
<li>新增 Strip Engine Code 支持，进一步降低包体占用。</li>
<li>开发工具上，除了此前支持的 ARM 架构 DevEco Studio 的 Emulator，完善了 x64 架构适配支持。目前 DevEco Studio 的 Emulator 可以在 Window 跟 Mac 上均可流畅使用。</li>
</ul>
<p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.unity.cn%2Fprojects%2F68e72ddcedbc2a001ec6b74e" target="_blank">查看官方公告</a>。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>团结引擎 1.7.1 preview 已发布！本次技术更新涵盖渲染（Rendering）、实时动态全局光照系统（TuanjieGI）、团结粒子系统（Particle System）、团结动画系统（Animation）、ShaderGraph、Content Pipeline、 小游戏（MiniGame) 以及 Platform 几大方向。</p>
<h3><strong>渲染（Rendering）</strong></h3>
<p>在1.7.1 preview 版本中做了如下更新：</p>
<h4><strong>虚拟几何体功能支持通用渲染管线（URP）：</strong></h4>
<p>在 Tuanjie 1.7.1 preview 中，虚拟几何体功能进一步拓展了对团结引擎已有渲染管线的支持。重新设计模型的导入设置 UI，为开发者带来更优的模型导入使用体验。优化虚拟几何体执行效率，提升渲染性能。</p>
<ul>
<li>虚拟几何体功能现已支持团结引擎的通用渲染管线（URP），为其带来了同屏实时流畅渲染上亿三角形的能力。</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-6c864d82f1.png"></p>
<ul>
<li>使用虚拟几何体的渲染方式优化阴影绘制过程，使用 GPU 剔除，以 Multi View 的方式绘制光源所有的 Cascade，大幅减少阴影绘制所需的 Draw Call 数量。</li>
<li>进一步优化虚拟几何体的 CPU 和 GPU 执行效率，提升渲染性能。提供固定压缩比例的 Cluster 数据编码方式，获取更好的 GPU 性能。</li>
<li>重新设计的模型导入设置 UI，为开发者提供了更方便快捷管理的跨平台设置调整虚拟几何体导入参数的工作流。</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7fec8d849a.png"></p>
<p>更多使用指南参考 Tuanjie 虚拟几何体手册： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2Ftuanjiemanual%2FManual%2FVirtualGeometry.html" target="_blank">https://docs.unity.cn/cn/tuanjiemanual/Manual/VirtualGeometry.html</a></p>
<h3><strong>实时动态全局光照（TuanjieGI）</strong></h3>
<p>在 Tuanjie 1.7.1 preview 中对实时动态全局光照系统做了以下功能改进：</p>
<ul>
<li><strong>新增 TuanjieGI 参数调节功能，通过 Volume Profile 提供灵活的参数配置管理，使开发者能够精细化调整全局或局部光照效果。</strong>开发者可自由调节直接光、天光及自发光在反弹后的颜色系数，并对最终计算出的间接光进行优化。借助该功能，开发者能够便捷地控制间接光照强度及颜色，打造理想的光线氛围</li>
<li><strong>新增可视化调试器，便于开发者调试实时光照效果</strong>，可通过 Window &gt; Analysis &gt; Rendering Debugger &gt; TuanjieGI 快速体验。</li>
<li><strong>新增 Screen Tracing 功能</strong>，利用屏幕空间信息模拟光线追踪效果，可通过 Volume Profile 控制开启或关闭。</li>
<li>支持 4K 及更高分辨率输出。</li>
<li>优化渲染性能并降低显存占用。</li>
</ul>
<p>获取 TuanjieGI 官方 Demo：</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpackages.cdn.unity.cn%2Fgi%2FTuanjieGI_Demo_Tower_171.zip" target="_blank">https://packages.cdn.unity.cn/gi/TuanjieGI_Demo_Tower_171.zip</a></p>
</blockquote>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcnb.cool%2Ftuanjie%2FTuanjieGI_Demo_Tower" target="_blank">https://cnb.cool/tuanjie/TuanjieGI_Demo_Tower</a></p>
</blockquote>
<p>更多使用指南参考 TuanjieGI 手册：</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2Ftuanjiemanual%2FManual%2FTuanjieGI.html" target="_blank">https://docs.unity.cn/cn/tuanjiemanual/Manual/TuanjieGI.html</a></p>
</blockquote>
<h3><strong>团结动画系统 (Animation）</strong></h3>
<p>在 Tuanjie 1.7.1 preview 中，团结动画系统在 1.7.0_preview 推出的 AnimGraph 和 IK&amp;Retargeter 模块的基础上持续进行升级、优化，主要包括：</p>
<ul>
<li><strong>全新推出 RigGraph（骨骼程序化绑定编辑器）</strong>，通过可视化界面进行模型骨骼绑定，支持一键生成高级骨骼绑定方案或自定义控制器进行个性化模型绑定</li>
<li><strong>升级 AnimGraph</strong>，包括添加反射 Missing 检测、Node Binding 等功能和 ApplyAdditive Node、Constraints Node、Mirror Node 等节点，提升使用体验；</li>
<li><strong>IK&amp;Retargeter 中新增基于 PBD 的 Fullbody IK 功能</strong>，同时修复了一系列 preview 版本中的异常，在前一版本基础上优化、提升了 Editor 的使用。</li>
</ul>
<p>更多使用指南参考 Tuanjie Animation 用户手册：</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2Ftuanjiemanual%2FManual%2FAnimation-whats-new.html" target="_blank">https://docs.unity.cn/cn/tuanjiemanual/Manual/Animation-whats-new.html</a></p>
</blockquote>
<h3><strong>团结粒子系统（Particle System)</strong></h3>
<p>在 Tuanjie 1.7.1 preview 中，优化了 Infinity 粒子系统的性能。按以下步骤即可快速体验：</p>
<p>1.打开或新建一个 URP 项目（未来将支持 HDRP 管线）。</p>
<p>2.打开 <strong>Package Manager</strong>，点击左上角的 <strong>+</strong> 图标，选择 <strong>Add package by name</strong>，输入 cn.tuanjie.infinity 并安装。</p>
<p>3.在 <strong>Hierarchy</strong> 面板中右键单击，从 <strong>Effects</strong> 折叠菜单中选择 <strong>Infinity Particle System</strong>，即可向场景添加粒子系统。其操作方式与内置粒子系统保持一致。</p>
<p>4.通过菜单 <strong>Tools &gt; Convert Particle System to Infinity</strong>，可轻松迁移现有粒子系统资产。</p>
<p>更多使用指南参阅文档 :</p>
<blockquote>
<p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.unity.cn%2Fcn%2FPackages-cn%2Fcn.tuanjie.infinity%401.2%2Fmanual%2F" target="_blank">https://docs.unity.cn/cn/Packages-cn/cn.tuanjie.infinity@1.2/manual/</a></p>
</blockquote>
<h3><strong>Shader Graph</strong></h3>
<p>在本次更新中进行了如下优化：</p>
<h4><strong>易用性优化</strong></h4>
<p>在最新版本的团结引擎 Shader Graph 中对 <strong>Portal Nodes（入口/出口节点）</strong> 的快捷创建功能进行了优化，让开发者的工作流更加高效、直观。</p>
<ul>
<li>一键生成多个 Get Var Node，自动贴近端口</li>
</ul>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c28a5b5f1e.gif"></p>
<ul>
<li>新建 Group 智能命名：框选若干与 Register 节点相连的节点，右键选择 Group Selection 或直接按 Ctrl + G，新建的 Group 框顶部会自动显示其中 Register 节点的变量名集合，省去手动修改。</li>
</ul>
<h4><strong>Scalable Lit 支持 Custom Pass</strong></h4>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c9dd92f793.png"></p>
<p>在新版本中，用户可以在 Scalable Lit 单个 Shader Graph中定义和配置多个 Pass。每个自定义 Pass 拥有独立的顶点和片段着色器，并可配置其渲染状态（如混合模式、深度测试等）。上图展示了使用多 Pass 可实现一些特定的渲染效果。</p>
<h4><strong>支持URP Stencil Test</strong></h4>
<p>现在可在 Shader Graph 中直接使用和配置 Stencil Buffer。方便快速实现游戏中常用的遮挡描边/透视效果。</p>
<h3><strong>Content Pipeline</strong></h3>
<p>在 Tuaniie 1.7.1 preview 版本中，深入理解开发者对 <strong>Instant Asset</strong> 功能模块的体验和建议，并进行了多项升级：</p>
<ul>
<li> <p><strong>新增 Instant Asset 场景资产构建与编辑器模拟运行</strong>：支持场景资产构建能力，同时支持了在编辑器中直接模拟运行测试，加速项目开发迭代流程；</p> </li>
<li> <p><strong>新增 Instant Asset 资源表管理接口与确定性资产构建</strong>：支持资源表合并、Diff 接口和确定性资产构建机制，确保增量构建精准高效，完美适配热更新等第三方解决方案，轻松应对复杂项目兼容挑战。</p> </li>
</ul>
<h3><strong>小游戏（MiniGame）</strong></h3>
<p>在 Tuanjie 1.7.1 preview 版本中，团结引擎小游戏平台<strong>新增了对 TapTap 子平台的支持。</strong>此次更新进一步扩展了平台发布能力，对 TapTap 子平台的构建支持，使开发者能够更便捷地发布游戏至 TapTap 平台，触达更广泛的玩家群体。</p>
<h3><strong>Platform</strong></h3>
<h4><strong>OpenHarmony 平台</strong></h4>
<ul>
<li>更完善的 UAAL 模式，从 1.7.1 的版本开始，导出工程的 ETS 代码会被封装到 har 包里面去，并且 Default 的模式不再提供，UAAL 成为 Default 模式。模块集成到原生应用更加的便捷。</li>
<li>新增蓝牙键盘鼠标支持，可在 PC/平板中使用该能力。</li>
<li>Webview 接口新增 registerJavaScriptProxy API 和 border 属性相关 API。</li>
<li>新增 Strip Engine Code 支持，进一步降低包体占用。</li>
<li>开发工具上，除了此前支持的 ARM 架构 DevEco Studio 的 Emulator，完善了 x64 架构适配支持。目前 DevEco Studio 的 Emulator 可以在 Window 跟 Mac 上均可流畅使用。</li>
</ul>
<p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.unity.cn%2Fprojects%2F68e72ddcedbc2a001ec6b74e" target="_blank">查看官方公告</a>。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-6c864d82f1.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-6c864d82f1.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 14:10:59 +0800</pubDate>
  </item><item>
    <title><![CDATA[小米登记第三代人形机器人 CyberOne 作品著作权]]></title>
    <link>https://www.oschina.net/news/377341</link>
    <itunes:title><![CDATA[小米登记第三代人形机器人 CyberOne 作品著作权]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><span>天眼查App显示，10月10日，北京小米机器人技术有限公司登记“第三代人形机器人CyberOne”作品著作权，作品类别为美术。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c972e81ff5.png"></p>
<p><span>该公司成立于2023年4月，法定代表人为曾学忠，注册资本约5556万人民币，经营范围包括工业机器人制造、工业机器人销售、服务消费机器人制造等，由北京小米智能科技有限公司、北京屹唐创欣创业投资中心（有限合伙）共同持股。</span></p>
<p><span>公开资料显示，CyberOne（中文名：铁大）是小米集团于2022年8月11日发布的首款全尺寸人形仿生机器人，主要应用于家庭护理与陪伴场景。今年2月，小米开始推进该机器人在自有制造产线的分阶段落地。</span></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><span>天眼查App显示，10月10日，北京小米机器人技术有限公司登记“第三代人形机器人CyberOne”作品著作权，作品类别为美术。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c972e81ff5.png"></p>
<p><span>该公司成立于2023年4月，法定代表人为曾学忠，注册资本约5556万人民币，经营范围包括工业机器人制造、工业机器人销售、服务消费机器人制造等，由北京小米智能科技有限公司、北京屹唐创欣创业投资中心（有限合伙）共同持股。</span></p>
<p><span>公开资料显示，CyberOne（中文名：铁大）是小米集团于2022年8月11日发布的首款全尺寸人形仿生机器人，主要应用于家庭护理与陪伴场景。今年2月，小米开始推进该机器人在自有制造产线的分阶段落地。</span></p>]]>
    </description>
    <content:encoded><![CDATA[<p><span>天眼查App显示，10月10日，北京小米机器人技术有限公司登记“第三代人形机器人CyberOne”作品著作权，作品类别为美术。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c972e81ff5.png"></p>
<p><span>该公司成立于2023年4月，法定代表人为曾学忠，注册资本约5556万人民币，经营范围包括工业机器人制造、工业机器人销售、服务消费机器人制造等，由北京小米智能科技有限公司、北京屹唐创欣创业投资中心（有限合伙）共同持股。</span></p>
<p><span>公开资料显示，CyberOne（中文名：铁大）是小米集团于2022年8月11日发布的首款全尺寸人形仿生机器人，主要应用于家庭护理与陪伴场景。今年2月，小米开始推进该机器人在自有制造产线的分阶段落地。</span></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c972e81ff5.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-c972e81ff5.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 13:59:17 +0800</pubDate>
  </item><item>
    <title><![CDATA[Bun 1.3 正式发布]]></title>
    <link>https://www.oschina.net/news/377329/bun-v1-3</link>
    <itunes:title><![CDATA[Bun 1.3 正式发布]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>2025年10月10日，高性能 JavaScript 运行时 Bun 发布了 1.3 版本。这是 Bun 项目迄今为止最重大的版本更新，标志着 Bun 从单纯的运行时工具演变为一个功能完备的全栈 JavaScript 开发平台。</p>
<h2>从运行时到全栈平台的跨越</h2>
<p>Bun 1.3 的核心突破在于将前端开发能力深度整合进运行时。据官方介绍，此次更新新增了对前端开发的一级支持，开发者现在可以直接运行 HTML 文件，无需额外配置即可启动一个功能完整的开发服务器。</p>
<pre><code>bun ./**/*.html
</code></pre>
<p>执行上述命令后，Bun 会自动识别项目中的 HTML 文件并启动开发服务器，包括热模块替换（HMR）、React Fast Refresh 等现代前端开发必备功能。这种「开箱即用」的设计理念贯穿整个 1.3 版本。</p>
<p>值得注意的是，Bun 的开发服务器并非简单的静态文件服务器，而是集成了原生的 JavaScript、CSS 和 HTML 转译与打包能力。这意味着开发者可以在同一个进程中同时处理前后端代码，从根本上解决了传统开发中前后端端口分离导致的 CORS 跨域问题。</p>
<h2>内置数据库客户端：性能与便利的平衡</h2>
<p>数据库访问是后端开发的核心需求。Bun 1.3 将内置数据库支持从单一的 PostgreSQL 扩展到 MySQL、MariaDB 和 SQLite，并提供了统一的 <code>Bun.SQL</code> API。这一设计大幅降低了项目的依赖复杂度，同时带来显著的性能提升。</p>
<pre><code>import { sql, SQL } from "bun";
// 使用统一 API 连接不同数据库
const postgres = new SQL("postgres://localhost/mydb");
const mysql = new SQL("mysql://localhost/mydb");
const sqlite = new SQL("sqlite://data.db");
// 自动从环境变量读取连接信息
const seniorAge = 65;
const seniorUsers = await sql`
SELECT name, age FROM users
WHERE age &gt;= ${seniorAge}
`;
</code></pre>
<p>虽然现有的 npm 包如 <code>postgres</code> 和 <code>mysql2</code> 在 Bun 中已有不错的性能表现，但原生实现带来的优势不容小觑。官方数据显示，内置客户端在某些场景下性能提升可达数倍。</p>
<p>新版本还新增了 Redis 客户端支持，官方基准测试显示其性能是流行的 ioredis 库的 7.9 倍以上。对于需要缓存和消息队列的应用场景，这一改进意义重大。</p>
<p><img alt="Redis 性能对比" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-662ba9a9f1.png"></p>
<h2>路由系统：简化全栈应用架构</h2>
<p>Bun 1.3 为 <code>Bun.serve()</code> 引入了内置路由系统，支持参数化路由和通配符，让开发者能够在同一服务中优雅地处理前端页面和后端 API。</p>
<pre><code>import { serve, sql } from "bun";
import App from "./myReactSPA.html";
serve({
port: 3000,
routes: {
"/*": App,
"/api/users": {
GET: async () =&gt; Response.json(await sql`SELECT * FROM users LIMIT 10`),
POST: async (req) =&gt; {
const { name, email } = await req.json();
const [user] = await sql`
INSERT INTO users ${sql({ name, email })}
RETURNING *;
`;
return Response.json(user);
},
},
"/api/users/:id": async (req) =&gt; {
const { id } = req.params;
const [user] = await sql`SELECT * FROM users WHERE id = ${id} LIMIT 1`;
if (!user) return new Response("User not found", { status: 404 });
return Response.json(user);
},
},
});
</code></pre>
<p>这种设计让全栈应用的部署变得极为简单。更令人印象深刻的是，Bun 的打包器现在支持将前后端代码打包成单一的可执行文件:</p>
<pre><code>bun build --compile ./index.html --outfile myapp
</code></pre>
<p>生成的可执行文件可以在任何地方运行，无需安装 Node.js 或其他依赖，这对于微服务部署和边缘计算场景具有重要价值。</p>
<h2>Cookie 管理：告别第三方库</h2>
<p>Web 应用中的 Cookie 处理一直是个痛点。开发者要么选择单一功能的 <code>tough-cookie</code> 库，要么被迫引入 Express、Elysia 等完整框架。Bun 1.3 提供了原生的 Cookie API，采用类似 Map 的接口设计:</p>
<pre><code>import { serve, randomUUIDv7 } from "bun";
serve({
routes: {
"/api/users/sign-in": (request) =&gt; {
request.cookies.set("sessionId", randomUUIDv7(), {
httpOnly: true,
sameSite: "strict",
});
return new Response("Signed in");
},
"/api/users/sign-out": (request) =&gt; {
request.cookies.delete("sessionId");
return new Response("Signed out");
},
},
});
</code></pre>
<p>这个 API 的巧妙之处在于零性能开销的延迟解析——只有在实际访问 <code>request.cookies</code> 时才会解析请求头中的 Cookie，避免了不必要的计算。</p>
<h2>包管理器的重大革新</h2>
<p>Bun 的包管理器在 1.3 版本中获得了多项企业级特性。其中最值得关注的是<strong>隔离安装模式</strong>成为工作空间的默认行为。这一改变解决了大型 monorepo 项目中最常见的问题之一：包的幽灵依赖（phantom dependencies）。</p>
<p>在传统的提升安装模式下，所有依赖都平铺在根目录的 <code>node_modules</code> 中，包可能意外访问到未在 <code>package.json</code> 中声明的依赖。隔离模式确保每个包只能访问其明确声明的依赖，提高了构建的可预测性和可靠性。</p>
<p><strong>依赖目录 (Catalogs)</strong> 功能为 monorepo 中的版本管理提供了优雅的解决方案:</p>
<pre><code>{
"name": "monorepo",
"workspaces": ["packages/*"],
"catalog": {
"react": "^18.0.0",
"typescript": "^5.0.0"
}
}
</code></pre>
<p>工作空间包可以通过 <code>"react": "catalog:"</code> 引用目录中的版本，实现集中式版本管理。这一设计借鉴了 pnpm 的成功经验，但整合得更加自然。</p>
<p>新增的<strong>安全扫描 API</strong> 让企业能够在安装阶段拦截恶意包。Bun 团队与 Socket 安全公司合作，推出了官方安全扫描器 <code>@socketsecurity/bun-security-scanner</code>。开发者也可以基于公开的 API 编写自定义扫描器，满足特定的安全合规需求。</p>
<pre><code>[install.security]
scanner = "@socketsecurity/bun-security-scanner"
</code></pre>
<p><strong>最小发布时间限制</strong>功能则提供了对供应链攻击的防护:</p>
<pre><code>[install]
minimumReleaseAge = 604800  # 7天
</code></pre>
<p>这项配置要求包必须发布至少指定时间后才允许安装，给社区留出时间识别潜在的恶意包。</p>
<p>交互式更新命令 <code>bun update --interactive</code> 让依赖升级变得可控:</p>
<pre><code>bun update --interactive
</code></pre>
<p>开发者可以逐个选择要更新的包，而不是一次性升级所有依赖，从而更好地控制潜在的破坏性变更。</p>
<h2>测试框架的成熟化</h2>
<p>Bun 的测试运行器在 1.3 版本中获得了与 VS Code Test Explorer 的深度集成。开发者可以直接在编辑器侧边栏查看测试列表，一键运行或调试单个测试，内联查看错误信息。这种开发体验与 Jest + VS Code Jest 扩展相当，但得益于 Bun 的性能优势，测试执行速度更快。</p>
<p>并发测试支持的加入让 I/O 密集型测试套件的运行时间大幅缩短：</p>
<pre><code>import { test } from "bun:test";
test.concurrent("fetch user 1", async () =&gt; {
const res = await fetch("https://api.example.com/users/1");
expect(res.status).toBe(200);
});
describe.concurrent("server tests", () =&gt; {
test("sends a request to server 1", async () =&gt; {
const response = await fetch("https://example.com/server-1");
expect(response.status).toBe(200);
});
});
</code></pre>
<p>默认情况下，最多 20 个测试会并发运行，这个数字可以通过 <code>--max-concurrency</code> 标志调整。对于需要保持串行执行的测试，可以使用 <code>test.serial</code> 修饰符。</p>
<p>类型测试功能 <code>expectTypeOf()</code> 的引入是另一个亮点。开发者现在可以在单元测试中直接验证 TypeScript 类型:</p>
<pre><code>import { expectTypeOf, test } from "bun:test";
test("types are correct", () =&gt; {
expectTypeOf&lt;string&gt;().toEqualTypeOf&lt;string&gt;();
expectTypeOf({ foo: 1 }).toHaveProperty("foo");
expectTypeOf&lt;Promise&lt;number&gt;&gt;().resolves.toBeNumber();
});
</code></pre>
<p>这些类型断言可以通过 <code>bunx tsc --noEmit</code> 在 CI 流程中验证，将类型安全检查提升到了新的高度。</p>
<h2>Node.js 兼容性的持续推进</h2>
<p>Bun 1.3 在每次提交时运行 Node.js 测试套件中额外的 800 个测试用例。官方数据显示，N-API 测试通过率已达 98% 以上，timers 模块通过率达到 98.4%。</p>
<p>对 <code>node:vm</code> 模块的全面支持是本次更新的重要成果。这个模块常用于代码沙箱、插件系统等高级场景，其实现难度较大。Bun 1.3 不仅支持基础的 <code>vm.Script</code>，还实现了 <code>vm.SourceTextModule</code>、<code>vm.SyntheticModule</code> 等高级 API，并支持字节码缓存以提升编译性能。</p>
<pre><code>import vm from "node:vm";
const script = new vm.Script('console.log("Hello from VM")');
script.runInThisContext();
</code></pre>
<p><code>node:test</code> 模块的初步支持让使用 Node.js 原生测试框架的项目能够在 Bun 上运行，这对于生态系统的兼容性意义重大。</p>
<p>加密性能的提升令人瞩目。DiffieHellman 和 Cipheriv/Decipheriv 的速度提升了约 400 倍，scrypt 提升了 6 倍。这些改进通过将关键路径用原生 C++ 重写实现，大幅降低了密码学操作的开销。</p>
<h2>Web 标准与现代 API</h2>
<p>Bun 1.3 新增了对多项 Web 标准的支持。<strong>YAML 原生支持</strong>让配置文件处理变得简单：</p>
<pre><code>import { YAML } from "bun";
const obj = YAML.parse("key: value");
const yaml = YAML.stringify({ key: "value" }, 0, 2);
// 直接导入 YAML 文件
import config from "./config.yaml";
</code></pre>
<p>官方实现的 YAML 解析器目前通过了官方测试套件的 90% 用例，性能表现优异。</p>
<p><strong>WebSocket 压缩</strong>功能的自动协商是另一个实用特性。当连接支持 <code>permessage-deflate</code> 时，Bun 会自动启用压缩，对于 JSON 等结构化数据，压缩率可达 60-80%，显著减少带宽消耗。</p>
<p><strong>Zstandard 压缩</strong>的全面支持包括 <code>fetch()</code> 的自动解压和手动压缩 API：</p>
<pre><code>import { zstdCompress, zstdDecompress } from "bun";
const compressed = await zstdCompress("Hello, world!");
const decompressed = await zstdDecompress(compressed);
</code></pre>
<p><code>DisposableStack</code> 和 <code>AsyncDisposableStack</code> 的实现体现了 Bun 对 TC39 提案的快速跟进。这些 API 与 <code>using</code> 和 <code>await using</code> 声明配合，提供了优雅的资源管理机制。</p>
<h2>性能优化：细节见真章</h2>
<p>Bun 1.3 的性能优化遍布各个层面。空闲 CPU 占用的降低源于对垃圾回收调度的优化，在没有进行中的请求时，<code>Bun.serve</code> 的计时器不再活跃。JavaScript 内存占用降低 10-30% (Next.js 应用降低 28%，Elysia 降低 11%) 归功于更智能的 GC 计时器调度。</p>
<p>I/O 线程池的优化让 macOS 上的 <code>Bun.build</code> 快了 60%。Express 性能提升 9%，Fastify 提升 5.4%，这些改进来自对 <code>node:http</code> 模块的持续优化。</p>
<p><code>postMessage</code> 的性能提升尤为惊人——字符串传递快了 500 倍，简单对象快了 240 倍。这通过避免对安全共享的字符串进行序列化实现，同时减少了约 22 倍的峰值内存使用。</p>
<pre><code>// 在 Worker 间传递大型 JSON 字符串现在快了 500 倍
const response = await fetch("https://api.example.com/data");
const json = await response.text();
postMessage(json);
</code></pre>
<p>启动时间减少了 1ms，内存占用减少了 3MB，这些看似微小的优化累积起来，对用户体验产生显著影响。</p>
<h2>开发者体验的精心打磨</h2>
<p>Bun 1.3 在开发者体验上的改进同样值得称道。TypeScript 默认配置改为 <code>"module": "Preserve"</code>，保留模块语法的原始形态，更符合 Bun 作为原生 ES 模块运行时的定位。</p>
<p><code>console.log()</code> 的深度控制让调试大型对象变得可控:</p>
<pre><code>bun --console-depth=5 ./app.ts
</code></pre>
<pre><code>[console]
depth = 5
</code></pre>
<p><code>BUN_OPTIONS</code> 环境变量提供了设置默认 CLI 参数的便捷方式：</p>
<pre><code>export BUN_OPTIONS="--watch --hot"
bun run ./app.ts
# 等同于: bun --watch --hot run ./app.ts
</code></pre>
<p><code>bunx --package</code> 标志让运行二进制文件与包名不一致的包变得简单：</p>
<pre><code>bunx --package=@typescript-eslint/parser eslint ./src
</code></pre>
<p>新增的 <code>bun why</code> 命令清晰地展示依赖链，解答「为什么这个包会出现在我的项目中」：</p>
<pre><code>bun why tailwindcss
</code></pre>
<p>这些看似细小的改进，体现了 Bun 团队对开发者日常工作流程的深刻理解。</p>
<h2>打包器与构建系统的增强</h2>
<p>Bun 的打包器在 1.3 版本中获得了跨平台编译能力。开发者现在可以在任何平台上为 Windows、macOS 和 Linux 构建可执行文件：</p>
<pre><code>bun build --compile --target=linux-x64 ./app.ts --outfile myapp-linux
bun build --compile --target=darwin-arm64 ./app.ts --outfile myapp-macos
bun build --compile --target=windows-x64 ./app.ts --outfile myapp.exe
</code></pre>
<p>Windows 可执行文件元数据的支持让企业应用的打包更加专业：</p>
<pre><code>bun build --compile --target=windows-x64 \\
--title="My App" \\
--publisher="My Company" \\
--version="1.0.0" \\
./app.ts
</code></pre>
<p>代码签名支持（Windows 的 Authenticode 和 macOS 的 codesign）确保了发布的可执行文件可以通过操作系统的安全验证。</p>
<p>压缩器变得更加智能，能够移除未使用的函数和类名，优化 <code>new Object()</code>、<code>new Array()</code> 等表达式，消除无用的 <code>try...catch...finally</code> 块。这些优化让生产构建的体积进一步缩小。</p>
<h2>安全性的持续关注</h2>
<p>Bun 1.3 引入了 <code>Bun.secrets</code> API，利用操作系统的原生凭据存储（macOS 的 Keychain、Linux 的 libsecret、Windows 的 Credential Manager）：</p>
<pre><code>import { secrets } from "bun";
await secrets.set({
service: "my-app",
name: "api-key",
value: "secret-value",
});
const key = await secrets.get({
service: "my-app",
name: "api-key",
});
</code></pre>
<p>凭据在静态时被加密，与环境变量分离存储，提供了更高的安全保障。</p>
<p><code>Bun.CSRF</code> 模块为跨站请求伪造防护提供了原生支持：</p>
<pre><code>import { CSRF } from "bun";
const secret = "your-secret-key";
const token = CSRF.generate({ secret, encoding: "hex", expiresIn: 60 * 1000 });
const isValid = CSRF.verify(token, { secret });
</code></pre>
<p>这些安全特性的内置化降低了开发者的心智负担，让安全最佳实践更容易落地。</p>
<h2>生态系统的影响与展望</h2>
<p>Bun 1.3 的发布标志着该项目从「快速运行时」向「完整开发平台」的战略转型。Midjourney 等知名公司已在生产环境中使用 Bun 进行前端开发，这证明了其稳定性和可靠性。</p>
<p>官方提到，每个提交都会运行大量的 Node.js 测试套件，表明 Bun 团队对兼容性的重视。对 <code>pnpm.lock</code> 和 <code>yarn.lock</code> 的迁移支持，让团队可以无痛试用 Bun，而无需说服所有成员同时升级工具链。</p>
<p>不过，1.3 版本也带来了一些破坏性变更。<code>Bun.serve()</code> 的 TypeScript 类型被重构，WebSocket 数据定义方式发生变化；SQL 客户端现在强制使用标签模板语法；测试过滤器在没有匹配用例时会报错而非静默成功。这些变更虽然可能给现有项目带来迁移成本，但从长远看有利于 API 的一致性和可维护性。</p>
<h2>数据说话：性能对比</h2>
<p>根据官方提供的基准测试数据:</p>
<ul>
<li><strong>Redis 客户端</strong>: 比 ioredis 快 7.9 倍以上</li>
<li><strong>postMessage 字符串</strong>: 速度提升 500 倍，内存减少 22 倍</li>
<li><strong>postMessage 对象</strong>: 速度提升 240 倍</li>
<li><strong>DiffieHellman</strong>: 约快 400 倍</li>
<li><strong>Cipheriv/Decipheriv</strong>: 约快 400 倍</li>
<li><strong>scrypt</strong>: 快 6 倍</li>
<li><strong><code>AbortSignal.timeout</code></strong>: 快 40 倍</li>
<li><strong><code>Headers</code> 操作</strong>: 快 2 倍</li>
<li><strong><code>Bun.build</code> on macOS</strong>: 快 60%</li>
<li><strong>Express</strong>: 快 9%</li>
<li><strong>Fastify</strong>: 快 5.4%</li>
</ul>
<p>这些数据展示了 Bun 在性能上的持续领先优势。</p>
<h2>社区反响与未来规划</h2>
<p>Bun 1.3 的发布在社区引发了热烈讨论。开发者们尤其关注全栈开发能力的提升和包管理器的企业级特性。Socket 公司 CTO Ahmad Nassri 对安全扫描 API 的评价颇具代表性：「Bun 团队行动迅速，在包管理器层面保护开发者。开放安全扫描 API，让 Socket 这样的工具能够在安装过程中提供实时威胁检测。这是让开源开发默认更安全的重要一步。」</p>
<p>官方表示，1.3 系列将持续关注全栈应用开发体验的提升。Redis 集群、流式处理和 Lua 脚本支持已在规划中。WebAssembly 流式编译的实现，以及对更多 TC39 提案的支持，也在进行中。</p>
<p>Bun 的快速迭代和对社区反馈的积极响应，让人对其未来发展充满期待。从一个「更快的 Node.js」到一个「完整的 JavaScript 平台」，Bun 正在书写属于自己的故事。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>2025年10月10日，高性能 JavaScript 运行时 Bun 发布了 1.3 版本。这是 Bun 项目迄今为止最重大的版本更新，标志着 Bun 从单纯的运行时工具演变为一个功能完备的全栈 JavaScript 开发平台。</p>
<h2>从运行时到全栈平台的跨越</h2>
<p>Bun 1.3 的核心突破在于将前端开发能力深度整合进运行时。据官方介绍，此次更新新增了对前端开发的一级支持，开发者现在可以直接运行 HTML 文件，无需额外配置即可启动一个功能完整的开发服务器。</p>
<pre><code>bun ./**/*.html
</code></pre>
<p>执行上述命令后，Bun 会自动识别项目中的 HTML 文件并启动开发服务器，包括热模块替换（HMR）、React Fast Refresh 等现代前端开发必备功能。这种「开箱即用」的设计理念贯穿整个 1.3 版本。</p>
<p>值得注意的是，Bun 的开发服务器并非简单的静态文件服务器，而是集成了原生的 JavaScript、CSS 和 HTML 转译与打包能力。这意味着开发者可以在同一个进程中同时处理前后端代码，从根本上解决了传统开发中前后端端口分离导致的 CORS 跨域问题。</p>
<h2>内置数据库客户端：性能与便利的平衡</h2>
<p>数据库访问是后端开发的核心需求。Bun 1.3 将内置数据库支持从单一的 PostgreSQL 扩展到 MySQL、MariaDB 和 SQLite，并提供了统一的 <code>Bun.SQL</code> API。这一设计大幅降低了项目的依赖复杂度，同时带来显著的性能提升。</p>
<pre><code>import { sql, SQL } from "bun";
// 使用统一 API 连接不同数据库
const postgres = new SQL("postgres://localhost/mydb");
const mysql = new SQL("mysql://localhost/mydb");
const sqlite = new SQL("sqlite://data.db");
// 自动从环境变量读取连接信息
const seniorAge = 65;
const seniorUsers = await sql`
SELECT name, age FROM users
WHERE age &gt;= ${seniorAge}
`;
</code></pre>
<p>虽然现有的 npm 包如 <code>postgres</code> 和 <code>mysql2</code> 在 Bun 中已有不错的性能表现，但原生实现带来的优势不容小觑。官方数据显示，内置客户端在某些场景下性能提升可达数倍。</p>
<p>新版本还新增了 Redis 客户端支持，官方基准测试显示其性能是流行的 ioredis 库的 7.9 倍以上。对于需要缓存和消息队列的应用场景，这一改进意义重大。</p>
<p><img alt="Redis 性能对比" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-662ba9a9f1.png"></p>
<h2>路由系统：简化全栈应用架构</h2>
<p>Bun 1.3 为 <code>Bun.serve()</code> 引入了内置路由系统，支持参数化路由和通配符，让开发者能够在同一服务中优雅地处理前端页面和后端 API。</p>
<pre><code>import { serve, sql } from "bun";
import App from "./myReactSPA.html";
serve({
port: 3000,
routes: {
"/*": App,
"/api/users": {
GET: async () =&gt; Response.json(await sql`SELECT * FROM users LIMIT 10`),
POST: async (req) =&gt; {
const { name, email } = await req.json();
const [user] = await sql`
INSERT INTO users ${sql({ name, email })}
RETURNING *;
`;
return Response.json(user);
},
},
"/api/users/:id": async (req) =&gt; {
const { id } = req.params;
const [user] = await sql`SELECT * FROM users WHERE id = ${id} LIMIT 1`;
if (!user) return new Response("User not found", { status: 404 });
return Response.json(user);
},
},
});
</code></pre>
<p>这种设计让全栈应用的部署变得极为简单。更令人印象深刻的是，Bun 的打包器现在支持将前后端代码打包成单一的可执行文件:</p>
<pre><code>bun build --compile ./index.html --outfile myapp
</code></pre>
<p>生成的可执行文件可以在任何地方运行，无需安装 Node.js 或其他依赖，这对于微服务部署和边缘计算场景具有重要价值。</p>
<h2>Cookie 管理：告别第三方库</h2>
<p>Web 应用中的 Cookie 处理一直是个痛点。开发者要么选择单一功能的 <code>tough-cookie</code> 库，要么被迫引入 Express、Elysia 等完整框架。Bun 1.3 提供了原生的 Cookie API，采用类似 Map 的接口设计:</p>
<pre><code>import { serve, randomUUIDv7 } from "bun";
serve({
routes: {
"/api/users/sign-in": (request) =&gt; {
request.cookies.set("sessionId", randomUUIDv7(), {
httpOnly: true,
sameSite: "strict",
});
return new Response("Signed in");
},
"/api/users/sign-out": (request) =&gt; {
request.cookies.delete("sessionId");
return new Response("Signed out");
},
},
});
</code></pre>
<p>这个 API 的巧妙之处在于零性能开销的延迟解析——只有在实际访问 <code>request.cookies</code> 时才会解析请求头中的 Cookie，避免了不必要的计算。</p>
<h2>包管理器的重大革新</h2>
<p>Bun 的包管理器在 1.3 版本中获得了多项企业级特性。其中最值得关注的是<strong>隔离安装模式</strong>成为工作空间的默认行为。这一改变解决了大型 monorepo 项目中最常见的问题之一：包的幽灵依赖（phantom dependencies）。</p>
<p>在传统的提升安装模式下，所有依赖都平铺在根目录的 <code>node_modules</code> 中，包可能意外访问到未在 <code>package.json</code> 中声明的依赖。隔离模式确保每个包只能访问其明确声明的依赖，提高了构建的可预测性和可靠性。</p>
<p><strong>依赖目录 (Catalogs)</strong> 功能为 monorepo 中的版本管理提供了优雅的解决方案:</p>
<pre><code>{
"name": "monorepo",
"workspaces": ["packages/*"],
"catalog": {
"react": "^18.0.0",
"typescript": "^5.0.0"
}
}
</code></pre>
<p>工作空间包可以通过 <code>"react": "catalog:"</code> 引用目录中的版本，实现集中式版本管理。这一设计借鉴了 pnpm 的成功经验，但整合得更加自然。</p>
<p>新增的<strong>安全扫描 API</strong> 让企业能够在安装阶段拦截恶意包。Bun 团队与 Socket 安全公司合作，推出了官方安全扫描器 <code>@socketsecurity/bun-security-scanner</code>。开发者也可以基于公开的 API 编写自定义扫描器，满足特定的安全合规需求。</p>
<pre><code>[install.security]
scanner = "@socketsecurity/bun-security-scanner"
</code></pre>
<p><strong>最小发布时间限制</strong>功能则提供了对供应链攻击的防护:</p>
<pre><code>[install]
minimumReleaseAge = 604800  # 7天
</code></pre>
<p>这项配置要求包必须发布至少指定时间后才允许安装，给社区留出时间识别潜在的恶意包。</p>
<p>交互式更新命令 <code>bun update --interactive</code> 让依赖升级变得可控:</p>
<pre><code>bun update --interactive
</code></pre>
<p>开发者可以逐个选择要更新的包，而不是一次性升级所有依赖，从而更好地控制潜在的破坏性变更。</p>
<h2>测试框架的成熟化</h2>
<p>Bun 的测试运行器在 1.3 版本中获得了与 VS Code Test Explorer 的深度集成。开发者可以直接在编辑器侧边栏查看测试列表，一键运行或调试单个测试，内联查看错误信息。这种开发体验与 Jest + VS Code Jest 扩展相当，但得益于 Bun 的性能优势，测试执行速度更快。</p>
<p>并发测试支持的加入让 I/O 密集型测试套件的运行时间大幅缩短：</p>
<pre><code>import { test } from "bun:test";
test.concurrent("fetch user 1", async () =&gt; {
const res = await fetch("https://api.example.com/users/1");
expect(res.status).toBe(200);
});
describe.concurrent("server tests", () =&gt; {
test("sends a request to server 1", async () =&gt; {
const response = await fetch("https://example.com/server-1");
expect(response.status).toBe(200);
});
});
</code></pre>
<p>默认情况下，最多 20 个测试会并发运行，这个数字可以通过 <code>--max-concurrency</code> 标志调整。对于需要保持串行执行的测试，可以使用 <code>test.serial</code> 修饰符。</p>
<p>类型测试功能 <code>expectTypeOf()</code> 的引入是另一个亮点。开发者现在可以在单元测试中直接验证 TypeScript 类型:</p>
<pre><code>import { expectTypeOf, test } from "bun:test";
test("types are correct", () =&gt; {
expectTypeOf&lt;string&gt;().toEqualTypeOf&lt;string&gt;();
expectTypeOf({ foo: 1 }).toHaveProperty("foo");
expectTypeOf&lt;Promise&lt;number&gt;&gt;().resolves.toBeNumber();
});
</code></pre>
<p>这些类型断言可以通过 <code>bunx tsc --noEmit</code> 在 CI 流程中验证，将类型安全检查提升到了新的高度。</p>
<h2>Node.js 兼容性的持续推进</h2>
<p>Bun 1.3 在每次提交时运行 Node.js 测试套件中额外的 800 个测试用例。官方数据显示，N-API 测试通过率已达 98% 以上，timers 模块通过率达到 98.4%。</p>
<p>对 <code>node:vm</code> 模块的全面支持是本次更新的重要成果。这个模块常用于代码沙箱、插件系统等高级场景，其实现难度较大。Bun 1.3 不仅支持基础的 <code>vm.Script</code>，还实现了 <code>vm.SourceTextModule</code>、<code>vm.SyntheticModule</code> 等高级 API，并支持字节码缓存以提升编译性能。</p>
<pre><code>import vm from "node:vm";
const script = new vm.Script('console.log("Hello from VM")');
script.runInThisContext();
</code></pre>
<p><code>node:test</code> 模块的初步支持让使用 Node.js 原生测试框架的项目能够在 Bun 上运行，这对于生态系统的兼容性意义重大。</p>
<p>加密性能的提升令人瞩目。DiffieHellman 和 Cipheriv/Decipheriv 的速度提升了约 400 倍，scrypt 提升了 6 倍。这些改进通过将关键路径用原生 C++ 重写实现，大幅降低了密码学操作的开销。</p>
<h2>Web 标准与现代 API</h2>
<p>Bun 1.3 新增了对多项 Web 标准的支持。<strong>YAML 原生支持</strong>让配置文件处理变得简单：</p>
<pre><code>import { YAML } from "bun";
const obj = YAML.parse("key: value");
const yaml = YAML.stringify({ key: "value" }, 0, 2);
// 直接导入 YAML 文件
import config from "./config.yaml";
</code></pre>
<p>官方实现的 YAML 解析器目前通过了官方测试套件的 90% 用例，性能表现优异。</p>
<p><strong>WebSocket 压缩</strong>功能的自动协商是另一个实用特性。当连接支持 <code>permessage-deflate</code> 时，Bun 会自动启用压缩，对于 JSON 等结构化数据，压缩率可达 60-80%，显著减少带宽消耗。</p>
<p><strong>Zstandard 压缩</strong>的全面支持包括 <code>fetch()</code> 的自动解压和手动压缩 API：</p>
<pre><code>import { zstdCompress, zstdDecompress } from "bun";
const compressed = await zstdCompress("Hello, world!");
const decompressed = await zstdDecompress(compressed);
</code></pre>
<p><code>DisposableStack</code> 和 <code>AsyncDisposableStack</code> 的实现体现了 Bun 对 TC39 提案的快速跟进。这些 API 与 <code>using</code> 和 <code>await using</code> 声明配合，提供了优雅的资源管理机制。</p>
<h2>性能优化：细节见真章</h2>
<p>Bun 1.3 的性能优化遍布各个层面。空闲 CPU 占用的降低源于对垃圾回收调度的优化，在没有进行中的请求时，<code>Bun.serve</code> 的计时器不再活跃。JavaScript 内存占用降低 10-30% (Next.js 应用降低 28%，Elysia 降低 11%) 归功于更智能的 GC 计时器调度。</p>
<p>I/O 线程池的优化让 macOS 上的 <code>Bun.build</code> 快了 60%。Express 性能提升 9%，Fastify 提升 5.4%，这些改进来自对 <code>node:http</code> 模块的持续优化。</p>
<p><code>postMessage</code> 的性能提升尤为惊人——字符串传递快了 500 倍，简单对象快了 240 倍。这通过避免对安全共享的字符串进行序列化实现，同时减少了约 22 倍的峰值内存使用。</p>
<pre><code>// 在 Worker 间传递大型 JSON 字符串现在快了 500 倍
const response = await fetch("https://api.example.com/data");
const json = await response.text();
postMessage(json);
</code></pre>
<p>启动时间减少了 1ms，内存占用减少了 3MB，这些看似微小的优化累积起来，对用户体验产生显著影响。</p>
<h2>开发者体验的精心打磨</h2>
<p>Bun 1.3 在开发者体验上的改进同样值得称道。TypeScript 默认配置改为 <code>"module": "Preserve"</code>，保留模块语法的原始形态，更符合 Bun 作为原生 ES 模块运行时的定位。</p>
<p><code>console.log()</code> 的深度控制让调试大型对象变得可控:</p>
<pre><code>bun --console-depth=5 ./app.ts
</code></pre>
<pre><code>[console]
depth = 5
</code></pre>
<p><code>BUN_OPTIONS</code> 环境变量提供了设置默认 CLI 参数的便捷方式：</p>
<pre><code>export BUN_OPTIONS="--watch --hot"
bun run ./app.ts
# 等同于: bun --watch --hot run ./app.ts
</code></pre>
<p><code>bunx --package</code> 标志让运行二进制文件与包名不一致的包变得简单：</p>
<pre><code>bunx --package=@typescript-eslint/parser eslint ./src
</code></pre>
<p>新增的 <code>bun why</code> 命令清晰地展示依赖链，解答「为什么这个包会出现在我的项目中」：</p>
<pre><code>bun why tailwindcss
</code></pre>
<p>这些看似细小的改进，体现了 Bun 团队对开发者日常工作流程的深刻理解。</p>
<h2>打包器与构建系统的增强</h2>
<p>Bun 的打包器在 1.3 版本中获得了跨平台编译能力。开发者现在可以在任何平台上为 Windows、macOS 和 Linux 构建可执行文件：</p>
<pre><code>bun build --compile --target=linux-x64 ./app.ts --outfile myapp-linux
bun build --compile --target=darwin-arm64 ./app.ts --outfile myapp-macos
bun build --compile --target=windows-x64 ./app.ts --outfile myapp.exe
</code></pre>
<p>Windows 可执行文件元数据的支持让企业应用的打包更加专业：</p>
<pre><code>bun build --compile --target=windows-x64 \\
--title="My App" \\
--publisher="My Company" \\
--version="1.0.0" \\
./app.ts
</code></pre>
<p>代码签名支持（Windows 的 Authenticode 和 macOS 的 codesign）确保了发布的可执行文件可以通过操作系统的安全验证。</p>
<p>压缩器变得更加智能，能够移除未使用的函数和类名，优化 <code>new Object()</code>、<code>new Array()</code> 等表达式，消除无用的 <code>try...catch...finally</code> 块。这些优化让生产构建的体积进一步缩小。</p>
<h2>安全性的持续关注</h2>
<p>Bun 1.3 引入了 <code>Bun.secrets</code> API，利用操作系统的原生凭据存储（macOS 的 Keychain、Linux 的 libsecret、Windows 的 Credential Manager）：</p>
<pre><code>import { secrets } from "bun";
await secrets.set({
service: "my-app",
name: "api-key",
value: "secret-value",
});
const key = await secrets.get({
service: "my-app",
name: "api-key",
});
</code></pre>
<p>凭据在静态时被加密，与环境变量分离存储，提供了更高的安全保障。</p>
<p><code>Bun.CSRF</code> 模块为跨站请求伪造防护提供了原生支持：</p>
<pre><code>import { CSRF } from "bun";
const secret = "your-secret-key";
const token = CSRF.generate({ secret, encoding: "hex", expiresIn: 60 * 1000 });
const isValid = CSRF.verify(token, { secret });
</code></pre>
<p>这些安全特性的内置化降低了开发者的心智负担，让安全最佳实践更容易落地。</p>
<h2>生态系统的影响与展望</h2>
<p>Bun 1.3 的发布标志着该项目从「快速运行时」向「完整开发平台」的战略转型。Midjourney 等知名公司已在生产环境中使用 Bun 进行前端开发，这证明了其稳定性和可靠性。</p>
<p>官方提到，每个提交都会运行大量的 Node.js 测试套件，表明 Bun 团队对兼容性的重视。对 <code>pnpm.lock</code> 和 <code>yarn.lock</code> 的迁移支持，让团队可以无痛试用 Bun，而无需说服所有成员同时升级工具链。</p>
<p>不过，1.3 版本也带来了一些破坏性变更。<code>Bun.serve()</code> 的 TypeScript 类型被重构，WebSocket 数据定义方式发生变化；SQL 客户端现在强制使用标签模板语法；测试过滤器在没有匹配用例时会报错而非静默成功。这些变更虽然可能给现有项目带来迁移成本，但从长远看有利于 API 的一致性和可维护性。</p>
<h2>数据说话：性能对比</h2>
<p>根据官方提供的基准测试数据:</p>
<ul>
<li><strong>Redis 客户端</strong>: 比 ioredis 快 7.9 倍以上</li>
<li><strong>postMessage 字符串</strong>: 速度提升 500 倍，内存减少 22 倍</li>
<li><strong>postMessage 对象</strong>: 速度提升 240 倍</li>
<li><strong>DiffieHellman</strong>: 约快 400 倍</li>
<li><strong>Cipheriv/Decipheriv</strong>: 约快 400 倍</li>
<li><strong>scrypt</strong>: 快 6 倍</li>
<li><strong><code>AbortSignal.timeout</code></strong>: 快 40 倍</li>
<li><strong><code>Headers</code> 操作</strong>: 快 2 倍</li>
<li><strong><code>Bun.build</code> on macOS</strong>: 快 60%</li>
<li><strong>Express</strong>: 快 9%</li>
<li><strong>Fastify</strong>: 快 5.4%</li>
</ul>
<p>这些数据展示了 Bun 在性能上的持续领先优势。</p>
<h2>社区反响与未来规划</h2>
<p>Bun 1.3 的发布在社区引发了热烈讨论。开发者们尤其关注全栈开发能力的提升和包管理器的企业级特性。Socket 公司 CTO Ahmad Nassri 对安全扫描 API 的评价颇具代表性：「Bun 团队行动迅速，在包管理器层面保护开发者。开放安全扫描 API，让 Socket 这样的工具能够在安装过程中提供实时威胁检测。这是让开源开发默认更安全的重要一步。」</p>
<p>官方表示，1.3 系列将持续关注全栈应用开发体验的提升。Redis 集群、流式处理和 Lua 脚本支持已在规划中。WebAssembly 流式编译的实现，以及对更多 TC39 提案的支持，也在进行中。</p>
<p>Bun 的快速迭代和对社区反馈的积极响应，让人对其未来发展充满期待。从一个「更快的 Node.js」到一个「完整的 JavaScript 平台」，Bun 正在书写属于自己的故事。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>2025年10月10日，高性能 JavaScript 运行时 Bun 发布了 1.3 版本。这是 Bun 项目迄今为止最重大的版本更新，标志着 Bun 从单纯的运行时工具演变为一个功能完备的全栈 JavaScript 开发平台。</p>
<h2>从运行时到全栈平台的跨越</h2>
<p>Bun 1.3 的核心突破在于将前端开发能力深度整合进运行时。据官方介绍，此次更新新增了对前端开发的一级支持，开发者现在可以直接运行 HTML 文件，无需额外配置即可启动一个功能完整的开发服务器。</p>
<pre><code>bun ./**/*.html
</code></pre>
<p>执行上述命令后，Bun 会自动识别项目中的 HTML 文件并启动开发服务器，包括热模块替换（HMR）、React Fast Refresh 等现代前端开发必备功能。这种「开箱即用」的设计理念贯穿整个 1.3 版本。</p>
<p>值得注意的是，Bun 的开发服务器并非简单的静态文件服务器，而是集成了原生的 JavaScript、CSS 和 HTML 转译与打包能力。这意味着开发者可以在同一个进程中同时处理前后端代码，从根本上解决了传统开发中前后端端口分离导致的 CORS 跨域问题。</p>
<h2>内置数据库客户端：性能与便利的平衡</h2>
<p>数据库访问是后端开发的核心需求。Bun 1.3 将内置数据库支持从单一的 PostgreSQL 扩展到 MySQL、MariaDB 和 SQLite，并提供了统一的 <code>Bun.SQL</code> API。这一设计大幅降低了项目的依赖复杂度，同时带来显著的性能提升。</p>
<pre><code>import { sql, SQL } from "bun";
// 使用统一 API 连接不同数据库
const postgres = new SQL("postgres://localhost/mydb");
const mysql = new SQL("mysql://localhost/mydb");
const sqlite = new SQL("sqlite://data.db");
// 自动从环境变量读取连接信息
const seniorAge = 65;
const seniorUsers = await sql`
SELECT name, age FROM users
WHERE age &gt;= ${seniorAge}
`;
</code></pre>
<p>虽然现有的 npm 包如 <code>postgres</code> 和 <code>mysql2</code> 在 Bun 中已有不错的性能表现，但原生实现带来的优势不容小觑。官方数据显示，内置客户端在某些场景下性能提升可达数倍。</p>
<p>新版本还新增了 Redis 客户端支持，官方基准测试显示其性能是流行的 ioredis 库的 7.9 倍以上。对于需要缓存和消息队列的应用场景，这一改进意义重大。</p>
<p><img alt="Redis 性能对比" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-662ba9a9f1.png"></p>
<h2>路由系统：简化全栈应用架构</h2>
<p>Bun 1.3 为 <code>Bun.serve()</code> 引入了内置路由系统，支持参数化路由和通配符，让开发者能够在同一服务中优雅地处理前端页面和后端 API。</p>
<pre><code>import { serve, sql } from "bun";
import App from "./myReactSPA.html";
serve({
port: 3000,
routes: {
"/*": App,
"/api/users": {
GET: async () =&gt; Response.json(await sql`SELECT * FROM users LIMIT 10`),
POST: async (req) =&gt; {
const { name, email } = await req.json();
const [user] = await sql`
INSERT INTO users ${sql({ name, email })}
RETURNING *;
`;
return Response.json(user);
},
},
"/api/users/:id": async (req) =&gt; {
const { id } = req.params;
const [user] = await sql`SELECT * FROM users WHERE id = ${id} LIMIT 1`;
if (!user) return new Response("User not found", { status: 404 });
return Response.json(user);
},
},
});
</code></pre>
<p>这种设计让全栈应用的部署变得极为简单。更令人印象深刻的是，Bun 的打包器现在支持将前后端代码打包成单一的可执行文件:</p>
<pre><code>bun build --compile ./index.html --outfile myapp
</code></pre>
<p>生成的可执行文件可以在任何地方运行，无需安装 Node.js 或其他依赖，这对于微服务部署和边缘计算场景具有重要价值。</p>
<h2>Cookie 管理：告别第三方库</h2>
<p>Web 应用中的 Cookie 处理一直是个痛点。开发者要么选择单一功能的 <code>tough-cookie</code> 库，要么被迫引入 Express、Elysia 等完整框架。Bun 1.3 提供了原生的 Cookie API，采用类似 Map 的接口设计:</p>
<pre><code>import { serve, randomUUIDv7 } from "bun";
serve({
routes: {
"/api/users/sign-in": (request) =&gt; {
request.cookies.set("sessionId", randomUUIDv7(), {
httpOnly: true,
sameSite: "strict",
});
return new Response("Signed in");
},
"/api/users/sign-out": (request) =&gt; {
request.cookies.delete("sessionId");
return new Response("Signed out");
},
},
});
</code></pre>
<p>这个 API 的巧妙之处在于零性能开销的延迟解析——只有在实际访问 <code>request.cookies</code> 时才会解析请求头中的 Cookie，避免了不必要的计算。</p>
<h2>包管理器的重大革新</h2>
<p>Bun 的包管理器在 1.3 版本中获得了多项企业级特性。其中最值得关注的是<strong>隔离安装模式</strong>成为工作空间的默认行为。这一改变解决了大型 monorepo 项目中最常见的问题之一：包的幽灵依赖（phantom dependencies）。</p>
<p>在传统的提升安装模式下，所有依赖都平铺在根目录的 <code>node_modules</code> 中，包可能意外访问到未在 <code>package.json</code> 中声明的依赖。隔离模式确保每个包只能访问其明确声明的依赖，提高了构建的可预测性和可靠性。</p>
<p><strong>依赖目录 (Catalogs)</strong> 功能为 monorepo 中的版本管理提供了优雅的解决方案:</p>
<pre><code>{
"name": "monorepo",
"workspaces": ["packages/*"],
"catalog": {
"react": "^18.0.0",
"typescript": "^5.0.0"
}
}
</code></pre>
<p>工作空间包可以通过 <code>"react": "catalog:"</code> 引用目录中的版本，实现集中式版本管理。这一设计借鉴了 pnpm 的成功经验，但整合得更加自然。</p>
<p>新增的<strong>安全扫描 API</strong> 让企业能够在安装阶段拦截恶意包。Bun 团队与 Socket 安全公司合作，推出了官方安全扫描器 <code>@socketsecurity/bun-security-scanner</code>。开发者也可以基于公开的 API 编写自定义扫描器，满足特定的安全合规需求。</p>
<pre><code>[install.security]
scanner = "@socketsecurity/bun-security-scanner"
</code></pre>
<p><strong>最小发布时间限制</strong>功能则提供了对供应链攻击的防护:</p>
<pre><code>[install]
minimumReleaseAge = 604800  # 7天
</code></pre>
<p>这项配置要求包必须发布至少指定时间后才允许安装，给社区留出时间识别潜在的恶意包。</p>
<p>交互式更新命令 <code>bun update --interactive</code> 让依赖升级变得可控:</p>
<pre><code>bun update --interactive
</code></pre>
<p>开发者可以逐个选择要更新的包，而不是一次性升级所有依赖，从而更好地控制潜在的破坏性变更。</p>
<h2>测试框架的成熟化</h2>
<p>Bun 的测试运行器在 1.3 版本中获得了与 VS Code Test Explorer 的深度集成。开发者可以直接在编辑器侧边栏查看测试列表，一键运行或调试单个测试，内联查看错误信息。这种开发体验与 Jest + VS Code Jest 扩展相当，但得益于 Bun 的性能优势，测试执行速度更快。</p>
<p>并发测试支持的加入让 I/O 密集型测试套件的运行时间大幅缩短：</p>
<pre><code>import { test } from "bun:test";
test.concurrent("fetch user 1", async () =&gt; {
const res = await fetch("https://api.example.com/users/1");
expect(res.status).toBe(200);
});
describe.concurrent("server tests", () =&gt; {
test("sends a request to server 1", async () =&gt; {
const response = await fetch("https://example.com/server-1");
expect(response.status).toBe(200);
});
});
</code></pre>
<p>默认情况下，最多 20 个测试会并发运行，这个数字可以通过 <code>--max-concurrency</code> 标志调整。对于需要保持串行执行的测试，可以使用 <code>test.serial</code> 修饰符。</p>
<p>类型测试功能 <code>expectTypeOf()</code> 的引入是另一个亮点。开发者现在可以在单元测试中直接验证 TypeScript 类型:</p>
<pre><code>import { expectTypeOf, test } from "bun:test";
test("types are correct", () =&gt; {
expectTypeOf&lt;string&gt;().toEqualTypeOf&lt;string&gt;();
expectTypeOf({ foo: 1 }).toHaveProperty("foo");
expectTypeOf&lt;Promise&lt;number&gt;&gt;().resolves.toBeNumber();
});
</code></pre>
<p>这些类型断言可以通过 <code>bunx tsc --noEmit</code> 在 CI 流程中验证，将类型安全检查提升到了新的高度。</p>
<h2>Node.js 兼容性的持续推进</h2>
<p>Bun 1.3 在每次提交时运行 Node.js 测试套件中额外的 800 个测试用例。官方数据显示，N-API 测试通过率已达 98% 以上，timers 模块通过率达到 98.4%。</p>
<p>对 <code>node:vm</code> 模块的全面支持是本次更新的重要成果。这个模块常用于代码沙箱、插件系统等高级场景，其实现难度较大。Bun 1.3 不仅支持基础的 <code>vm.Script</code>，还实现了 <code>vm.SourceTextModule</code>、<code>vm.SyntheticModule</code> 等高级 API，并支持字节码缓存以提升编译性能。</p>
<pre><code>import vm from "node:vm";
const script = new vm.Script('console.log("Hello from VM")');
script.runInThisContext();
</code></pre>
<p><code>node:test</code> 模块的初步支持让使用 Node.js 原生测试框架的项目能够在 Bun 上运行，这对于生态系统的兼容性意义重大。</p>
<p>加密性能的提升令人瞩目。DiffieHellman 和 Cipheriv/Decipheriv 的速度提升了约 400 倍，scrypt 提升了 6 倍。这些改进通过将关键路径用原生 C++ 重写实现，大幅降低了密码学操作的开销。</p>
<h2>Web 标准与现代 API</h2>
<p>Bun 1.3 新增了对多项 Web 标准的支持。<strong>YAML 原生支持</strong>让配置文件处理变得简单：</p>
<pre><code>import { YAML } from "bun";
const obj = YAML.parse("key: value");
const yaml = YAML.stringify({ key: "value" }, 0, 2);
// 直接导入 YAML 文件
import config from "./config.yaml";
</code></pre>
<p>官方实现的 YAML 解析器目前通过了官方测试套件的 90% 用例，性能表现优异。</p>
<p><strong>WebSocket 压缩</strong>功能的自动协商是另一个实用特性。当连接支持 <code>permessage-deflate</code> 时，Bun 会自动启用压缩，对于 JSON 等结构化数据，压缩率可达 60-80%，显著减少带宽消耗。</p>
<p><strong>Zstandard 压缩</strong>的全面支持包括 <code>fetch()</code> 的自动解压和手动压缩 API：</p>
<pre><code>import { zstdCompress, zstdDecompress } from "bun";
const compressed = await zstdCompress("Hello, world!");
const decompressed = await zstdDecompress(compressed);
</code></pre>
<p><code>DisposableStack</code> 和 <code>AsyncDisposableStack</code> 的实现体现了 Bun 对 TC39 提案的快速跟进。这些 API 与 <code>using</code> 和 <code>await using</code> 声明配合，提供了优雅的资源管理机制。</p>
<h2>性能优化：细节见真章</h2>
<p>Bun 1.3 的性能优化遍布各个层面。空闲 CPU 占用的降低源于对垃圾回收调度的优化，在没有进行中的请求时，<code>Bun.serve</code> 的计时器不再活跃。JavaScript 内存占用降低 10-30% (Next.js 应用降低 28%，Elysia 降低 11%) 归功于更智能的 GC 计时器调度。</p>
<p>I/O 线程池的优化让 macOS 上的 <code>Bun.build</code> 快了 60%。Express 性能提升 9%，Fastify 提升 5.4%，这些改进来自对 <code>node:http</code> 模块的持续优化。</p>
<p><code>postMessage</code> 的性能提升尤为惊人——字符串传递快了 500 倍，简单对象快了 240 倍。这通过避免对安全共享的字符串进行序列化实现，同时减少了约 22 倍的峰值内存使用。</p>
<pre><code>// 在 Worker 间传递大型 JSON 字符串现在快了 500 倍
const response = await fetch("https://api.example.com/data");
const json = await response.text();
postMessage(json);
</code></pre>
<p>启动时间减少了 1ms，内存占用减少了 3MB，这些看似微小的优化累积起来，对用户体验产生显著影响。</p>
<h2>开发者体验的精心打磨</h2>
<p>Bun 1.3 在开发者体验上的改进同样值得称道。TypeScript 默认配置改为 <code>"module": "Preserve"</code>，保留模块语法的原始形态，更符合 Bun 作为原生 ES 模块运行时的定位。</p>
<p><code>console.log()</code> 的深度控制让调试大型对象变得可控:</p>
<pre><code>bun --console-depth=5 ./app.ts
</code></pre>
<pre><code>[console]
depth = 5
</code></pre>
<p><code>BUN_OPTIONS</code> 环境变量提供了设置默认 CLI 参数的便捷方式：</p>
<pre><code>export BUN_OPTIONS="--watch --hot"
bun run ./app.ts
# 等同于: bun --watch --hot run ./app.ts
</code></pre>
<p><code>bunx --package</code> 标志让运行二进制文件与包名不一致的包变得简单：</p>
<pre><code>bunx --package=@typescript-eslint/parser eslint ./src
</code></pre>
<p>新增的 <code>bun why</code> 命令清晰地展示依赖链，解答「为什么这个包会出现在我的项目中」：</p>
<pre><code>bun why tailwindcss
</code></pre>
<p>这些看似细小的改进，体现了 Bun 团队对开发者日常工作流程的深刻理解。</p>
<h2>打包器与构建系统的增强</h2>
<p>Bun 的打包器在 1.3 版本中获得了跨平台编译能力。开发者现在可以在任何平台上为 Windows、macOS 和 Linux 构建可执行文件：</p>
<pre><code>bun build --compile --target=linux-x64 ./app.ts --outfile myapp-linux
bun build --compile --target=darwin-arm64 ./app.ts --outfile myapp-macos
bun build --compile --target=windows-x64 ./app.ts --outfile myapp.exe
</code></pre>
<p>Windows 可执行文件元数据的支持让企业应用的打包更加专业：</p>
<pre><code>bun build --compile --target=windows-x64 \\
--title="My App" \\
--publisher="My Company" \\
--version="1.0.0" \\
./app.ts
</code></pre>
<p>代码签名支持（Windows 的 Authenticode 和 macOS 的 codesign）确保了发布的可执行文件可以通过操作系统的安全验证。</p>
<p>压缩器变得更加智能，能够移除未使用的函数和类名，优化 <code>new Object()</code>、<code>new Array()</code> 等表达式，消除无用的 <code>try...catch...finally</code> 块。这些优化让生产构建的体积进一步缩小。</p>
<h2>安全性的持续关注</h2>
<p>Bun 1.3 引入了 <code>Bun.secrets</code> API，利用操作系统的原生凭据存储（macOS 的 Keychain、Linux 的 libsecret、Windows 的 Credential Manager）：</p>
<pre><code>import { secrets } from "bun";
await secrets.set({
service: "my-app",
name: "api-key",
value: "secret-value",
});
const key = await secrets.get({
service: "my-app",
name: "api-key",
});
</code></pre>
<p>凭据在静态时被加密，与环境变量分离存储，提供了更高的安全保障。</p>
<p><code>Bun.CSRF</code> 模块为跨站请求伪造防护提供了原生支持：</p>
<pre><code>import { CSRF } from "bun";
const secret = "your-secret-key";
const token = CSRF.generate({ secret, encoding: "hex", expiresIn: 60 * 1000 });
const isValid = CSRF.verify(token, { secret });
</code></pre>
<p>这些安全特性的内置化降低了开发者的心智负担，让安全最佳实践更容易落地。</p>
<h2>生态系统的影响与展望</h2>
<p>Bun 1.3 的发布标志着该项目从「快速运行时」向「完整开发平台」的战略转型。Midjourney 等知名公司已在生产环境中使用 Bun 进行前端开发，这证明了其稳定性和可靠性。</p>
<p>官方提到，每个提交都会运行大量的 Node.js 测试套件，表明 Bun 团队对兼容性的重视。对 <code>pnpm.lock</code> 和 <code>yarn.lock</code> 的迁移支持，让团队可以无痛试用 Bun，而无需说服所有成员同时升级工具链。</p>
<p>不过，1.3 版本也带来了一些破坏性变更。<code>Bun.serve()</code> 的 TypeScript 类型被重构，WebSocket 数据定义方式发生变化；SQL 客户端现在强制使用标签模板语法；测试过滤器在没有匹配用例时会报错而非静默成功。这些变更虽然可能给现有项目带来迁移成本，但从长远看有利于 API 的一致性和可维护性。</p>
<h2>数据说话：性能对比</h2>
<p>根据官方提供的基准测试数据:</p>
<ul>
<li><strong>Redis 客户端</strong>: 比 ioredis 快 7.9 倍以上</li>
<li><strong>postMessage 字符串</strong>: 速度提升 500 倍，内存减少 22 倍</li>
<li><strong>postMessage 对象</strong>: 速度提升 240 倍</li>
<li><strong>DiffieHellman</strong>: 约快 400 倍</li>
<li><strong>Cipheriv/Decipheriv</strong>: 约快 400 倍</li>
<li><strong>scrypt</strong>: 快 6 倍</li>
<li><strong><code>AbortSignal.timeout</code></strong>: 快 40 倍</li>
<li><strong><code>Headers</code> 操作</strong>: 快 2 倍</li>
<li><strong><code>Bun.build</code> on macOS</strong>: 快 60%</li>
<li><strong>Express</strong>: 快 9%</li>
<li><strong>Fastify</strong>: 快 5.4%</li>
</ul>
<p>这些数据展示了 Bun 在性能上的持续领先优势。</p>
<h2>社区反响与未来规划</h2>
<p>Bun 1.3 的发布在社区引发了热烈讨论。开发者们尤其关注全栈开发能力的提升和包管理器的企业级特性。Socket 公司 CTO Ahmad Nassri 对安全扫描 API 的评价颇具代表性：「Bun 团队行动迅速，在包管理器层面保护开发者。开放安全扫描 API，让 Socket 这样的工具能够在安装过程中提供实时威胁检测。这是让开源开发默认更安全的重要一步。」</p>
<p>官方表示，1.3 系列将持续关注全栈应用开发体验的提升。Redis 集群、流式处理和 Lua 脚本支持已在规划中。WebAssembly 流式编译的实现，以及对更多 TC39 提案的支持，也在进行中。</p>
<p>Bun 的快速迭代和对社区反馈的积极响应，让人对其未来发展充满期待。从一个「更快的 Node.js」到一个「完整的 JavaScript 平台」，Bun 正在书写属于自己的故事。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-662ba9a9f1.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-662ba9a9f1.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 12:33:57 +0800</pubDate>
  </item><item>
    <title><![CDATA[知鱼 v1.4.0 发布，项目管理系统]]></title>
    <link>https://www.oschina.net/news/377327</link>
    <itunes:title><![CDATA[知鱼 v1.4.0 发布，项目管理系统]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>知鱼项目管理系统1.4.0版本发布了，主要增加了甘特图功能，具体功能如下：</p>
<p>一、甘特图 增加工时导入功能，可通过甘特图方式查看项目任务进度</p>
<p>二、任务看板功能 任务看板增加了两侧任务栏的收起和展开功能，更方便界面的浏览 增加了任务启动时的开始时间和结束时间设置</p>
<p>三、其他 bug 修复： 修复项目任务未开始时进度显示错误问题 修复了知识库等一些权限问题 修复了任务详情页面的权限问题</p>
<p>功能预览 <img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-c540ab112a.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-55a1da0508.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-c405456208.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-929de46543.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-6181c4f607.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-bc8a5a1533.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-25174560bb.png"></p>
<p>其他下载方式：</p>
<p><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.zhiyupm.com%2Fdownloads%2F" target="_blank">https://www.zhiyupm.com/downloads/</a></p>
<p>安装使用： <a href="https://gitee.com/link?target=https%3A%2F%2Fwww.zhiyupm.com%2Fwiki%2F" target="_blank">https://www.zhiyupm.com/wiki/</a></p>
<p>详情查看：<a href="https://gitee.com/wy-soft/zhiyu/releases/v1.4.0">https://gitee.com/wy-soft/zhiyu/releases/v1.4.0</a></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>知鱼项目管理系统1.4.0版本发布了，主要增加了甘特图功能，具体功能如下：</p>
<p>一、甘特图 增加工时导入功能，可通过甘特图方式查看项目任务进度</p>
<p>二、任务看板功能 任务看板增加了两侧任务栏的收起和展开功能，更方便界面的浏览 增加了任务启动时的开始时间和结束时间设置</p>
<p>三、其他 bug 修复： 修复项目任务未开始时进度显示错误问题 修复了知识库等一些权限问题 修复了任务详情页面的权限问题</p>
<p>功能预览 <img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-c540ab112a.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-55a1da0508.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-c405456208.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-929de46543.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-6181c4f607.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-bc8a5a1533.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-25174560bb.png"></p>
<p>其他下载方式：</p>
<p><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.zhiyupm.com%2Fdownloads%2F" target="_blank">https://www.zhiyupm.com/downloads/</a></p>
<p>安装使用： <a href="https://gitee.com/link?target=https%3A%2F%2Fwww.zhiyupm.com%2Fwiki%2F" target="_blank">https://www.zhiyupm.com/wiki/</a></p>
<p>详情查看：<a href="https://gitee.com/wy-soft/zhiyu/releases/v1.4.0">https://gitee.com/wy-soft/zhiyu/releases/v1.4.0</a></p>]]>
    </description>
    <content:encoded><![CDATA[<p>知鱼项目管理系统1.4.0版本发布了，主要增加了甘特图功能，具体功能如下：</p>
<p>一、甘特图 增加工时导入功能，可通过甘特图方式查看项目任务进度</p>
<p>二、任务看板功能 任务看板增加了两侧任务栏的收起和展开功能，更方便界面的浏览 增加了任务启动时的开始时间和结束时间设置</p>
<p>三、其他 bug 修复： 修复项目任务未开始时进度显示错误问题 修复了知识库等一些权限问题 修复了任务详情页面的权限问题</p>
<p>功能预览 <img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-c540ab112a.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-55a1da0508.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-c405456208.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-929de46543.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-6181c4f607.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-bc8a5a1533.png"><img alt="输入图片说明" src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-25174560bb.png"></p>
<p>其他下载方式：</p>
<p><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.zhiyupm.com%2Fdownloads%2F" target="_blank">https://www.zhiyupm.com/downloads/</a></p>
<p>安装使用： <a href="https://gitee.com/link?target=https%3A%2F%2Fwww.zhiyupm.com%2Fwiki%2F" target="_blank">https://www.zhiyupm.com/wiki/</a></p>
<p>详情查看：<a href="https://gitee.com/wy-soft/zhiyu/releases/v1.4.0">https://gitee.com/wy-soft/zhiyu/releases/v1.4.0</a></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-c540ab112a.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/foruda.gitee.com-c540ab112a.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 12:23:20 +0800</pubDate>
  </item><item>
    <title><![CDATA[微软 Copilot AI 实现邮件和文件直接连接]]></title>
    <link>https://www.oschina.net/news/377321</link>
    <itunes:title><![CDATA[微软 Copilot AI 实现邮件和文件直接连接]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>微软近日宣布对其 Copilot AI 助手进行了重要升级，允许用户直接连接 Outlook、Gmail 等多款个人生产力应用。这项新功能的推出，使得用户在处理日常工作任务时更加高效，能够更轻松地获取所需信息。</p>
<p>根据微软的介绍，这项连接器功能是可选的，用户可以在设置中选择需要连接的服务。通过自然语言的提示，Copilot 可以自动查找邮件内容，例如总结同事发送的工作项目时间表，节省了用户查找信息的时间和精力。</p>
<p>此外，微软还宣布 Copilot 用户现在可以通过自然语言指令创建和导出 Word 文档、Excel 电子表格、PDF 文件和 PowerPoint 演示文稿。这意味着，用户只需简单输入提示，就可以立即将想法、笔记和数据转换为可分享和可编辑的文档。例如，用户可以要求 Copilot 帮助生成一个针对即将到来的假期的日常旅行行程，并将该行程上传至 PowerPoint 演示文稿中，然后导出文件与他人分享。</p>
<p>目前，这项升级和新连接器功能正逐步向微软 Copilot 的 “内测者” 计划成员推送。尽管该功能正在推广中，但并不是所有的内测者都会立刻获得这项更新。</p>
<p>这一升级的推出，正值微软加大力度扩展其核心 AI 产品的同时，努力将其与日常使用的生产力应用紧密结合。在这方面，微软与其他科技公司的竞争也愈加激烈。例如，近期，Perplexity 推出了新的 Email Assistant，能够直接从用户的 Gmail 或 Outlook 账户中提取信息。同时，微软还与 AI 初创公司 Anthropic 建立了合作关系，计划在 Microsoft365套件中嵌入该公司的 AI 系统。</p>
<p>对于企业用户来说，这些新功能的推出意味着他们可以更高效地管理日常工作流程，减少在信息检索和文档制作上的时间，提升整体办公效率。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>微软近日宣布对其 Copilot AI 助手进行了重要升级，允许用户直接连接 Outlook、Gmail 等多款个人生产力应用。这项新功能的推出，使得用户在处理日常工作任务时更加高效，能够更轻松地获取所需信息。</p>
<p>根据微软的介绍，这项连接器功能是可选的，用户可以在设置中选择需要连接的服务。通过自然语言的提示，Copilot 可以自动查找邮件内容，例如总结同事发送的工作项目时间表，节省了用户查找信息的时间和精力。</p>
<p>此外，微软还宣布 Copilot 用户现在可以通过自然语言指令创建和导出 Word 文档、Excel 电子表格、PDF 文件和 PowerPoint 演示文稿。这意味着，用户只需简单输入提示，就可以立即将想法、笔记和数据转换为可分享和可编辑的文档。例如，用户可以要求 Copilot 帮助生成一个针对即将到来的假期的日常旅行行程，并将该行程上传至 PowerPoint 演示文稿中，然后导出文件与他人分享。</p>
<p>目前，这项升级和新连接器功能正逐步向微软 Copilot 的 “内测者” 计划成员推送。尽管该功能正在推广中，但并不是所有的内测者都会立刻获得这项更新。</p>
<p>这一升级的推出，正值微软加大力度扩展其核心 AI 产品的同时，努力将其与日常使用的生产力应用紧密结合。在这方面，微软与其他科技公司的竞争也愈加激烈。例如，近期，Perplexity 推出了新的 Email Assistant，能够直接从用户的 Gmail 或 Outlook 账户中提取信息。同时，微软还与 AI 初创公司 Anthropic 建立了合作关系，计划在 Microsoft365套件中嵌入该公司的 AI 系统。</p>
<p>对于企业用户来说，这些新功能的推出意味着他们可以更高效地管理日常工作流程，减少在信息检索和文档制作上的时间，提升整体办公效率。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>微软近日宣布对其 Copilot AI 助手进行了重要升级，允许用户直接连接 Outlook、Gmail 等多款个人生产力应用。这项新功能的推出，使得用户在处理日常工作任务时更加高效，能够更轻松地获取所需信息。</p>
<p>根据微软的介绍，这项连接器功能是可选的，用户可以在设置中选择需要连接的服务。通过自然语言的提示，Copilot 可以自动查找邮件内容，例如总结同事发送的工作项目时间表，节省了用户查找信息的时间和精力。</p>
<p>此外，微软还宣布 Copilot 用户现在可以通过自然语言指令创建和导出 Word 文档、Excel 电子表格、PDF 文件和 PowerPoint 演示文稿。这意味着，用户只需简单输入提示，就可以立即将想法、笔记和数据转换为可分享和可编辑的文档。例如，用户可以要求 Copilot 帮助生成一个针对即将到来的假期的日常旅行行程，并将该行程上传至 PowerPoint 演示文稿中，然后导出文件与他人分享。</p>
<p>目前，这项升级和新连接器功能正逐步向微软 Copilot 的 “内测者” 计划成员推送。尽管该功能正在推广中，但并不是所有的内测者都会立刻获得这项更新。</p>
<p>这一升级的推出，正值微软加大力度扩展其核心 AI 产品的同时，努力将其与日常使用的生产力应用紧密结合。在这方面，微软与其他科技公司的竞争也愈加激烈。例如，近期，Perplexity 推出了新的 Email Assistant，能够直接从用户的 Gmail 或 Outlook 账户中提取信息。同时，微软还与 AI 初创公司 Anthropic 建立了合作关系，计划在 Microsoft365套件中嵌入该公司的 AI 系统。</p>
<p>对于企业用户来说，这些新功能的推出意味着他们可以更高效地管理日常工作流程，减少在信息检索和文档制作上的时间，提升整体办公效率。</p>]]></content:encoded>
    
    <pubDate>Tue, 14 Oct 2025 11:49:57 +0800</pubDate>
  </item><item>
    <title><![CDATA[宇树 G1 机器人最新演示视频：耍起了功夫、后空翻稳健落地]]></title>
    <link>https://www.oschina.net/news/377320</link>
    <itunes:title><![CDATA[宇树 G1 机器人最新演示视频：耍起了功夫、后空翻稳健落地]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>宇树科技昨天<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1818617132%2FQ8WcC724X%3Fpagetype%3Dprofilefeed" target="_blank">发布</a>最新演示视频，展示了其 G1 人形机器人在复杂动作上的新突破。视频中，机器人不仅完成了标准的功夫招式，还流畅地完成了后空翻、翻跟头等高难度动作，整体表现更加自然、连贯。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-9bc6474cda.png"></p>
<p>官方介绍称，这一版本在动作控制与稳定性方面进行了大幅优化，使机器人能够在高速运动中保持平衡，并展现出接近人类的灵活性。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-484d233ac8.gif"></p>
<p>相比此前的演示，本次视频更强调“功夫”元素，动作衔接丝滑，观感更具娱乐性与观赏性。此前，宇树科技曾发布机器人<a href="https://www.oschina.net/news/373906" target="_blank">被“围殴”的视频</a>，视频中的机器人在遭受围堵、多次被踹倒的情况下依然能快速起身、保持平衡，并作出多次后空翻等高难度动作。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>宇树科技昨天<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1818617132%2FQ8WcC724X%3Fpagetype%3Dprofilefeed" target="_blank">发布</a>最新演示视频，展示了其 G1 人形机器人在复杂动作上的新突破。视频中，机器人不仅完成了标准的功夫招式，还流畅地完成了后空翻、翻跟头等高难度动作，整体表现更加自然、连贯。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-9bc6474cda.png"></p>
<p>官方介绍称，这一版本在动作控制与稳定性方面进行了大幅优化，使机器人能够在高速运动中保持平衡，并展现出接近人类的灵活性。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-484d233ac8.gif"></p>
<p>相比此前的演示，本次视频更强调“功夫”元素，动作衔接丝滑，观感更具娱乐性与观赏性。此前，宇树科技曾发布机器人<a href="https://www.oschina.net/news/373906" target="_blank">被“围殴”的视频</a>，视频中的机器人在遭受围堵、多次被踹倒的情况下依然能快速起身、保持平衡，并作出多次后空翻等高难度动作。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>宇树科技昨天<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1818617132%2FQ8WcC724X%3Fpagetype%3Dprofilefeed" target="_blank">发布</a>最新演示视频，展示了其 G1 人形机器人在复杂动作上的新突破。视频中，机器人不仅完成了标准的功夫招式，还流畅地完成了后空翻、翻跟头等高难度动作，整体表现更加自然、连贯。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-9bc6474cda.png"></p>
<p>官方介绍称，这一版本在动作控制与稳定性方面进行了大幅优化，使机器人能够在高速运动中保持平衡，并展现出接近人类的灵活性。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-484d233ac8.gif"></p>
<p>相比此前的演示，本次视频更强调“功夫”元素，动作衔接丝滑，观感更具娱乐性与观赏性。此前，宇树科技曾发布机器人<a href="https://www.oschina.net/news/373906" target="_blank">被“围殴”的视频</a>，视频中的机器人在遭受围堵、多次被踹倒的情况下依然能快速起身、保持平衡，并作出多次后空翻等高难度动作。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-9bc6474cda.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-9bc6474cda.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 11:46:07 +0800</pubDate>
  </item><item>
    <title><![CDATA[微软 Windows 10 停服在即，官方升级工具却突然罢工]]></title>
    <link>https://www.oschina.net/news/377316</link>
    <itunes:title><![CDATA[微软 Windows 10 停服在即，官方升级工具却突然罢工]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>10月13日消息，Windows 10的官方支持将在10月14日<a href="https://www.oschina.net/news/377083" target="_blank">结束</a>，微软最近也经常发布广告和推广内容，以突出Windows 11的巨大优势，不少用户也选择升级到Windows 11。</p>
<p>不过讽刺的是，就在这个关键时刻，微软的官方工具却出现了问题。微软的官方可启动ISO创建工具——媒体创建工具（Media Creation Tool，MCT）在Windows 10上突然无法正常工作。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b361c0c47.png"></p>
<p>微软确认了这一问题，并表示：“Windows 11媒体创建工具26100.6584（于2025年9月29日发布）在Windows 10设备上使用时可能无法按预期工作。该工具可能会意外关闭，且不会显示任何错误信息。”对于用户来说，如果电脑满足硬件要求，升级到Windows 11的最简单方法是打开Windows 10的“设置”，选择“更新与安全”&gt;“Windows更新”，然后点击“检查更新”。</p>
<p>如果想要全新安装，目前最可靠的方法仍然是使用Windows 11媒体创建工具，但不幸的是，这个工具现在却因为微软的失误而无法使用。微软表示正在努力修复这一问题，并将在未来发布更新，但没有明确的时间表。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>10月13日消息，Windows 10的官方支持将在10月14日<a href="https://www.oschina.net/news/377083" target="_blank">结束</a>，微软最近也经常发布广告和推广内容，以突出Windows 11的巨大优势，不少用户也选择升级到Windows 11。</p>
<p>不过讽刺的是，就在这个关键时刻，微软的官方工具却出现了问题。微软的官方可启动ISO创建工具——媒体创建工具（Media Creation Tool，MCT）在Windows 10上突然无法正常工作。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b361c0c47.png"></p>
<p>微软确认了这一问题，并表示：“Windows 11媒体创建工具26100.6584（于2025年9月29日发布）在Windows 10设备上使用时可能无法按预期工作。该工具可能会意外关闭，且不会显示任何错误信息。”对于用户来说，如果电脑满足硬件要求，升级到Windows 11的最简单方法是打开Windows 10的“设置”，选择“更新与安全”&gt;“Windows更新”，然后点击“检查更新”。</p>
<p>如果想要全新安装，目前最可靠的方法仍然是使用Windows 11媒体创建工具，但不幸的是，这个工具现在却因为微软的失误而无法使用。微软表示正在努力修复这一问题，并将在未来发布更新，但没有明确的时间表。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>10月13日消息，Windows 10的官方支持将在10月14日<a href="https://www.oschina.net/news/377083" target="_blank">结束</a>，微软最近也经常发布广告和推广内容，以突出Windows 11的巨大优势，不少用户也选择升级到Windows 11。</p>
<p>不过讽刺的是，就在这个关键时刻，微软的官方工具却出现了问题。微软的官方可启动ISO创建工具——媒体创建工具（Media Creation Tool，MCT）在Windows 10上突然无法正常工作。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b361c0c47.png"></p>
<p>微软确认了这一问题，并表示：“Windows 11媒体创建工具26100.6584（于2025年9月29日发布）在Windows 10设备上使用时可能无法按预期工作。该工具可能会意外关闭，且不会显示任何错误信息。”对于用户来说，如果电脑满足硬件要求，升级到Windows 11的最简单方法是打开Windows 10的“设置”，选择“更新与安全”&gt;“Windows更新”，然后点击“检查更新”。</p>
<p>如果想要全新安装，目前最可靠的方法仍然是使用Windows 11媒体创建工具，但不幸的是，这个工具现在却因为微软的失误而无法使用。微软表示正在努力修复这一问题，并将在未来发布更新，但没有明确的时间表。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b361c0c47.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b361c0c47.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 11:35:46 +0800</pubDate>
  </item><item>
    <title><![CDATA[蚂蚁百灵大模型团队正式发布并开源万亿思考模型 Ring-1T]]></title>
    <link>https://www.oschina.net/news/377308</link>
    <itunes:title><![CDATA[蚂蚁百灵大模型团队正式发布并开源万亿思考模型 Ring-1T]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>蚂蚁百灵大模型团队正式发布了万亿思考模型Ring-1T。发布即开源，开发者可以通过Hugging Face、魔搭社区下载模型权重，也可以通过Ling Chat页面和ZenMux 进行直连模型的chat体验和API 调用。</p>
<blockquote>
<p>Hugging Face：https://huggingface.co/inclusionAI/Ring-1T<br> ModelScope：https://modelscope.cn/models/inclusionAI/Ring-1T<br> Ling chat（国内用户）：https://ling.tbox.cn/chat<br> ZenMux（海外开发者，Chat/API ）：https://zenmux.ai/inclusionai/ring-1t</p>
</blockquote>
<p>Ring-1T是一款基于Ling 2.0架构的万亿参数思考模型。其总参数量达到1万亿，激活参数为500亿，并支持128K上下文窗口。模型权重已同步上线Hugging Face与ModelScope，同时提供了FP8版本。</p>
<p>正式版在上月底发布的<a href="https://www.oschina.net/news/375363" target="_blank"> preview 版本</a>基础上，持续扩展大规模可验证奖励强化学习（RLVR）训练，进一步激发万亿基座的自然语言推理能力，并通过 RLHF 训练完善模型通用能力，使得本次发布的 Ring-1T 在各项任务上表现更均衡。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-41c62cd65d.png"></p>
<p>Ring-1T 沿用 Ling 2.0 架构，在1T总参数、50B激活参数的 Ling-1T-base 基座上进行训练，支持最高 128K 上下文窗口。依托自研的强化学习稳定训练方法icepop（棒冰）与高效强化学习系统&nbsp;ASystem（其中&nbsp;AReaL 框架已开源），实现了从百亿（Ring-mini-2.0）到千亿（Ring-flash-2.0）再到万亿（Ring-1T）的&nbsp;MoE 架构强化学习平稳扩展，显著提升模型的深度思考与自然语言推理能力。</p>
<blockquote>
<p>相关阅读：<em><a href="https://www.oschina.net/news/376333" target="news">蚂蚁百灵大模型团队发布 Ling-1T：万亿参数“非思考”模型、基于 MoE 架构</a></em></p>
</blockquote>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>蚂蚁百灵大模型团队正式发布了万亿思考模型Ring-1T。发布即开源，开发者可以通过Hugging Face、魔搭社区下载模型权重，也可以通过Ling Chat页面和ZenMux 进行直连模型的chat体验和API 调用。</p>
<blockquote>
<p>Hugging Face：https://huggingface.co/inclusionAI/Ring-1T<br> ModelScope：https://modelscope.cn/models/inclusionAI/Ring-1T<br> Ling chat（国内用户）：https://ling.tbox.cn/chat<br> ZenMux（海外开发者，Chat/API ）：https://zenmux.ai/inclusionai/ring-1t</p>
</blockquote>
<p>Ring-1T是一款基于Ling 2.0架构的万亿参数思考模型。其总参数量达到1万亿，激活参数为500亿，并支持128K上下文窗口。模型权重已同步上线Hugging Face与ModelScope，同时提供了FP8版本。</p>
<p>正式版在上月底发布的<a href="https://www.oschina.net/news/375363" target="_blank"> preview 版本</a>基础上，持续扩展大规模可验证奖励强化学习（RLVR）训练，进一步激发万亿基座的自然语言推理能力，并通过 RLHF 训练完善模型通用能力，使得本次发布的 Ring-1T 在各项任务上表现更均衡。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-41c62cd65d.png"></p>
<p>Ring-1T 沿用 Ling 2.0 架构，在1T总参数、50B激活参数的 Ling-1T-base 基座上进行训练，支持最高 128K 上下文窗口。依托自研的强化学习稳定训练方法icepop（棒冰）与高效强化学习系统&nbsp;ASystem（其中&nbsp;AReaL 框架已开源），实现了从百亿（Ring-mini-2.0）到千亿（Ring-flash-2.0）再到万亿（Ring-1T）的&nbsp;MoE 架构强化学习平稳扩展，显著提升模型的深度思考与自然语言推理能力。</p>
<blockquote>
<p>相关阅读：<em><a href="https://www.oschina.net/news/376333" target="news">蚂蚁百灵大模型团队发布 Ling-1T：万亿参数“非思考”模型、基于 MoE 架构</a></em></p>
</blockquote>]]>
    </description>
    <content:encoded><![CDATA[<p>蚂蚁百灵大模型团队正式发布了万亿思考模型Ring-1T。发布即开源，开发者可以通过Hugging Face、魔搭社区下载模型权重，也可以通过Ling Chat页面和ZenMux 进行直连模型的chat体验和API 调用。</p>
<blockquote>
<p>Hugging Face：https://huggingface.co/inclusionAI/Ring-1T<br> ModelScope：https://modelscope.cn/models/inclusionAI/Ring-1T<br> Ling chat（国内用户）：https://ling.tbox.cn/chat<br> ZenMux（海外开发者，Chat/API ）：https://zenmux.ai/inclusionai/ring-1t</p>
</blockquote>
<p>Ring-1T是一款基于Ling 2.0架构的万亿参数思考模型。其总参数量达到1万亿，激活参数为500亿，并支持128K上下文窗口。模型权重已同步上线Hugging Face与ModelScope，同时提供了FP8版本。</p>
<p>正式版在上月底发布的<a href="https://www.oschina.net/news/375363" target="_blank"> preview 版本</a>基础上，持续扩展大规模可验证奖励强化学习（RLVR）训练，进一步激发万亿基座的自然语言推理能力，并通过 RLHF 训练完善模型通用能力，使得本次发布的 Ring-1T 在各项任务上表现更均衡。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-41c62cd65d.png"></p>
<p>Ring-1T 沿用 Ling 2.0 架构，在1T总参数、50B激活参数的 Ling-1T-base 基座上进行训练，支持最高 128K 上下文窗口。依托自研的强化学习稳定训练方法icepop（棒冰）与高效强化学习系统&nbsp;ASystem（其中&nbsp;AReaL 框架已开源），实现了从百亿（Ring-mini-2.0）到千亿（Ring-flash-2.0）再到万亿（Ring-1T）的&nbsp;MoE 架构强化学习平稳扩展，显著提升模型的深度思考与自然语言推理能力。</p>
<blockquote>
<p>相关阅读：<em><a href="https://www.oschina.net/news/376333" target="news">蚂蚁百灵大模型团队发布 Ling-1T：万亿参数“非思考”模型、基于 MoE 架构</a></em></p>
</blockquote>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-41c62cd65d.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-41c62cd65d.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 11:18:17 +0800</pubDate>
  </item><item>
    <title><![CDATA[MuseScore Studio 4.6.2 发布]]></title>
    <link>https://www.oschina.net/news/377305/musescore-4-6-2-released</link>
    <itunes:title><![CDATA[MuseScore Studio 4.6.2 发布]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><span>MuseScore&nbsp;是一个可运行在多种平台上的 WYSIWYG 的音乐制谱软件。目前，MuseScore 4.6.2 已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmusescore.org%2Fen%2F4.6.2" target="_blank">发布</a>，<span>本次更新修复了几个重要的回归问题。</span></span></p>
<p><span><span>此版本还恢复了与 macOS 10.15 和 11 的兼容性。该兼容性在 4.6 版本中因升级至 Qt 6.9.2 而意外丢失。现已对 Qt 进行修补，确保其与上述版本以及最新发布的 macOS Tahoe 26 保持兼容。</span></span></p>
<p><span>具体更新内容如下：</span></p>
<p><span><span><strong>Engraving</strong></span></span></p>
<ul>
<li><span>修复了和弦符号中的回归问题，其中“omit”呈现为“sommit”</span></li>
<li><span>修复了不可见 measure numbers 影响垂直间距的回归问题</span></li>
<li><span>修复了音符括号被错误标记为<code>generated</code>导致保存加载异常的回归问题</span></li>
</ul>
<p><span><span><strong>Interaction</strong></span></span></p>
<ul>
<li><span>修复了使用鼠标拖动音符导致系统高度缩小的问题</span></li>
<li><span>修复了对某系统音符进行微调会导致后续系统指板图位移的回归问题</span></li>
</ul>
<p><span><span><strong>UI</strong></span></span></p>
<ul>
<li><span>修复了应用程序若在最后退出前处于最小化状态，则下次启动时仍保持最小化的回归问题</span></li>
</ul>
<p><span><span>此版本还包含&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmusescore.org%2Fen%2F4.6" target="_blank"><span>4.6</span></a>&nbsp;<span>和&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmusescore.org%2Fen%2F4.6.1" target="_blank"><span>4.6.1</span></a>&nbsp;<span>中的所有新功能，详情可查看</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJ2gY9CbMuoI" target="_blank">4.6 版本视频</a><span>。</span></span></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><span>MuseScore&nbsp;是一个可运行在多种平台上的 WYSIWYG 的音乐制谱软件。目前，MuseScore 4.6.2 已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmusescore.org%2Fen%2F4.6.2" target="_blank">发布</a>，<span>本次更新修复了几个重要的回归问题。</span></span></p>
<p><span><span>此版本还恢复了与 macOS 10.15 和 11 的兼容性。该兼容性在 4.6 版本中因升级至 Qt 6.9.2 而意外丢失。现已对 Qt 进行修补，确保其与上述版本以及最新发布的 macOS Tahoe 26 保持兼容。</span></span></p>
<p><span>具体更新内容如下：</span></p>
<p><span><span><strong>Engraving</strong></span></span></p>
<ul>
<li><span>修复了和弦符号中的回归问题，其中“omit”呈现为“sommit”</span></li>
<li><span>修复了不可见 measure numbers 影响垂直间距的回归问题</span></li>
<li><span>修复了音符括号被错误标记为<code>generated</code>导致保存加载异常的回归问题</span></li>
</ul>
<p><span><span><strong>Interaction</strong></span></span></p>
<ul>
<li><span>修复了使用鼠标拖动音符导致系统高度缩小的问题</span></li>
<li><span>修复了对某系统音符进行微调会导致后续系统指板图位移的回归问题</span></li>
</ul>
<p><span><span><strong>UI</strong></span></span></p>
<ul>
<li><span>修复了应用程序若在最后退出前处于最小化状态，则下次启动时仍保持最小化的回归问题</span></li>
</ul>
<p><span><span>此版本还包含&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmusescore.org%2Fen%2F4.6" target="_blank"><span>4.6</span></a>&nbsp;<span>和&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmusescore.org%2Fen%2F4.6.1" target="_blank"><span>4.6.1</span></a>&nbsp;<span>中的所有新功能，详情可查看</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJ2gY9CbMuoI" target="_blank">4.6 版本视频</a><span>。</span></span></p>]]>
    </description>
    <content:encoded><![CDATA[<p><span>MuseScore&nbsp;是一个可运行在多种平台上的 WYSIWYG 的音乐制谱软件。目前，MuseScore 4.6.2 已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmusescore.org%2Fen%2F4.6.2" target="_blank">发布</a>，<span>本次更新修复了几个重要的回归问题。</span></span></p>
<p><span><span>此版本还恢复了与 macOS 10.15 和 11 的兼容性。该兼容性在 4.6 版本中因升级至 Qt 6.9.2 而意外丢失。现已对 Qt 进行修补，确保其与上述版本以及最新发布的 macOS Tahoe 26 保持兼容。</span></span></p>
<p><span>具体更新内容如下：</span></p>
<p><span><span><strong>Engraving</strong></span></span></p>
<ul>
<li><span>修复了和弦符号中的回归问题，其中“omit”呈现为“sommit”</span></li>
<li><span>修复了不可见 measure numbers 影响垂直间距的回归问题</span></li>
<li><span>修复了音符括号被错误标记为<code>generated</code>导致保存加载异常的回归问题</span></li>
</ul>
<p><span><span><strong>Interaction</strong></span></span></p>
<ul>
<li><span>修复了使用鼠标拖动音符导致系统高度缩小的问题</span></li>
<li><span>修复了对某系统音符进行微调会导致后续系统指板图位移的回归问题</span></li>
</ul>
<p><span><span><strong>UI</strong></span></span></p>
<ul>
<li><span>修复了应用程序若在最后退出前处于最小化状态，则下次启动时仍保持最小化的回归问题</span></li>
</ul>
<p><span><span>此版本还包含&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmusescore.org%2Fen%2F4.6" target="_blank"><span>4.6</span></a>&nbsp;<span>和&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmusescore.org%2Fen%2F4.6.1" target="_blank"><span>4.6.1</span></a>&nbsp;<span>中的所有新功能，详情可查看</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJ2gY9CbMuoI" target="_blank">4.6 版本视频</a><span>。</span></span></p>]]></content:encoded>
    
    <pubDate>Tue, 14 Oct 2025 11:08:15 +0800</pubDate>
  </item><item>
    <title><![CDATA[Meta 超级智能实验室推出新技术，使大模型 RAG 推理速度提升 30 倍]]></title>
    <link>https://www.oschina.net/news/377303</link>
    <itunes:title><![CDATA[Meta 超级智能实验室推出新技术，使大模型 RAG 推理速度提升 30 倍]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>Meta 的<span>超级</span>智能实验室（Meta Superintelligence Labs，MSL）发表了首篇重要论文，研究成果显著提升了大语言模型在检索增强生成(RAG)任务中的推理速度，提升幅度达到了30倍以上。</p>
<p>这篇论文名为《<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.01092" target="_blank">REFRAG:Rethinking RAG based Decoding</a>》，主要探讨如何让大型语言模型在执行 RAG 任务时，快速提炼出重要信息，以减少计算量并缩短反应时间，而同时保持准确性不变。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e8164a9e5.png"></p>
<p>Meta<span>&nbsp;</span><span>超级</span>智能实验室于今年6月正式成立，总部位于加利福尼亚州的门洛帕克，旨在研发<span>超级</span>智能技术。根据报道，扎克伯格在4月份对 Meta<span>&nbsp;</span><span>最新</span>发布的 Llama4模型表现不满，甚至要求员工加班加点来改进。这促使他成立了这个新实验室，并引入了大量<span>顶尖</span>人才，包括 Scale AI 的创始人 Alexandr Wang。</p>
<p>在实验室内部，团队被分为四个小组，分别负责大语言模型的研发、人工智能基础研究、产品技术落地以及基础设施的保障。REFRAG 框架的提出，正是实验室在优化大语言模型性能方面的<span>第一</span>步。</p>
<p>REFRAG 框架的核心理念是，通过一个轻量级模型将冗长的上下文内容压缩成摘要，减少解码器处理的输入信息。这种方法不仅加快了处理速度，还降低了计算量，提高了模型的效率。此外，研究团队还采用了 “持续预训练” 的方法，通过重建任务训练模型，以便在压缩信息的同时，尽量保留重要的细节。</p>
<p>经过全面测试，REFRAG 在多种任务中表现出色，尤其在时间延迟和吞吐量方面大幅提升。实验结果显示，REFRAG 在压缩比为16倍的情况下，能够在速度上超越之前的<span>最先</span>进模型 CEPE，并且在准确性上几乎没有损失。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>Meta 的<span>超级</span>智能实验室（Meta Superintelligence Labs，MSL）发表了首篇重要论文，研究成果显著提升了大语言模型在检索增强生成(RAG)任务中的推理速度，提升幅度达到了30倍以上。</p>
<p>这篇论文名为《<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.01092" target="_blank">REFRAG:Rethinking RAG based Decoding</a>》，主要探讨如何让大型语言模型在执行 RAG 任务时，快速提炼出重要信息，以减少计算量并缩短反应时间，而同时保持准确性不变。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e8164a9e5.png"></p>
<p>Meta<span>&nbsp;</span><span>超级</span>智能实验室于今年6月正式成立，总部位于加利福尼亚州的门洛帕克，旨在研发<span>超级</span>智能技术。根据报道，扎克伯格在4月份对 Meta<span>&nbsp;</span><span>最新</span>发布的 Llama4模型表现不满，甚至要求员工加班加点来改进。这促使他成立了这个新实验室，并引入了大量<span>顶尖</span>人才，包括 Scale AI 的创始人 Alexandr Wang。</p>
<p>在实验室内部，团队被分为四个小组，分别负责大语言模型的研发、人工智能基础研究、产品技术落地以及基础设施的保障。REFRAG 框架的提出，正是实验室在优化大语言模型性能方面的<span>第一</span>步。</p>
<p>REFRAG 框架的核心理念是，通过一个轻量级模型将冗长的上下文内容压缩成摘要，减少解码器处理的输入信息。这种方法不仅加快了处理速度，还降低了计算量，提高了模型的效率。此外，研究团队还采用了 “持续预训练” 的方法，通过重建任务训练模型，以便在压缩信息的同时，尽量保留重要的细节。</p>
<p>经过全面测试，REFRAG 在多种任务中表现出色，尤其在时间延迟和吞吐量方面大幅提升。实验结果显示，REFRAG 在压缩比为16倍的情况下，能够在速度上超越之前的<span>最先</span>进模型 CEPE，并且在准确性上几乎没有损失。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>Meta 的<span>超级</span>智能实验室（Meta Superintelligence Labs，MSL）发表了首篇重要论文，研究成果显著提升了大语言模型在检索增强生成(RAG)任务中的推理速度，提升幅度达到了30倍以上。</p>
<p>这篇论文名为《<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.01092" target="_blank">REFRAG:Rethinking RAG based Decoding</a>》，主要探讨如何让大型语言模型在执行 RAG 任务时，快速提炼出重要信息，以减少计算量并缩短反应时间，而同时保持准确性不变。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e8164a9e5.png"></p>
<p>Meta<span>&nbsp;</span><span>超级</span>智能实验室于今年6月正式成立，总部位于加利福尼亚州的门洛帕克，旨在研发<span>超级</span>智能技术。根据报道，扎克伯格在4月份对 Meta<span>&nbsp;</span><span>最新</span>发布的 Llama4模型表现不满，甚至要求员工加班加点来改进。这促使他成立了这个新实验室，并引入了大量<span>顶尖</span>人才，包括 Scale AI 的创始人 Alexandr Wang。</p>
<p>在实验室内部，团队被分为四个小组，分别负责大语言模型的研发、人工智能基础研究、产品技术落地以及基础设施的保障。REFRAG 框架的提出，正是实验室在优化大语言模型性能方面的<span>第一</span>步。</p>
<p>REFRAG 框架的核心理念是，通过一个轻量级模型将冗长的上下文内容压缩成摘要，减少解码器处理的输入信息。这种方法不仅加快了处理速度，还降低了计算量，提高了模型的效率。此外，研究团队还采用了 “持续预训练” 的方法，通过重建任务训练模型，以便在压缩信息的同时，尽量保留重要的细节。</p>
<p>经过全面测试，REFRAG 在多种任务中表现出色，尤其在时间延迟和吞吐量方面大幅提升。实验结果显示，REFRAG 在压缩比为16倍的情况下，能够在速度上超越之前的<span>最先</span>进模型 CEPE，并且在准确性上几乎没有损失。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e8164a9e5.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8e8164a9e5.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 10:56:19 +0800</pubDate>
  </item><item>
    <title><![CDATA[Rancher 社区双周报｜ Longhorn v1.10.0 重磅发布]]></title>
    <link>https://my.oschina.net/rancher/blog/18695411</link>
    <itunes:title><![CDATA[Rancher 社区双周报｜ Longhorn v1.10.0 重磅发布]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>在本期 <strong>Rancher 社区双周报</strong> 中，我们为大家带来了多个核心产品的最新版本动态： <strong>Longhorn</strong> 发布了 v1.9.2 与 v1.10.0 两个版本，其中 v1.10.0 引入了 V2 Data Engine 的重大增强，带来更高性能与更强扩展性； <strong>Rancher</strong> 发布了四个版本（v2.9.12、v2.10.10、v2.11.6、v2.12.2），其中多个 Prime 版本聚焦于安全修复与系统稳健性提升； <strong>RKE2 与 K3s</strong> 分支均完成了 Kubernetes 版本的例行更新，优化核心组件并强化集群可靠性； 同时，<strong>Harvester v1.5.2</strong> 带来了更流畅的虚拟化体验，<strong>K3k v0.3.5</strong> 则在资源同步与镜像管理方面持续进化。</p>
<p>这一系列更新共同展现了 Rancher 技术生态的持续完善与活力，为用户在容器、虚拟化与边缘计算场景中的落地提供了更加坚实的基础。</p>
<h2>Longhorn</h2>
<p>Longhorn 发布了 <strong>v1.9.2</strong> 与 <strong>v1.10.0</strong> 两个版本更新。本次更新聚焦于系统稳定性与性能增强，同时引入了多项新特性与兼容性优化，进一步提升了 Longhorn 在企业级云原生存储场景中的可靠性与易用性。</p>
<h3>Longhorn v1.9.2</h3>
<p>Longhorn v1.9.2 版本主要聚焦于系统稳定性和可靠性提升，修复了多个潜在问题，包括卷扩容时可能出现的数据损坏、磁盘空间不足引起的卷异常、以及备份目标不可用导致的恢复失败等。该版本还改进了探针配置、日志输出及备份超时机制，优化了支持包的收集范围，进一步提升了系统的可维护性和故障排查效率。</p>
<h3>Longhorn v1.10.0</h3>
<p>Longhorn v1.10.0 是一次重要的功能更新，引入了对 <strong>V2 Data Engine</strong> 的全面增强，在性能、资源占用和可扩展性方面均有显著提升。 该版本在存储引擎层面增加了中断模式以降低 CPU 消耗，支持卷克隆、在线扩容、QoS 限速等关键能力，为多租户与高负载场景提供了更高的灵活性和性能保障。</p>
<p>此外，Longhorn v1.10.0 进一步完善了网络与存储调度支持，新增 <strong>IPv6 网络兼容</strong> 与 <strong>CSIStorageCapacity</strong> 调度优化机制；在数据保护方面，引入了可配置的备份块大小与更高效的备份清理逻辑。 同时，Longhorn 统一了配置文件格式，移除了旧版 v1beta1 API，并对 UI 界面与监控指标体系进行了重构，使系统的可观测性、操作体验与上层产品（如 Harvester）的集成能力显著增强。</p>
<p>如需详细了解各版本的更新内容，请查阅：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Flonghorn%2Flonghorn%2Freleases%2Ftag%2Fv1.9.2" target="_blank">Longhorn v1.9.2 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Flonghorn%2Flonghorn%2Freleases%2Ftag%2Fv1.10.0" target="_blank">Longhorn v1.10.0 发布说明</a></li>
</ul>
<h2>Rancher</h2>
<p>近期，Rancher 团队同步发布了多个补丁版本：<strong>v2.9.12、v2.10.10、v2.11.6 与 v2.12.2</strong>。其中，<strong>v2.9.12、v2.10.10 和 v2.11.6 为 Prime 版本</strong>，主要面向企业用户提供安全加固与维护修复；<strong>v2.12.2 则为 Community 与 Prime 双版本同步更新</strong>，覆盖了开源与商业发行渠道。</p>
<p>本次更新的核心聚焦于 <strong>安全漏洞修复与系统稳健性提升</strong>，共涉及以下三个安全问题：</p>
<h3>安全修复重点</h3>
<ul>
<li> <p><strong>用户名唯一性限制（CVE-2024-58260）</strong> 之前版本中，若用户 A 和 B 拥有相同用户名，可能导致任一用户无法登录，甚至可被恶意利用阻止管理员登录。 新版本中，用户名一旦设置将不可修改，且禁止创建重复用户名，从根本上防止该问题发生。</p> </li>
<li> <p><strong>CLI 登录安全增强（CVE-2024-58267）</strong> Rancher CLI 现已在登录过程中更明显地显示 <code>requestId</code>，并在请求中添加 <code>cli=true</code> 标识。 Rancher Dashboard 会根据该标识识别请求来源并提示用户验证操作，从而防止伪造登录请求。</p> </li>
<li> <p><strong>敏感头字段清理（CVE-2025-54468）</strong> Rancher 现已移除 <code>/meta/proxy</code> 接口中的 <code>Impersonate-*</code> 请求头，避免在创建云凭证等场景中泄露可识别或敏感信息。</p> </li>
</ul>
<p>这些修复已全部在 <strong>v2.9.12、v2.10.10、v2.11.6 与 v2.12.2</strong> 中生效，显著提升了 Rancher 在身份验证与数据安全方面的防护能力。</p>
<p>如需详细了解各版本发布内容，可参阅：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.9.12" target="_blank">Rancher v2.9.12 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.10.10" target="_blank">Rancher v2.10.10 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.11.6" target="_blank">Rancher v2.11.6 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.12.2" target="_blank">Rancher v2.12.2 发布说明</a></li>
</ul>
<h2>Harvester</h2>
<p>在本次更新中，<strong>Harvester 发布了 v1.5.2 版本</strong>，带来了多项功能优化与系统修复，进一步提升了稳定性与用户体验。本次版本将底层组件全面升级至最新补丁版本，包括 <strong>Rancher v2.11.3</strong>、<strong>RKE2 v1.32.7+rke2r1</strong> 与 <strong>KubeVirt v1.4.1</strong>，从而增强虚拟化性能与资源管理能力。同时，系统在高可用性与升级流程方面也进行了改进，确保集群运行更流畅可靠。</p>
<p>在修复部分，v1.5.2 解决了多项关键问题，如 RWX 卷竞争条件、ShareManager 异常终止、EFI 模式下 PXE 启动失败以及版本同步器崩溃等问题。此外，还修复了节点磁盘使用多路径（mpath）时的升级卡顿问题，并改善了 Rancher 集成时的 UI 兼容性。</p>
<p>详细内容请参阅：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fharvester%2Fharvester%2Freleases%2Ftag%2Fv1.5.2" target="_blank">Harvester v1.5.2 发布说明</a></p>
<h2>RKE2</h2>
<p>在本次更新中，RKE2 各稳定分支均完成了例行版本升级，分别对应最新的 Kubernetes 版本：v1.31.13、v1.32.9、v1.33.5、v1.34.1。</p>
<p>目前，RKE2 stable 版本为 <strong>v1.33.5+rke2r1</strong>，latest 版本为 <strong>v1.34.1+rke2r1</strong>。</p>
<p>此外，还同步更新了 containerd、runc、etcd、CoreDNS、metrics-server 等核心组件，CNI 插件（Calico、Cilium）与 ingress-nginx 也获得了新版本支持。此次更新同时提升了对 vSphere 环境的兼容性，并引入了 Go 语言版本更新，以进一步增强系统的稳定性与性能。</p>
<p>详细更新内容可查看以下发布说明：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.31.13%252Brke2r1" target="_blank">RKE2 v1.31.13+rke2r1 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.32.9%252Brke2r1" target="_blank">RKE2 v1.32.9+rke2r1 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.33.5%252Brke2r1" target="_blank">RKE2 v1.33.5+rke2r1 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.34.1%252Brke2r1" target="_blank">RKE2 v1.34.1+rke2r1 发布说明</a></li>
</ul>
<h2>K3S</h2>
<p>在本次更新中，<strong>K3s 发布了 v1.31.13+k3s1、v1.32.9+k3s1、v1.33.5+k3s1 和 v1.34.1+k3s1</strong> 四个版本，其中 <strong>stable 版本为 v1.33.5+k3s1</strong>，<strong>latest 版本为 v1.34.1+k3s1</strong>。这些版本同步更新至对应的 Kubernetes 版本，集中修复了多项组件问题，并优化了 etcd、containerd、runc 等关键模块的性能与可靠性。</p>
<p>更新亮点包括：改进 <strong>etcd 节点加入与超时机制</strong>、优化 <strong>Spegel 日志与启动流程</strong>、支持通过 <code>--debug</code> 统一控制 cri-dockerd 日志等级、增强 <strong>CRD 创建冲突的重试机制</strong>，并新增 <strong>kine 与 remotedialer 指标采集</strong> 功能。特别是 v1.34.1 版本引入了基于 <strong>Buildroot 2025.02 LTS</strong> 的用户态二进制包，并支持 <strong>nft json 输出格式</strong>，提升了与 kube-proxy 的兼容性。</p>
<p>详细更新内容请参阅以下发布说明：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.31.13%2Bk3s1" target="_blank">K3s v1.31.13 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.32.9%2Bk3s1" target="_blank">K3s v1.32.9 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.33.5%2Bk3s1" target="_blank">K3s v1.33.5 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.34.1%2Bk3s1" target="_blank">K3s v1.34.1 发布说明</a></li>
</ul>
<h2>K3K</h2>
<p><strong>K3k v0.3.5</strong> 发布，本次更新进一步完善了虚拟集群同步与资源管理逻辑，显著提升了系统稳定性与易用性。版本中新增了 <strong>资源同步配置（Resource Sync Configuration）</strong>，允许用户更灵活地控制虚拟集群与宿主集群间的同步范围；同时增强了 <strong>PVC 同步机制</strong>，修复了默认值处理异常问题。此外，还新增了 <strong>镜像拉取凭证（ImagePullSecrets）</strong> 支持，使控制器、Server 与 Agent 的镜像管理更加安全可靠。</p>
<p>开发与测试方面，K3k v0.3.5 引入了 <strong>控制器覆盖率统计</strong>、<strong>单实例暴露模式验证</strong> 以及测试拆分优化，提升了代码可维护性与持续集成质量。</p>
<p>详细更新内容请参阅：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Fk3k%2Freleases%2Ftag%2Fv0.3.5" target="_blank">K3k v0.3.5 发布说明</a></li>
</ul>
<h2>写在最后</h2>
<p>通过本期更新，我们可以看到 Rancher 在多产品线间保持着稳定且高频的版本节奏，不仅持续强化底层技术栈的安全性与性能，也在不断优化用户体验与生态协同。 未来，Rancher 社区将继续聚焦开源创新，推动 Kubernetes 与云原生技术的普惠化与场景化应用，助力更多企业构建更高效、更可信赖的基础设施。</p>
<p>敬请关注 <strong>Rancher 官方微信公众号</strong>，获取最新版本动态、实践案例与技术分享。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d7043d10d7.png"></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>在本期 <strong>Rancher 社区双周报</strong> 中，我们为大家带来了多个核心产品的最新版本动态： <strong>Longhorn</strong> 发布了 v1.9.2 与 v1.10.0 两个版本，其中 v1.10.0 引入了 V2 Data Engine 的重大增强，带来更高性能与更强扩展性； <strong>Rancher</strong> 发布了四个版本（v2.9.12、v2.10.10、v2.11.6、v2.12.2），其中多个 Prime 版本聚焦于安全修复与系统稳健性提升； <strong>RKE2 与 K3s</strong> 分支均完成了 Kubernetes 版本的例行更新，优化核心组件并强化集群可靠性； 同时，<strong>Harvester v1.5.2</strong> 带来了更流畅的虚拟化体验，<strong>K3k v0.3.5</strong> 则在资源同步与镜像管理方面持续进化。</p>
<p>这一系列更新共同展现了 Rancher 技术生态的持续完善与活力，为用户在容器、虚拟化与边缘计算场景中的落地提供了更加坚实的基础。</p>
<h2>Longhorn</h2>
<p>Longhorn 发布了 <strong>v1.9.2</strong> 与 <strong>v1.10.0</strong> 两个版本更新。本次更新聚焦于系统稳定性与性能增强，同时引入了多项新特性与兼容性优化，进一步提升了 Longhorn 在企业级云原生存储场景中的可靠性与易用性。</p>
<h3>Longhorn v1.9.2</h3>
<p>Longhorn v1.9.2 版本主要聚焦于系统稳定性和可靠性提升，修复了多个潜在问题，包括卷扩容时可能出现的数据损坏、磁盘空间不足引起的卷异常、以及备份目标不可用导致的恢复失败等。该版本还改进了探针配置、日志输出及备份超时机制，优化了支持包的收集范围，进一步提升了系统的可维护性和故障排查效率。</p>
<h3>Longhorn v1.10.0</h3>
<p>Longhorn v1.10.0 是一次重要的功能更新，引入了对 <strong>V2 Data Engine</strong> 的全面增强，在性能、资源占用和可扩展性方面均有显著提升。 该版本在存储引擎层面增加了中断模式以降低 CPU 消耗，支持卷克隆、在线扩容、QoS 限速等关键能力，为多租户与高负载场景提供了更高的灵活性和性能保障。</p>
<p>此外，Longhorn v1.10.0 进一步完善了网络与存储调度支持，新增 <strong>IPv6 网络兼容</strong> 与 <strong>CSIStorageCapacity</strong> 调度优化机制；在数据保护方面，引入了可配置的备份块大小与更高效的备份清理逻辑。 同时，Longhorn 统一了配置文件格式，移除了旧版 v1beta1 API，并对 UI 界面与监控指标体系进行了重构，使系统的可观测性、操作体验与上层产品（如 Harvester）的集成能力显著增强。</p>
<p>如需详细了解各版本的更新内容，请查阅：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Flonghorn%2Flonghorn%2Freleases%2Ftag%2Fv1.9.2" target="_blank">Longhorn v1.9.2 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Flonghorn%2Flonghorn%2Freleases%2Ftag%2Fv1.10.0" target="_blank">Longhorn v1.10.0 发布说明</a></li>
</ul>
<h2>Rancher</h2>
<p>近期，Rancher 团队同步发布了多个补丁版本：<strong>v2.9.12、v2.10.10、v2.11.6 与 v2.12.2</strong>。其中，<strong>v2.9.12、v2.10.10 和 v2.11.6 为 Prime 版本</strong>，主要面向企业用户提供安全加固与维护修复；<strong>v2.12.2 则为 Community 与 Prime 双版本同步更新</strong>，覆盖了开源与商业发行渠道。</p>
<p>本次更新的核心聚焦于 <strong>安全漏洞修复与系统稳健性提升</strong>，共涉及以下三个安全问题：</p>
<h3>安全修复重点</h3>
<ul>
<li> <p><strong>用户名唯一性限制（CVE-2024-58260）</strong> 之前版本中，若用户 A 和 B 拥有相同用户名，可能导致任一用户无法登录，甚至可被恶意利用阻止管理员登录。 新版本中，用户名一旦设置将不可修改，且禁止创建重复用户名，从根本上防止该问题发生。</p> </li>
<li> <p><strong>CLI 登录安全增强（CVE-2024-58267）</strong> Rancher CLI 现已在登录过程中更明显地显示 <code>requestId</code>，并在请求中添加 <code>cli=true</code> 标识。 Rancher Dashboard 会根据该标识识别请求来源并提示用户验证操作，从而防止伪造登录请求。</p> </li>
<li> <p><strong>敏感头字段清理（CVE-2025-54468）</strong> Rancher 现已移除 <code>/meta/proxy</code> 接口中的 <code>Impersonate-*</code> 请求头，避免在创建云凭证等场景中泄露可识别或敏感信息。</p> </li>
</ul>
<p>这些修复已全部在 <strong>v2.9.12、v2.10.10、v2.11.6 与 v2.12.2</strong> 中生效，显著提升了 Rancher 在身份验证与数据安全方面的防护能力。</p>
<p>如需详细了解各版本发布内容，可参阅：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.9.12" target="_blank">Rancher v2.9.12 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.10.10" target="_blank">Rancher v2.10.10 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.11.6" target="_blank">Rancher v2.11.6 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.12.2" target="_blank">Rancher v2.12.2 发布说明</a></li>
</ul>
<h2>Harvester</h2>
<p>在本次更新中，<strong>Harvester 发布了 v1.5.2 版本</strong>，带来了多项功能优化与系统修复，进一步提升了稳定性与用户体验。本次版本将底层组件全面升级至最新补丁版本，包括 <strong>Rancher v2.11.3</strong>、<strong>RKE2 v1.32.7+rke2r1</strong> 与 <strong>KubeVirt v1.4.1</strong>，从而增强虚拟化性能与资源管理能力。同时，系统在高可用性与升级流程方面也进行了改进，确保集群运行更流畅可靠。</p>
<p>在修复部分，v1.5.2 解决了多项关键问题，如 RWX 卷竞争条件、ShareManager 异常终止、EFI 模式下 PXE 启动失败以及版本同步器崩溃等问题。此外，还修复了节点磁盘使用多路径（mpath）时的升级卡顿问题，并改善了 Rancher 集成时的 UI 兼容性。</p>
<p>详细内容请参阅：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fharvester%2Fharvester%2Freleases%2Ftag%2Fv1.5.2" target="_blank">Harvester v1.5.2 发布说明</a></p>
<h2>RKE2</h2>
<p>在本次更新中，RKE2 各稳定分支均完成了例行版本升级，分别对应最新的 Kubernetes 版本：v1.31.13、v1.32.9、v1.33.5、v1.34.1。</p>
<p>目前，RKE2 stable 版本为 <strong>v1.33.5+rke2r1</strong>，latest 版本为 <strong>v1.34.1+rke2r1</strong>。</p>
<p>此外，还同步更新了 containerd、runc、etcd、CoreDNS、metrics-server 等核心组件，CNI 插件（Calico、Cilium）与 ingress-nginx 也获得了新版本支持。此次更新同时提升了对 vSphere 环境的兼容性，并引入了 Go 语言版本更新，以进一步增强系统的稳定性与性能。</p>
<p>详细更新内容可查看以下发布说明：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.31.13%252Brke2r1" target="_blank">RKE2 v1.31.13+rke2r1 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.32.9%252Brke2r1" target="_blank">RKE2 v1.32.9+rke2r1 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.33.5%252Brke2r1" target="_blank">RKE2 v1.33.5+rke2r1 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.34.1%252Brke2r1" target="_blank">RKE2 v1.34.1+rke2r1 发布说明</a></li>
</ul>
<h2>K3S</h2>
<p>在本次更新中，<strong>K3s 发布了 v1.31.13+k3s1、v1.32.9+k3s1、v1.33.5+k3s1 和 v1.34.1+k3s1</strong> 四个版本，其中 <strong>stable 版本为 v1.33.5+k3s1</strong>，<strong>latest 版本为 v1.34.1+k3s1</strong>。这些版本同步更新至对应的 Kubernetes 版本，集中修复了多项组件问题，并优化了 etcd、containerd、runc 等关键模块的性能与可靠性。</p>
<p>更新亮点包括：改进 <strong>etcd 节点加入与超时机制</strong>、优化 <strong>Spegel 日志与启动流程</strong>、支持通过 <code>--debug</code> 统一控制 cri-dockerd 日志等级、增强 <strong>CRD 创建冲突的重试机制</strong>，并新增 <strong>kine 与 remotedialer 指标采集</strong> 功能。特别是 v1.34.1 版本引入了基于 <strong>Buildroot 2025.02 LTS</strong> 的用户态二进制包，并支持 <strong>nft json 输出格式</strong>，提升了与 kube-proxy 的兼容性。</p>
<p>详细更新内容请参阅以下发布说明：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.31.13%2Bk3s1" target="_blank">K3s v1.31.13 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.32.9%2Bk3s1" target="_blank">K3s v1.32.9 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.33.5%2Bk3s1" target="_blank">K3s v1.33.5 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.34.1%2Bk3s1" target="_blank">K3s v1.34.1 发布说明</a></li>
</ul>
<h2>K3K</h2>
<p><strong>K3k v0.3.5</strong> 发布，本次更新进一步完善了虚拟集群同步与资源管理逻辑，显著提升了系统稳定性与易用性。版本中新增了 <strong>资源同步配置（Resource Sync Configuration）</strong>，允许用户更灵活地控制虚拟集群与宿主集群间的同步范围；同时增强了 <strong>PVC 同步机制</strong>，修复了默认值处理异常问题。此外，还新增了 <strong>镜像拉取凭证（ImagePullSecrets）</strong> 支持，使控制器、Server 与 Agent 的镜像管理更加安全可靠。</p>
<p>开发与测试方面，K3k v0.3.5 引入了 <strong>控制器覆盖率统计</strong>、<strong>单实例暴露模式验证</strong> 以及测试拆分优化，提升了代码可维护性与持续集成质量。</p>
<p>详细更新内容请参阅：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Fk3k%2Freleases%2Ftag%2Fv0.3.5" target="_blank">K3k v0.3.5 发布说明</a></li>
</ul>
<h2>写在最后</h2>
<p>通过本期更新，我们可以看到 Rancher 在多产品线间保持着稳定且高频的版本节奏，不仅持续强化底层技术栈的安全性与性能，也在不断优化用户体验与生态协同。 未来，Rancher 社区将继续聚焦开源创新，推动 Kubernetes 与云原生技术的普惠化与场景化应用，助力更多企业构建更高效、更可信赖的基础设施。</p>
<p>敬请关注 <strong>Rancher 官方微信公众号</strong>，获取最新版本动态、实践案例与技术分享。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d7043d10d7.png"></p>]]>
    </description>
    <content:encoded><![CDATA[<p>在本期 <strong>Rancher 社区双周报</strong> 中，我们为大家带来了多个核心产品的最新版本动态： <strong>Longhorn</strong> 发布了 v1.9.2 与 v1.10.0 两个版本，其中 v1.10.0 引入了 V2 Data Engine 的重大增强，带来更高性能与更强扩展性； <strong>Rancher</strong> 发布了四个版本（v2.9.12、v2.10.10、v2.11.6、v2.12.2），其中多个 Prime 版本聚焦于安全修复与系统稳健性提升； <strong>RKE2 与 K3s</strong> 分支均完成了 Kubernetes 版本的例行更新，优化核心组件并强化集群可靠性； 同时，<strong>Harvester v1.5.2</strong> 带来了更流畅的虚拟化体验，<strong>K3k v0.3.5</strong> 则在资源同步与镜像管理方面持续进化。</p>
<p>这一系列更新共同展现了 Rancher 技术生态的持续完善与活力，为用户在容器、虚拟化与边缘计算场景中的落地提供了更加坚实的基础。</p>
<h2>Longhorn</h2>
<p>Longhorn 发布了 <strong>v1.9.2</strong> 与 <strong>v1.10.0</strong> 两个版本更新。本次更新聚焦于系统稳定性与性能增强，同时引入了多项新特性与兼容性优化，进一步提升了 Longhorn 在企业级云原生存储场景中的可靠性与易用性。</p>
<h3>Longhorn v1.9.2</h3>
<p>Longhorn v1.9.2 版本主要聚焦于系统稳定性和可靠性提升，修复了多个潜在问题，包括卷扩容时可能出现的数据损坏、磁盘空间不足引起的卷异常、以及备份目标不可用导致的恢复失败等。该版本还改进了探针配置、日志输出及备份超时机制，优化了支持包的收集范围，进一步提升了系统的可维护性和故障排查效率。</p>
<h3>Longhorn v1.10.0</h3>
<p>Longhorn v1.10.0 是一次重要的功能更新，引入了对 <strong>V2 Data Engine</strong> 的全面增强，在性能、资源占用和可扩展性方面均有显著提升。 该版本在存储引擎层面增加了中断模式以降低 CPU 消耗，支持卷克隆、在线扩容、QoS 限速等关键能力，为多租户与高负载场景提供了更高的灵活性和性能保障。</p>
<p>此外，Longhorn v1.10.0 进一步完善了网络与存储调度支持，新增 <strong>IPv6 网络兼容</strong> 与 <strong>CSIStorageCapacity</strong> 调度优化机制；在数据保护方面，引入了可配置的备份块大小与更高效的备份清理逻辑。 同时，Longhorn 统一了配置文件格式，移除了旧版 v1beta1 API，并对 UI 界面与监控指标体系进行了重构，使系统的可观测性、操作体验与上层产品（如 Harvester）的集成能力显著增强。</p>
<p>如需详细了解各版本的更新内容，请查阅：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Flonghorn%2Flonghorn%2Freleases%2Ftag%2Fv1.9.2" target="_blank">Longhorn v1.9.2 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Flonghorn%2Flonghorn%2Freleases%2Ftag%2Fv1.10.0" target="_blank">Longhorn v1.10.0 发布说明</a></li>
</ul>
<h2>Rancher</h2>
<p>近期，Rancher 团队同步发布了多个补丁版本：<strong>v2.9.12、v2.10.10、v2.11.6 与 v2.12.2</strong>。其中，<strong>v2.9.12、v2.10.10 和 v2.11.6 为 Prime 版本</strong>，主要面向企业用户提供安全加固与维护修复；<strong>v2.12.2 则为 Community 与 Prime 双版本同步更新</strong>，覆盖了开源与商业发行渠道。</p>
<p>本次更新的核心聚焦于 <strong>安全漏洞修复与系统稳健性提升</strong>，共涉及以下三个安全问题：</p>
<h3>安全修复重点</h3>
<ul>
<li> <p><strong>用户名唯一性限制（CVE-2024-58260）</strong> 之前版本中，若用户 A 和 B 拥有相同用户名，可能导致任一用户无法登录，甚至可被恶意利用阻止管理员登录。 新版本中，用户名一旦设置将不可修改，且禁止创建重复用户名，从根本上防止该问题发生。</p> </li>
<li> <p><strong>CLI 登录安全增强（CVE-2024-58267）</strong> Rancher CLI 现已在登录过程中更明显地显示 <code>requestId</code>，并在请求中添加 <code>cli=true</code> 标识。 Rancher Dashboard 会根据该标识识别请求来源并提示用户验证操作，从而防止伪造登录请求。</p> </li>
<li> <p><strong>敏感头字段清理（CVE-2025-54468）</strong> Rancher 现已移除 <code>/meta/proxy</code> 接口中的 <code>Impersonate-*</code> 请求头，避免在创建云凭证等场景中泄露可识别或敏感信息。</p> </li>
</ul>
<p>这些修复已全部在 <strong>v2.9.12、v2.10.10、v2.11.6 与 v2.12.2</strong> 中生效，显著提升了 Rancher 在身份验证与数据安全方面的防护能力。</p>
<p>如需详细了解各版本发布内容，可参阅：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.9.12" target="_blank">Rancher v2.9.12 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.10.10" target="_blank">Rancher v2.10.10 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.11.6" target="_blank">Rancher v2.11.6 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Francher%2Freleases%2Ftag%2Fv2.12.2" target="_blank">Rancher v2.12.2 发布说明</a></li>
</ul>
<h2>Harvester</h2>
<p>在本次更新中，<strong>Harvester 发布了 v1.5.2 版本</strong>，带来了多项功能优化与系统修复，进一步提升了稳定性与用户体验。本次版本将底层组件全面升级至最新补丁版本，包括 <strong>Rancher v2.11.3</strong>、<strong>RKE2 v1.32.7+rke2r1</strong> 与 <strong>KubeVirt v1.4.1</strong>，从而增强虚拟化性能与资源管理能力。同时，系统在高可用性与升级流程方面也进行了改进，确保集群运行更流畅可靠。</p>
<p>在修复部分，v1.5.2 解决了多项关键问题，如 RWX 卷竞争条件、ShareManager 异常终止、EFI 模式下 PXE 启动失败以及版本同步器崩溃等问题。此外，还修复了节点磁盘使用多路径（mpath）时的升级卡顿问题，并改善了 Rancher 集成时的 UI 兼容性。</p>
<p>详细内容请参阅：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fharvester%2Fharvester%2Freleases%2Ftag%2Fv1.5.2" target="_blank">Harvester v1.5.2 发布说明</a></p>
<h2>RKE2</h2>
<p>在本次更新中，RKE2 各稳定分支均完成了例行版本升级，分别对应最新的 Kubernetes 版本：v1.31.13、v1.32.9、v1.33.5、v1.34.1。</p>
<p>目前，RKE2 stable 版本为 <strong>v1.33.5+rke2r1</strong>，latest 版本为 <strong>v1.34.1+rke2r1</strong>。</p>
<p>此外，还同步更新了 containerd、runc、etcd、CoreDNS、metrics-server 等核心组件，CNI 插件（Calico、Cilium）与 ingress-nginx 也获得了新版本支持。此次更新同时提升了对 vSphere 环境的兼容性，并引入了 Go 语言版本更新，以进一步增强系统的稳定性与性能。</p>
<p>详细更新内容可查看以下发布说明：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.31.13%252Brke2r1" target="_blank">RKE2 v1.31.13+rke2r1 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.32.9%252Brke2r1" target="_blank">RKE2 v1.32.9+rke2r1 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.33.5%252Brke2r1" target="_blank">RKE2 v1.33.5+rke2r1 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Frke2%2Freleases%2Ftag%2Fv1.34.1%252Brke2r1" target="_blank">RKE2 v1.34.1+rke2r1 发布说明</a></li>
</ul>
<h2>K3S</h2>
<p>在本次更新中，<strong>K3s 发布了 v1.31.13+k3s1、v1.32.9+k3s1、v1.33.5+k3s1 和 v1.34.1+k3s1</strong> 四个版本，其中 <strong>stable 版本为 v1.33.5+k3s1</strong>，<strong>latest 版本为 v1.34.1+k3s1</strong>。这些版本同步更新至对应的 Kubernetes 版本，集中修复了多项组件问题，并优化了 etcd、containerd、runc 等关键模块的性能与可靠性。</p>
<p>更新亮点包括：改进 <strong>etcd 节点加入与超时机制</strong>、优化 <strong>Spegel 日志与启动流程</strong>、支持通过 <code>--debug</code> 统一控制 cri-dockerd 日志等级、增强 <strong>CRD 创建冲突的重试机制</strong>，并新增 <strong>kine 与 remotedialer 指标采集</strong> 功能。特别是 v1.34.1 版本引入了基于 <strong>Buildroot 2025.02 LTS</strong> 的用户态二进制包，并支持 <strong>nft json 输出格式</strong>，提升了与 kube-proxy 的兼容性。</p>
<p>详细更新内容请参阅以下发布说明：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.31.13%2Bk3s1" target="_blank">K3s v1.31.13 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.32.9%2Bk3s1" target="_blank">K3s v1.32.9 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.33.5%2Bk3s1" target="_blank">K3s v1.33.5 发布说明</a></li>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fk3s-io%2Fk3s%2Freleases%2Ftag%2Fv1.34.1%2Bk3s1" target="_blank">K3s v1.34.1 发布说明</a></li>
</ul>
<h2>K3K</h2>
<p><strong>K3k v0.3.5</strong> 发布，本次更新进一步完善了虚拟集群同步与资源管理逻辑，显著提升了系统稳定性与易用性。版本中新增了 <strong>资源同步配置（Resource Sync Configuration）</strong>，允许用户更灵活地控制虚拟集群与宿主集群间的同步范围；同时增强了 <strong>PVC 同步机制</strong>，修复了默认值处理异常问题。此外，还新增了 <strong>镜像拉取凭证（ImagePullSecrets）</strong> 支持，使控制器、Server 与 Agent 的镜像管理更加安全可靠。</p>
<p>开发与测试方面，K3k v0.3.5 引入了 <strong>控制器覆盖率统计</strong>、<strong>单实例暴露模式验证</strong> 以及测试拆分优化，提升了代码可维护性与持续集成质量。</p>
<p>详细更新内容请参阅：</p>
<ul>
<li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Francher%2Fk3k%2Freleases%2Ftag%2Fv0.3.5" target="_blank">K3k v0.3.5 发布说明</a></li>
</ul>
<h2>写在最后</h2>
<p>通过本期更新，我们可以看到 Rancher 在多产品线间保持着稳定且高频的版本节奏，不仅持续强化底层技术栈的安全性与性能，也在不断优化用户体验与生态协同。 未来，Rancher 社区将继续聚焦开源创新，推动 Kubernetes 与云原生技术的普惠化与场景化应用，助力更多企业构建更高效、更可信赖的基础设施。</p>
<p>敬请关注 <strong>Rancher 官方微信公众号</strong>，获取最新版本动态、实践案例与技术分享。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d7043d10d7.png"></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d7043d10d7.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d7043d10d7.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 10:49:33 +0800</pubDate>
  </item><item>
    <title><![CDATA[OpenAI 与博通达成战略合作，开发定制 AI 芯片]]></title>
    <link>https://www.oschina.net/news/377300</link>
    <itunes:title><![CDATA[OpenAI 与博通达成战略合作，开发定制 AI 芯片]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>OpenAI<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fopenai-and-broadcom-announce-strategic-collaboration%2F" target="_blank">官宣</a>与博通（Broadcom）建立战略协作，双方将联合开发定制AI芯片并部署规模达10吉瓦的推理基础设施，计划于2026至2029年分阶段落地，标志着OpenAI向AI基础设施自主化迈出关键一步。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8a0590e31.png"></p>
<p><span><strong>此次合作突破传统“采购芯片”模式，由OpenAI主导芯片设计</strong></span>，注入模型算法洞见，博通则负责底层架构协作、硬件集成及网络方案支撑，涵盖芯片制造、网络互联至数据中心优化全链路。其定制芯片针对大型语言模型负载优化，结合博通硅光子学技术，可降低15-20%功耗并减少30%以上数据传输瓶颈。</p>
<p>OpenAI CEO Sam Altman表示，此举旨在构建从硬件到服务的全链条系统，破解算力短缺难题。该合作预计将大幅缩短万亿参数模型训练周期，为全球数十亿用户的智能助理规模化落地提供算力基座，同时推动AI硬件从通用向专用化转型。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>OpenAI<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fopenai-and-broadcom-announce-strategic-collaboration%2F" target="_blank">官宣</a>与博通（Broadcom）建立战略协作，双方将联合开发定制AI芯片并部署规模达10吉瓦的推理基础设施，计划于2026至2029年分阶段落地，标志着OpenAI向AI基础设施自主化迈出关键一步。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8a0590e31.png"></p>
<p><span><strong>此次合作突破传统“采购芯片”模式，由OpenAI主导芯片设计</strong></span>，注入模型算法洞见，博通则负责底层架构协作、硬件集成及网络方案支撑，涵盖芯片制造、网络互联至数据中心优化全链路。其定制芯片针对大型语言模型负载优化，结合博通硅光子学技术，可降低15-20%功耗并减少30%以上数据传输瓶颈。</p>
<p>OpenAI CEO Sam Altman表示，此举旨在构建从硬件到服务的全链条系统，破解算力短缺难题。该合作预计将大幅缩短万亿参数模型训练周期，为全球数十亿用户的智能助理规模化落地提供算力基座，同时推动AI硬件从通用向专用化转型。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>OpenAI<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Findex%2Fopenai-and-broadcom-announce-strategic-collaboration%2F" target="_blank">官宣</a>与博通（Broadcom）建立战略协作，双方将联合开发定制AI芯片并部署规模达10吉瓦的推理基础设施，计划于2026至2029年分阶段落地，标志着OpenAI向AI基础设施自主化迈出关键一步。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8a0590e31.png"></p>
<p><span><strong>此次合作突破传统“采购芯片”模式，由OpenAI主导芯片设计</strong></span>，注入模型算法洞见，博通则负责底层架构协作、硬件集成及网络方案支撑，涵盖芯片制造、网络互联至数据中心优化全链路。其定制芯片针对大型语言模型负载优化，结合博通硅光子学技术，可降低15-20%功耗并减少30%以上数据传输瓶颈。</p>
<p>OpenAI CEO Sam Altman表示，此举旨在构建从硬件到服务的全链条系统，破解算力短缺难题。该合作预计将大幅缩短万亿参数模型训练周期，为全球数十亿用户的智能助理规模化落地提供算力基座，同时推动AI硬件从通用向专用化转型。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8a0590e31.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d8a0590e31.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 10:47:27 +0800</pubDate>
  </item><item>
    <title><![CDATA[Andrej Kaparthy 开源 nanochat：从零开始的极简全栈训练/推理方案]]></title>
    <link>https://www.oschina.net/news/377299</link>
    <itunes:title><![CDATA[Andrej Kaparthy 开源 nanochat：从零开始的极简全栈训练/推理方案]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>“Vibe Coding” 概念提出者 Andrej Karpathy 刚刚发布了名为「nanochat」的开源项目：</p>
<blockquote>
<p>nanochat 是类似于 ChatGPT 的 LLM 全栈实现，代码库单一、简洁、最小化、可定制且依赖极少。</p>
<p>nanochat 设计用于在单个 8XH100 节点上通过如 speedrun.sh 之类的脚本运行，实现从开始到结束的完整管道处理。这包括分词、预训练、微调、评估、推理以及通过简单的 UI 进行网络服务，这样你就可以像与 ChatGPT 交谈一样与自己的 LLM 交谈。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-f900633274.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7eef89fe31.png"></p>
<p><em>https://github.com/karpathy/nanochat</em></p>
</blockquote>
<p>下面内容来自&nbsp;Andrej Kaparthy 的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fkarpathy%2Fstatus%2F1977755427569111362" target="_blank">推特</a>：</p>
<blockquote>
<p>与我之前的项目 nanoGPT 不同——后者只涉及预训练部分——nanochat 是一个极简、从零构建的、完整的 ChatGPT 克隆版训练与推理全栈流程。整个系统被封装在一个几乎没有依赖的代码库中。</p>
<p>你只需要启动一台云端 GPU 服务器，运行一个脚本，大约 4 小时后，你就能在 ChatGPT 风格的网页界面上与自己的 LLM 对话。</p>
<p>整个项目约 8000 行代码，结构清晰，实现了以下功能：</p>
<ul>
<li>&nbsp;使用全新的 Rust 实现训练分词器</li>
<li>&nbsp;在 FineWeb 上预训练 Transformer LLM，并在多个指标上评估 CORE 分数</li>
<li>在 SmolTalk 的用户-助手对话、多选题、工具使用等数据上进行中期训练（Midtrain）</li>
<li>进行 SFT（有监督微调），并在常识（ARC-E/C、MMLU）、数学（GSM8K）、代码（HumanEval）任务上评估模型</li>
<li>可选地在 GSM8K 上使用 “GRPO” 算法进行强化学习（RL）</li>
<li>在推理引擎中实现高效推理：支持 KV cache、prefill/decode、工具调用（在轻量沙箱中运行 Python 解释器），可通过 CLI 或 ChatGPT 风格的 WebUI 与之交互</li>
<li>自动生成一份 Markdown “成绩单”，总结并游戏化整个流程</li>
</ul>
<p>即使仅花费约 100 美元（约 4 小时在 8×H100 节点上运行），你也能训练出一个“小型 ChatGPT 克隆”，能写故事、写诗、回答简单问题。</p>
<p>训练约 12 小时 后，它在 CORE 指标上就能超越 GPT-2。当预算提升至约 1000 美元（约 41.6 小时训练）时，模型的连贯性显著增强，能够解决简单的数学与代码问题，并通过多项选择测试。</p>
<p>例如，一个 深度 30 的模型，训练 24 小时（约等于 GPT-3 Small 125M 的 FLOPs，约为 GPT-3 的 1/1000），即可在 MMLU 上取得 40+ 分，在 ARC-Easy 上达 70+，在 GSM8K 上达 20+，等等。</p>
<p>我的目标，是将一个完整的 “强基线（strong baseline）” 堆栈整合进一个紧凑、可读、可改、可复刻的仓库中。</p>
<p>nanochat 将成为我课程 LLM101n 的收官项目（仍在开发中）。</p>
<p>我认为它也有潜力像 nanoGPT 一样，成长为一个研究用工具或基准平台。</p>
<p>目前它还远未“完成”、调优或优化（我相信仍有不少低垂果实可摘），但现在的框架已经足够稳定，足以放上 GitHub，让大家在此基础上改进各个部分。</p>
</blockquote>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>“Vibe Coding” 概念提出者 Andrej Karpathy 刚刚发布了名为「nanochat」的开源项目：</p>
<blockquote>
<p>nanochat 是类似于 ChatGPT 的 LLM 全栈实现，代码库单一、简洁、最小化、可定制且依赖极少。</p>
<p>nanochat 设计用于在单个 8XH100 节点上通过如 speedrun.sh 之类的脚本运行，实现从开始到结束的完整管道处理。这包括分词、预训练、微调、评估、推理以及通过简单的 UI 进行网络服务，这样你就可以像与 ChatGPT 交谈一样与自己的 LLM 交谈。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-f900633274.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7eef89fe31.png"></p>
<p><em>https://github.com/karpathy/nanochat</em></p>
</blockquote>
<p>下面内容来自&nbsp;Andrej Kaparthy 的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fkarpathy%2Fstatus%2F1977755427569111362" target="_blank">推特</a>：</p>
<blockquote>
<p>与我之前的项目 nanoGPT 不同——后者只涉及预训练部分——nanochat 是一个极简、从零构建的、完整的 ChatGPT 克隆版训练与推理全栈流程。整个系统被封装在一个几乎没有依赖的代码库中。</p>
<p>你只需要启动一台云端 GPU 服务器，运行一个脚本，大约 4 小时后，你就能在 ChatGPT 风格的网页界面上与自己的 LLM 对话。</p>
<p>整个项目约 8000 行代码，结构清晰，实现了以下功能：</p>
<ul>
<li>&nbsp;使用全新的 Rust 实现训练分词器</li>
<li>&nbsp;在 FineWeb 上预训练 Transformer LLM，并在多个指标上评估 CORE 分数</li>
<li>在 SmolTalk 的用户-助手对话、多选题、工具使用等数据上进行中期训练（Midtrain）</li>
<li>进行 SFT（有监督微调），并在常识（ARC-E/C、MMLU）、数学（GSM8K）、代码（HumanEval）任务上评估模型</li>
<li>可选地在 GSM8K 上使用 “GRPO” 算法进行强化学习（RL）</li>
<li>在推理引擎中实现高效推理：支持 KV cache、prefill/decode、工具调用（在轻量沙箱中运行 Python 解释器），可通过 CLI 或 ChatGPT 风格的 WebUI 与之交互</li>
<li>自动生成一份 Markdown “成绩单”，总结并游戏化整个流程</li>
</ul>
<p>即使仅花费约 100 美元（约 4 小时在 8×H100 节点上运行），你也能训练出一个“小型 ChatGPT 克隆”，能写故事、写诗、回答简单问题。</p>
<p>训练约 12 小时 后，它在 CORE 指标上就能超越 GPT-2。当预算提升至约 1000 美元（约 41.6 小时训练）时，模型的连贯性显著增强，能够解决简单的数学与代码问题，并通过多项选择测试。</p>
<p>例如，一个 深度 30 的模型，训练 24 小时（约等于 GPT-3 Small 125M 的 FLOPs，约为 GPT-3 的 1/1000），即可在 MMLU 上取得 40+ 分，在 ARC-Easy 上达 70+，在 GSM8K 上达 20+，等等。</p>
<p>我的目标，是将一个完整的 “强基线（strong baseline）” 堆栈整合进一个紧凑、可读、可改、可复刻的仓库中。</p>
<p>nanochat 将成为我课程 LLM101n 的收官项目（仍在开发中）。</p>
<p>我认为它也有潜力像 nanoGPT 一样，成长为一个研究用工具或基准平台。</p>
<p>目前它还远未“完成”、调优或优化（我相信仍有不少低垂果实可摘），但现在的框架已经足够稳定，足以放上 GitHub，让大家在此基础上改进各个部分。</p>
</blockquote>]]>
    </description>
    <content:encoded><![CDATA[<p>“Vibe Coding” 概念提出者 Andrej Karpathy 刚刚发布了名为「nanochat」的开源项目：</p>
<blockquote>
<p>nanochat 是类似于 ChatGPT 的 LLM 全栈实现，代码库单一、简洁、最小化、可定制且依赖极少。</p>
<p>nanochat 设计用于在单个 8XH100 节点上通过如 speedrun.sh 之类的脚本运行，实现从开始到结束的完整管道处理。这包括分词、预训练、微调、评估、推理以及通过简单的 UI 进行网络服务，这样你就可以像与 ChatGPT 交谈一样与自己的 LLM 交谈。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-f900633274.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-7eef89fe31.png"></p>
<p><em>https://github.com/karpathy/nanochat</em></p>
</blockquote>
<p>下面内容来自&nbsp;Andrej Kaparthy 的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Fkarpathy%2Fstatus%2F1977755427569111362" target="_blank">推特</a>：</p>
<blockquote>
<p>与我之前的项目 nanoGPT 不同——后者只涉及预训练部分——nanochat 是一个极简、从零构建的、完整的 ChatGPT 克隆版训练与推理全栈流程。整个系统被封装在一个几乎没有依赖的代码库中。</p>
<p>你只需要启动一台云端 GPU 服务器，运行一个脚本，大约 4 小时后，你就能在 ChatGPT 风格的网页界面上与自己的 LLM 对话。</p>
<p>整个项目约 8000 行代码，结构清晰，实现了以下功能：</p>
<ul>
<li>&nbsp;使用全新的 Rust 实现训练分词器</li>
<li>&nbsp;在 FineWeb 上预训练 Transformer LLM，并在多个指标上评估 CORE 分数</li>
<li>在 SmolTalk 的用户-助手对话、多选题、工具使用等数据上进行中期训练（Midtrain）</li>
<li>进行 SFT（有监督微调），并在常识（ARC-E/C、MMLU）、数学（GSM8K）、代码（HumanEval）任务上评估模型</li>
<li>可选地在 GSM8K 上使用 “GRPO” 算法进行强化学习（RL）</li>
<li>在推理引擎中实现高效推理：支持 KV cache、prefill/decode、工具调用（在轻量沙箱中运行 Python 解释器），可通过 CLI 或 ChatGPT 风格的 WebUI 与之交互</li>
<li>自动生成一份 Markdown “成绩单”，总结并游戏化整个流程</li>
</ul>
<p>即使仅花费约 100 美元（约 4 小时在 8×H100 节点上运行），你也能训练出一个“小型 ChatGPT 克隆”，能写故事、写诗、回答简单问题。</p>
<p>训练约 12 小时 后，它在 CORE 指标上就能超越 GPT-2。当预算提升至约 1000 美元（约 41.6 小时训练）时，模型的连贯性显著增强，能够解决简单的数学与代码问题，并通过多项选择测试。</p>
<p>例如，一个 深度 30 的模型，训练 24 小时（约等于 GPT-3 Small 125M 的 FLOPs，约为 GPT-3 的 1/1000），即可在 MMLU 上取得 40+ 分，在 ARC-Easy 上达 70+，在 GSM8K 上达 20+，等等。</p>
<p>我的目标，是将一个完整的 “强基线（strong baseline）” 堆栈整合进一个紧凑、可读、可改、可复刻的仓库中。</p>
<p>nanochat 将成为我课程 LLM101n 的收官项目（仍在开发中）。</p>
<p>我认为它也有潜力像 nanoGPT 一样，成长为一个研究用工具或基准平台。</p>
<p>目前它还远未“完成”、调优或优化（我相信仍有不少低垂果实可摘），但现在的框架已经足够稳定，足以放上 GitHub，让大家在此基础上改进各个部分。</p>
</blockquote>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-f900633274.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-f900633274.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 10:36:48 +0800</pubDate>
  </item><item>
    <title><![CDATA[开源鸿蒙 6.1 和 8.1 版本被确定为 LTS 建议版本]]></title>
    <link>https://www.oschina.net/news/377296</link>
    <itunes:title><![CDATA[开源鸿蒙 6.1 和 8.1 版本被确定为 LTS 建议版本]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><span>OpenAtom OpenHarmony <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F0o_MyxQHE6lG3wXOUI1VKw" target="_blank">发文</a>宣布，开源鸿蒙社区基于不同应用场景的需求差异与维护目标，将版本划分为Release版本与LTS版本两类，两者在定位、发布流程及维护周期上形成明确区分，开发者可根据业务需求精准选型。</span></p>
<p><span>（一）Release版本：稳定迭代的基础型版本</span></p>
<p><span>Release版本是开源鸿蒙社区发布的标准化稳定版本，其发布流程严格遵循质量管控体系：需先后通过集中编译验证、全量构建、集成测试验证，确保功能完整性与运行稳定性以及性能目标的达成，最终经开源鸿蒙PMC评审通过后正式发布，并为开发者提供固定周期的技术支持。</span></p>
<p><span>该版本的核心定位为 “快速迭代、稳定可用”，适用于对新功能有需求、但无需超长期维护支持的场景。</span></p>
<p><span>（二）LTS版本：长期支持的可靠型版本</span></p>
<p><span>LTS（Long-Term Support，长期支持）版本并非独立发布，而是从已正式上线的Release版本中筛选而来。社区结合Release分支的实际使用覆盖率、生态质量反馈及行业需求热度等因素，经开源鸿蒙PMC评审后确定；同时，社区会提前发布LTS版本规划路标，帮助开发者预判选型方向。</span></p>
<p><span>相较于Release版本分支，LTS版本分支的核心优势在于更长的维护生命周期，专为对系统稳定性、安全性要求更高的场景（如工业设备、智能终端量产产品等）提供持续技术支持，降低长期运维成本。</span></p>
<p><span>为帮助用户提前规划版本迁移时间线，开源鸿蒙社区针对Release与LTS版本分支制定了清晰的生命周期规则，明确不同阶段的维护范围、支持方式及调整机制。</span></p>
<p><span>（一）基础生命周期时长标准</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-560d6556a2.png"></p>
<p>（二）维护阶段定义说明</p>
<p><span>主动维护期：社区将例行规划并发布标签版本，持续合入缺陷修复、安全漏洞补丁及必要功能优化，确保分支持续处于稳定可用状态。</span></p>
<p><span>被动维护期：不再主动规划维护标签版本发布，仅针对社区评估认定的 “严重及以上级别” 安全漏洞与核心缺陷提供修复支持，聚焦关键稳定性保障。</span></p>
<p><span>（三）特殊规则调整机制</span></p>
<p><span>上述生命周期为社区版本的默认标准。若因行业特殊业务需求（如定制化设备研发、长期项目运维等）需调整某版本的生命周期，须由社区Release SIG发起正式申请，经开源鸿蒙PMC审议决策通过后实施调整，并通过社区官方邮件、公告专栏等渠道向全体开发者同步通知，确保信息透明。</span></p>
<hr>
<p><span>此外，社区明确后续将保持 “每两年发布一个LTS版本” 的迭代节奏。结合当前版本路线，拟定以下规划：</span></p>
<ul>
<li><span><strong>开源鸿蒙6.1 Release 版本：将作为2026年的LTS版本候选。</strong></span></li>
<li><span><strong>开源鸿蒙8.1 Release 版本：将作为2028年的LTS版本候选。</strong></span></li>
</ul>
<p><span>各版本的具体维护生命周期、关键时间节点（含主动维护期起止、被动维护期起止、认证截至时间等）可参考下表。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-cdac4a2a49.png"></p>
<p><span>重要提醒：“认证截至” 时间节点代表该版本停止接受新的设备认证申请，已完成认证的设备仍可在剩余维护周期内享受技术支持。建议开发者结合自身项目周期、产品迭代计划与版本生命周期，提前完成版本选型与迁移准备，避免因版本停止维护影响业务连续性。</span></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><span>OpenAtom OpenHarmony <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F0o_MyxQHE6lG3wXOUI1VKw" target="_blank">发文</a>宣布，开源鸿蒙社区基于不同应用场景的需求差异与维护目标，将版本划分为Release版本与LTS版本两类，两者在定位、发布流程及维护周期上形成明确区分，开发者可根据业务需求精准选型。</span></p>
<p><span>（一）Release版本：稳定迭代的基础型版本</span></p>
<p><span>Release版本是开源鸿蒙社区发布的标准化稳定版本，其发布流程严格遵循质量管控体系：需先后通过集中编译验证、全量构建、集成测试验证，确保功能完整性与运行稳定性以及性能目标的达成，最终经开源鸿蒙PMC评审通过后正式发布，并为开发者提供固定周期的技术支持。</span></p>
<p><span>该版本的核心定位为 “快速迭代、稳定可用”，适用于对新功能有需求、但无需超长期维护支持的场景。</span></p>
<p><span>（二）LTS版本：长期支持的可靠型版本</span></p>
<p><span>LTS（Long-Term Support，长期支持）版本并非独立发布，而是从已正式上线的Release版本中筛选而来。社区结合Release分支的实际使用覆盖率、生态质量反馈及行业需求热度等因素，经开源鸿蒙PMC评审后确定；同时，社区会提前发布LTS版本规划路标，帮助开发者预判选型方向。</span></p>
<p><span>相较于Release版本分支，LTS版本分支的核心优势在于更长的维护生命周期，专为对系统稳定性、安全性要求更高的场景（如工业设备、智能终端量产产品等）提供持续技术支持，降低长期运维成本。</span></p>
<p><span>为帮助用户提前规划版本迁移时间线，开源鸿蒙社区针对Release与LTS版本分支制定了清晰的生命周期规则，明确不同阶段的维护范围、支持方式及调整机制。</span></p>
<p><span>（一）基础生命周期时长标准</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-560d6556a2.png"></p>
<p>（二）维护阶段定义说明</p>
<p><span>主动维护期：社区将例行规划并发布标签版本，持续合入缺陷修复、安全漏洞补丁及必要功能优化，确保分支持续处于稳定可用状态。</span></p>
<p><span>被动维护期：不再主动规划维护标签版本发布，仅针对社区评估认定的 “严重及以上级别” 安全漏洞与核心缺陷提供修复支持，聚焦关键稳定性保障。</span></p>
<p><span>（三）特殊规则调整机制</span></p>
<p><span>上述生命周期为社区版本的默认标准。若因行业特殊业务需求（如定制化设备研发、长期项目运维等）需调整某版本的生命周期，须由社区Release SIG发起正式申请，经开源鸿蒙PMC审议决策通过后实施调整，并通过社区官方邮件、公告专栏等渠道向全体开发者同步通知，确保信息透明。</span></p>
<hr>
<p><span>此外，社区明确后续将保持 “每两年发布一个LTS版本” 的迭代节奏。结合当前版本路线，拟定以下规划：</span></p>
<ul>
<li><span><strong>开源鸿蒙6.1 Release 版本：将作为2026年的LTS版本候选。</strong></span></li>
<li><span><strong>开源鸿蒙8.1 Release 版本：将作为2028年的LTS版本候选。</strong></span></li>
</ul>
<p><span>各版本的具体维护生命周期、关键时间节点（含主动维护期起止、被动维护期起止、认证截至时间等）可参考下表。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-cdac4a2a49.png"></p>
<p><span>重要提醒：“认证截至” 时间节点代表该版本停止接受新的设备认证申请，已完成认证的设备仍可在剩余维护周期内享受技术支持。建议开发者结合自身项目周期、产品迭代计划与版本生命周期，提前完成版本选型与迁移准备，避免因版本停止维护影响业务连续性。</span></p>]]>
    </description>
    <content:encoded><![CDATA[<p><span>OpenAtom OpenHarmony <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F0o_MyxQHE6lG3wXOUI1VKw" target="_blank">发文</a>宣布，开源鸿蒙社区基于不同应用场景的需求差异与维护目标，将版本划分为Release版本与LTS版本两类，两者在定位、发布流程及维护周期上形成明确区分，开发者可根据业务需求精准选型。</span></p>
<p><span>（一）Release版本：稳定迭代的基础型版本</span></p>
<p><span>Release版本是开源鸿蒙社区发布的标准化稳定版本，其发布流程严格遵循质量管控体系：需先后通过集中编译验证、全量构建、集成测试验证，确保功能完整性与运行稳定性以及性能目标的达成，最终经开源鸿蒙PMC评审通过后正式发布，并为开发者提供固定周期的技术支持。</span></p>
<p><span>该版本的核心定位为 “快速迭代、稳定可用”，适用于对新功能有需求、但无需超长期维护支持的场景。</span></p>
<p><span>（二）LTS版本：长期支持的可靠型版本</span></p>
<p><span>LTS（Long-Term Support，长期支持）版本并非独立发布，而是从已正式上线的Release版本中筛选而来。社区结合Release分支的实际使用覆盖率、生态质量反馈及行业需求热度等因素，经开源鸿蒙PMC评审后确定；同时，社区会提前发布LTS版本规划路标，帮助开发者预判选型方向。</span></p>
<p><span>相较于Release版本分支，LTS版本分支的核心优势在于更长的维护生命周期，专为对系统稳定性、安全性要求更高的场景（如工业设备、智能终端量产产品等）提供持续技术支持，降低长期运维成本。</span></p>
<p><span>为帮助用户提前规划版本迁移时间线，开源鸿蒙社区针对Release与LTS版本分支制定了清晰的生命周期规则，明确不同阶段的维护范围、支持方式及调整机制。</span></p>
<p><span>（一）基础生命周期时长标准</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-560d6556a2.png"></p>
<p>（二）维护阶段定义说明</p>
<p><span>主动维护期：社区将例行规划并发布标签版本，持续合入缺陷修复、安全漏洞补丁及必要功能优化，确保分支持续处于稳定可用状态。</span></p>
<p><span>被动维护期：不再主动规划维护标签版本发布，仅针对社区评估认定的 “严重及以上级别” 安全漏洞与核心缺陷提供修复支持，聚焦关键稳定性保障。</span></p>
<p><span>（三）特殊规则调整机制</span></p>
<p><span>上述生命周期为社区版本的默认标准。若因行业特殊业务需求（如定制化设备研发、长期项目运维等）需调整某版本的生命周期，须由社区Release SIG发起正式申请，经开源鸿蒙PMC审议决策通过后实施调整，并通过社区官方邮件、公告专栏等渠道向全体开发者同步通知，确保信息透明。</span></p>
<hr>
<p><span>此外，社区明确后续将保持 “每两年发布一个LTS版本” 的迭代节奏。结合当前版本路线，拟定以下规划：</span></p>
<ul>
<li><span><strong>开源鸿蒙6.1 Release 版本：将作为2026年的LTS版本候选。</strong></span></li>
<li><span><strong>开源鸿蒙8.1 Release 版本：将作为2028年的LTS版本候选。</strong></span></li>
</ul>
<p><span>各版本的具体维护生命周期、关键时间节点（含主动维护期起止、被动维护期起止、认证截至时间等）可参考下表。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-cdac4a2a49.png"></p>
<p><span>重要提醒：“认证截至” 时间节点代表该版本停止接受新的设备认证申请，已完成认证的设备仍可在剩余维护周期内享受技术支持。建议开发者结合自身项目周期、产品迭代计划与版本生命周期，提前完成版本选型与迁移准备，避免因版本停止维护影响业务连续性。</span></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-560d6556a2.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-560d6556a2.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 10:25:52 +0800</pubDate>
  </item><item>
    <title><![CDATA[加州率先立法监管 AI 聊天机器人：最高罚款 25 万美元]]></title>
    <link>https://www.oschina.net/news/377293</link>
    <itunes:title><![CDATA[加州率先立法监管 AI 聊天机器人：最高罚款 25 万美元]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>美国加州在AI监管领域再次走在前列。州长加文·纽森周一签署了一项里程碑式法案，对AI伴侣聊天机器人实施监管，使加州成为全美首个要求AI聊天机器人运营商为AI伴侣实施安全协议的州。</p>
<p>这项名为SB243的法律旨在保护儿童和脆弱用户免受AI伴侣聊天机器人使用带来的伤害。该法律将要求从Meta、OpenAI等大型实验室到Character AI和Replika等专注伴侣服务的初创公司，如果其聊天机器人未能达到法律标准，将承担法律责任。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a52263dd85.png"></p>
<p>SB243由州参议员Steve Padilla和Josh Becker于今年1月提出，在青少年Adam Raine的悲剧发生后获得推动力。这名少年在与OpenAI的ChatGPT进行了一系列关于自杀的对话后选择结束生命。该立法还回应了泄露的内部文件，据报道这些文件显示Meta的聊天机器人被允许与儿童进行浪漫和感性对话。最近，科罗拉多州一个家庭对角色扮演初创公司Character AI提起诉讼，因为他们13岁的女儿在与该公司聊天机器人进行一系列问题性和性化对话后自杀身亡。</p>
<p>纽森在声明中表示，聊天机器人和社交媒体等新兴技术可以启发、教育和连接人们，但如果没有真正的护栏，技术也可能剥削、误导和危害孩子。他指出，已经看到一些真正可怕和悲惨的例子，年轻人因不受监管的技术而受到伤害，不会眼睁睁看着公司在没有必要限制和问责的情况下继续运营。加州可以继续引领AI和技术发展，但必须负责任地进行，每一步都要保护孩子。孩子的安全不是用来出售的。</p>
<p>SB243将于2026年1月1日生效，要求公司实施年龄验证以及关于社交媒体和伴侣聊天机器人的警告等功能。该法律还对从非法深度伪造中获利的人实施更严厉的处罚，每项违规行为最高可罚款25万美元。公司还必须建立应对自杀和自残的协议，这些协议将与该州公共卫生部门共享，同时提供关于服务如何向用户提供危机中心预防通知的统计数据。</p>
<p>根据法案条款，平台还必须明确表明任何互动都是人工生成的，聊天机器人不得将自己表示为医疗保健专业人员。公司必须为未成年人提供休息提醒，并防止他们查看聊天机器人生成的色情图像。</p>
<p>一些公司已经开始实施针对儿童的保护措施。例如OpenAI最近开始推出家长控制、内容保护和针对使用ChatGPT的儿童的自残检测系统。专为18岁以上成年人设计的Replika告诉TechCrunch，公司投入大量资源通过内容过滤系统和护栏来确保安全，这些护栏会引导用户使用值得信赖的危机资源，并致力于遵守现行法规。</p>
<p>Character AI表示其聊天机器人包含免责声明，说明所有聊天都是AI生成且虚构的。Character AI发言人告诉TechCrunch，公司欢迎与监管机构和立法者合作，因为他们为这个新兴领域制定法规和立法，并将遵守包括SB243在内的法律。</p>
<p>参议员Padilla告诉TechCrunch，该法案是朝着为这项极其强大的技术设置护栏方向迈出的一步。他表示必须迅速行动，不要错过机会窗口。他希望其他州能看到风险，认为许多州确实看到了，这是全国各地都在进行的对话，希望人们会采取行动。联邦政府显然没有采取行动，他认为加州有义务保护最脆弱的人群。</p>
<p>SB243是加州近几周出台的第二项重要AI监管法规。9月29日，纽森州长签署SB53成为法律，对大型AI公司建立新的透明度要求。该法案要求OpenAI、Anthropic、Meta和Google DeepMind等大型AI实验室对安全协议保持透明，还确保这些公司员工的举报人保护。</p>
<p>伊利诺伊州、内华达州和犹他州等其他州已经通过法律限制或完全禁止使用AI聊天机器人作为持证心理健康护理的替代品。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>美国加州在AI监管领域再次走在前列。州长加文·纽森周一签署了一项里程碑式法案，对AI伴侣聊天机器人实施监管，使加州成为全美首个要求AI聊天机器人运营商为AI伴侣实施安全协议的州。</p>
<p>这项名为SB243的法律旨在保护儿童和脆弱用户免受AI伴侣聊天机器人使用带来的伤害。该法律将要求从Meta、OpenAI等大型实验室到Character AI和Replika等专注伴侣服务的初创公司，如果其聊天机器人未能达到法律标准，将承担法律责任。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a52263dd85.png"></p>
<p>SB243由州参议员Steve Padilla和Josh Becker于今年1月提出，在青少年Adam Raine的悲剧发生后获得推动力。这名少年在与OpenAI的ChatGPT进行了一系列关于自杀的对话后选择结束生命。该立法还回应了泄露的内部文件，据报道这些文件显示Meta的聊天机器人被允许与儿童进行浪漫和感性对话。最近，科罗拉多州一个家庭对角色扮演初创公司Character AI提起诉讼，因为他们13岁的女儿在与该公司聊天机器人进行一系列问题性和性化对话后自杀身亡。</p>
<p>纽森在声明中表示，聊天机器人和社交媒体等新兴技术可以启发、教育和连接人们，但如果没有真正的护栏，技术也可能剥削、误导和危害孩子。他指出，已经看到一些真正可怕和悲惨的例子，年轻人因不受监管的技术而受到伤害，不会眼睁睁看着公司在没有必要限制和问责的情况下继续运营。加州可以继续引领AI和技术发展，但必须负责任地进行，每一步都要保护孩子。孩子的安全不是用来出售的。</p>
<p>SB243将于2026年1月1日生效，要求公司实施年龄验证以及关于社交媒体和伴侣聊天机器人的警告等功能。该法律还对从非法深度伪造中获利的人实施更严厉的处罚，每项违规行为最高可罚款25万美元。公司还必须建立应对自杀和自残的协议，这些协议将与该州公共卫生部门共享，同时提供关于服务如何向用户提供危机中心预防通知的统计数据。</p>
<p>根据法案条款，平台还必须明确表明任何互动都是人工生成的，聊天机器人不得将自己表示为医疗保健专业人员。公司必须为未成年人提供休息提醒，并防止他们查看聊天机器人生成的色情图像。</p>
<p>一些公司已经开始实施针对儿童的保护措施。例如OpenAI最近开始推出家长控制、内容保护和针对使用ChatGPT的儿童的自残检测系统。专为18岁以上成年人设计的Replika告诉TechCrunch，公司投入大量资源通过内容过滤系统和护栏来确保安全，这些护栏会引导用户使用值得信赖的危机资源，并致力于遵守现行法规。</p>
<p>Character AI表示其聊天机器人包含免责声明，说明所有聊天都是AI生成且虚构的。Character AI发言人告诉TechCrunch，公司欢迎与监管机构和立法者合作，因为他们为这个新兴领域制定法规和立法，并将遵守包括SB243在内的法律。</p>
<p>参议员Padilla告诉TechCrunch，该法案是朝着为这项极其强大的技术设置护栏方向迈出的一步。他表示必须迅速行动，不要错过机会窗口。他希望其他州能看到风险，认为许多州确实看到了，这是全国各地都在进行的对话，希望人们会采取行动。联邦政府显然没有采取行动，他认为加州有义务保护最脆弱的人群。</p>
<p>SB243是加州近几周出台的第二项重要AI监管法规。9月29日，纽森州长签署SB53成为法律，对大型AI公司建立新的透明度要求。该法案要求OpenAI、Anthropic、Meta和Google DeepMind等大型AI实验室对安全协议保持透明，还确保这些公司员工的举报人保护。</p>
<p>伊利诺伊州、内华达州和犹他州等其他州已经通过法律限制或完全禁止使用AI聊天机器人作为持证心理健康护理的替代品。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>美国加州在AI监管领域再次走在前列。州长加文·纽森周一签署了一项里程碑式法案，对AI伴侣聊天机器人实施监管，使加州成为全美首个要求AI聊天机器人运营商为AI伴侣实施安全协议的州。</p>
<p>这项名为SB243的法律旨在保护儿童和脆弱用户免受AI伴侣聊天机器人使用带来的伤害。该法律将要求从Meta、OpenAI等大型实验室到Character AI和Replika等专注伴侣服务的初创公司，如果其聊天机器人未能达到法律标准，将承担法律责任。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a52263dd85.png"></p>
<p>SB243由州参议员Steve Padilla和Josh Becker于今年1月提出，在青少年Adam Raine的悲剧发生后获得推动力。这名少年在与OpenAI的ChatGPT进行了一系列关于自杀的对话后选择结束生命。该立法还回应了泄露的内部文件，据报道这些文件显示Meta的聊天机器人被允许与儿童进行浪漫和感性对话。最近，科罗拉多州一个家庭对角色扮演初创公司Character AI提起诉讼，因为他们13岁的女儿在与该公司聊天机器人进行一系列问题性和性化对话后自杀身亡。</p>
<p>纽森在声明中表示，聊天机器人和社交媒体等新兴技术可以启发、教育和连接人们，但如果没有真正的护栏，技术也可能剥削、误导和危害孩子。他指出，已经看到一些真正可怕和悲惨的例子，年轻人因不受监管的技术而受到伤害，不会眼睁睁看着公司在没有必要限制和问责的情况下继续运营。加州可以继续引领AI和技术发展，但必须负责任地进行，每一步都要保护孩子。孩子的安全不是用来出售的。</p>
<p>SB243将于2026年1月1日生效，要求公司实施年龄验证以及关于社交媒体和伴侣聊天机器人的警告等功能。该法律还对从非法深度伪造中获利的人实施更严厉的处罚，每项违规行为最高可罚款25万美元。公司还必须建立应对自杀和自残的协议，这些协议将与该州公共卫生部门共享，同时提供关于服务如何向用户提供危机中心预防通知的统计数据。</p>
<p>根据法案条款，平台还必须明确表明任何互动都是人工生成的，聊天机器人不得将自己表示为医疗保健专业人员。公司必须为未成年人提供休息提醒，并防止他们查看聊天机器人生成的色情图像。</p>
<p>一些公司已经开始实施针对儿童的保护措施。例如OpenAI最近开始推出家长控制、内容保护和针对使用ChatGPT的儿童的自残检测系统。专为18岁以上成年人设计的Replika告诉TechCrunch，公司投入大量资源通过内容过滤系统和护栏来确保安全，这些护栏会引导用户使用值得信赖的危机资源，并致力于遵守现行法规。</p>
<p>Character AI表示其聊天机器人包含免责声明，说明所有聊天都是AI生成且虚构的。Character AI发言人告诉TechCrunch，公司欢迎与监管机构和立法者合作，因为他们为这个新兴领域制定法规和立法，并将遵守包括SB243在内的法律。</p>
<p>参议员Padilla告诉TechCrunch，该法案是朝着为这项极其强大的技术设置护栏方向迈出的一步。他表示必须迅速行动，不要错过机会窗口。他希望其他州能看到风险，认为许多州确实看到了，这是全国各地都在进行的对话，希望人们会采取行动。联邦政府显然没有采取行动，他认为加州有义务保护最脆弱的人群。</p>
<p>SB243是加州近几周出台的第二项重要AI监管法规。9月29日，纽森州长签署SB53成为法律，对大型AI公司建立新的透明度要求。该法案要求OpenAI、Anthropic、Meta和Google DeepMind等大型AI实验室对安全协议保持透明，还确保这些公司员工的举报人保护。</p>
<p>伊利诺伊州、内华达州和犹他州等其他州已经通过法律限制或完全禁止使用AI聊天机器人作为持证心理健康护理的替代品。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a52263dd85.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-a52263dd85.png" medium="image"/>
    <pubDate>Tue, 14 Oct 2025 10:10:50 +0800</pubDate>
  </item><item>
    <title><![CDATA[当关键软件也被卡，我们的答案在哪里]]></title>
    <link>https://www.oschina.net/news/377213</link>
    <itunes:title><![CDATA[当关键软件也被卡，我们的答案在哪里]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>10 月 11 日 凌晨，美国总统特朗普在社交平台上宣布，自 2025 年 11 月 1 日起，美方将对中国实施新的 100% 关税，并同步启动对「所有关键软件」的出口管制。这意味着，从芯片到算法、从源代码到工具链，限制正<strong>从「硬件禁运」延伸到「软件封锁」</strong>。</p>
<p>对任何依赖国外开发环境、工具生态或底层组件的企业来说，这不是一场简单的贸易摩擦，<strong>实质上是在考验中国软件产业的全链路自主能力</strong>。</p>
<p><span><strong>软件出口受限，真正的风险才刚开始</strong></span></p>
<p>过去几年，全球供应链焦点集中在芯片和硬件，但如果关键的软件基础设施被限制出口，影响将更深远：</p>
<ul>
<li><strong>开发环境</strong>：IDE、编译器、调试工具、构建流水线等，若涉及美国技术或授权，可能被纳入管控清单。</li>
<li><strong>依赖生态</strong>：许多国内项目在依赖树中使用了境外开源组件，一旦被封锁或移除访问权限，构建链条会中断。</li>
<li><strong>服务端软件</strong>：CI/CD 平台、代码扫描、安全测试等 SaaS 工具，多数部署在境外云上，面临被封锁或关闭账户的风险。</li>
</ul>
<p>换言之，受限从硬件蔓延到软件，将直接影响整个数字产业的研发能力。</p>
<p><span><strong>Gitee：软件出口管制下的「国产底座」</strong></span></p>
<p>在这种背景下，自主可控的研发基础设施不再是口号，而是生存条件。</p>
<p>Gitee 作为中国本土最大的企业级 DevSecOps 研发效能平台，一直致力于构建全国产化的软件研发生态，从托管、协作、安全到智能化开发工具，形成了完整链路：</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ea2773debc.png"></p>
<ul>
<li><strong>代码资产可控</strong>：所有数据、版本、分支记录均托管于境内，避免因境外平台政策变化导致的冻结或删除风险。</li>
<li><strong>研发流程闭环</strong>：基于 100% 可信的 DevSecOps 基础设施，Gitee 让软件生产过程真正实现「自主、安全、透明、可控」。</li>
<li><strong>AI 与安全双保障</strong>：模力方舟（Gitee AI）为开发者提供国产算力与大模型支持，实现代码生成、智能审查等 AI 能力。</li>
<li><strong>合规与私有部署</strong>：Gitee 企业版支持完全私有化部署，满足政企、科研、能源等行业的数据安全与合规要求。</li>
</ul>
<p><span><strong>从依赖到独立，开源是最好的防御</strong></span></p>
<p>软件出口管制同时暴露了一个事实：<strong>闭源即风险</strong>。</p>
<p>当核心工具、库、平台掌握在他人手中，「卡脖子」的可能性永远存在。唯有开源，才能让生态具备再造与继承能力。</p>
<p>Gitee 一直在推动国内开源基础设施建设，<strong>十余年来为超过 1350 万名开发者提供服务，托管仓库数超过 3600 万</strong>，覆盖操作系统、数据库、中间件、安全工具、AI 模型等多个关键领域。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-635e875eee.png"></p>
<p>开源的核心意义，是让技术可见、可复用、可持续。在供应链不确定的时代，这是保持研发连续性的关键机制。</p>
<p><span><strong>自主可控只是起点</strong></span></p>
<p>这场可能到来的软件出口封锁再次提醒我们：<strong>任何建立在他人授权上的便利，终有一天要付出代价</strong>。</p>
<p>从开发工具到模型算力，从代码托管到版本控制，从开源生态到数据安全，中国必须拥有自己的技术底座。</p>
<p>Gitee 将持续完善自主可控的研发基础设施，与开发者、企业、科研机构共同打造一个不惧封锁的开源生态系统。</p>
<p>在这一进程中，<strong>自主可控只是起点，能在开放中持续创新，才是长期的安全</strong>。</p>
<blockquote>
<p><a href="https://gitee.com/" target="_blank">https://gitee.com/</a></p>
</blockquote>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>10 月 11 日 凌晨，美国总统特朗普在社交平台上宣布，自 2025 年 11 月 1 日起，美方将对中国实施新的 100% 关税，并同步启动对「所有关键软件」的出口管制。这意味着，从芯片到算法、从源代码到工具链，限制正<strong>从「硬件禁运」延伸到「软件封锁」</strong>。</p>
<p>对任何依赖国外开发环境、工具生态或底层组件的企业来说，这不是一场简单的贸易摩擦，<strong>实质上是在考验中国软件产业的全链路自主能力</strong>。</p>
<p><span><strong>软件出口受限，真正的风险才刚开始</strong></span></p>
<p>过去几年，全球供应链焦点集中在芯片和硬件，但如果关键的软件基础设施被限制出口，影响将更深远：</p>
<ul>
<li><strong>开发环境</strong>：IDE、编译器、调试工具、构建流水线等，若涉及美国技术或授权，可能被纳入管控清单。</li>
<li><strong>依赖生态</strong>：许多国内项目在依赖树中使用了境外开源组件，一旦被封锁或移除访问权限，构建链条会中断。</li>
<li><strong>服务端软件</strong>：CI/CD 平台、代码扫描、安全测试等 SaaS 工具，多数部署在境外云上，面临被封锁或关闭账户的风险。</li>
</ul>
<p>换言之，受限从硬件蔓延到软件，将直接影响整个数字产业的研发能力。</p>
<p><span><strong>Gitee：软件出口管制下的「国产底座」</strong></span></p>
<p>在这种背景下，自主可控的研发基础设施不再是口号，而是生存条件。</p>
<p>Gitee 作为中国本土最大的企业级 DevSecOps 研发效能平台，一直致力于构建全国产化的软件研发生态，从托管、协作、安全到智能化开发工具，形成了完整链路：</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ea2773debc.png"></p>
<ul>
<li><strong>代码资产可控</strong>：所有数据、版本、分支记录均托管于境内，避免因境外平台政策变化导致的冻结或删除风险。</li>
<li><strong>研发流程闭环</strong>：基于 100% 可信的 DevSecOps 基础设施，Gitee 让软件生产过程真正实现「自主、安全、透明、可控」。</li>
<li><strong>AI 与安全双保障</strong>：模力方舟（Gitee AI）为开发者提供国产算力与大模型支持，实现代码生成、智能审查等 AI 能力。</li>
<li><strong>合规与私有部署</strong>：Gitee 企业版支持完全私有化部署，满足政企、科研、能源等行业的数据安全与合规要求。</li>
</ul>
<p><span><strong>从依赖到独立，开源是最好的防御</strong></span></p>
<p>软件出口管制同时暴露了一个事实：<strong>闭源即风险</strong>。</p>
<p>当核心工具、库、平台掌握在他人手中，「卡脖子」的可能性永远存在。唯有开源，才能让生态具备再造与继承能力。</p>
<p>Gitee 一直在推动国内开源基础设施建设，<strong>十余年来为超过 1350 万名开发者提供服务，托管仓库数超过 3600 万</strong>，覆盖操作系统、数据库、中间件、安全工具、AI 模型等多个关键领域。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-635e875eee.png"></p>
<p>开源的核心意义，是让技术可见、可复用、可持续。在供应链不确定的时代，这是保持研发连续性的关键机制。</p>
<p><span><strong>自主可控只是起点</strong></span></p>
<p>这场可能到来的软件出口封锁再次提醒我们：<strong>任何建立在他人授权上的便利，终有一天要付出代价</strong>。</p>
<p>从开发工具到模型算力，从代码托管到版本控制，从开源生态到数据安全，中国必须拥有自己的技术底座。</p>
<p>Gitee 将持续完善自主可控的研发基础设施，与开发者、企业、科研机构共同打造一个不惧封锁的开源生态系统。</p>
<p>在这一进程中，<strong>自主可控只是起点，能在开放中持续创新，才是长期的安全</strong>。</p>
<blockquote>
<p><a href="https://gitee.com/" target="_blank">https://gitee.com/</a></p>
</blockquote>]]>
    </description>
    <content:encoded><![CDATA[<p>10 月 11 日 凌晨，美国总统特朗普在社交平台上宣布，自 2025 年 11 月 1 日起，美方将对中国实施新的 100% 关税，并同步启动对「所有关键软件」的出口管制。这意味着，从芯片到算法、从源代码到工具链，限制正<strong>从「硬件禁运」延伸到「软件封锁」</strong>。</p>
<p>对任何依赖国外开发环境、工具生态或底层组件的企业来说，这不是一场简单的贸易摩擦，<strong>实质上是在考验中国软件产业的全链路自主能力</strong>。</p>
<p><span><strong>软件出口受限，真正的风险才刚开始</strong></span></p>
<p>过去几年，全球供应链焦点集中在芯片和硬件，但如果关键的软件基础设施被限制出口，影响将更深远：</p>
<ul>
<li><strong>开发环境</strong>：IDE、编译器、调试工具、构建流水线等，若涉及美国技术或授权，可能被纳入管控清单。</li>
<li><strong>依赖生态</strong>：许多国内项目在依赖树中使用了境外开源组件，一旦被封锁或移除访问权限，构建链条会中断。</li>
<li><strong>服务端软件</strong>：CI/CD 平台、代码扫描、安全测试等 SaaS 工具，多数部署在境外云上，面临被封锁或关闭账户的风险。</li>
</ul>
<p>换言之，受限从硬件蔓延到软件，将直接影响整个数字产业的研发能力。</p>
<p><span><strong>Gitee：软件出口管制下的「国产底座」</strong></span></p>
<p>在这种背景下，自主可控的研发基础设施不再是口号，而是生存条件。</p>
<p>Gitee 作为中国本土最大的企业级 DevSecOps 研发效能平台，一直致力于构建全国产化的软件研发生态，从托管、协作、安全到智能化开发工具，形成了完整链路：</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ea2773debc.png"></p>
<ul>
<li><strong>代码资产可控</strong>：所有数据、版本、分支记录均托管于境内，避免因境外平台政策变化导致的冻结或删除风险。</li>
<li><strong>研发流程闭环</strong>：基于 100% 可信的 DevSecOps 基础设施，Gitee 让软件生产过程真正实现「自主、安全、透明、可控」。</li>
<li><strong>AI 与安全双保障</strong>：模力方舟（Gitee AI）为开发者提供国产算力与大模型支持，实现代码生成、智能审查等 AI 能力。</li>
<li><strong>合规与私有部署</strong>：Gitee 企业版支持完全私有化部署，满足政企、科研、能源等行业的数据安全与合规要求。</li>
</ul>
<p><span><strong>从依赖到独立，开源是最好的防御</strong></span></p>
<p>软件出口管制同时暴露了一个事实：<strong>闭源即风险</strong>。</p>
<p>当核心工具、库、平台掌握在他人手中，「卡脖子」的可能性永远存在。唯有开源，才能让生态具备再造与继承能力。</p>
<p>Gitee 一直在推动国内开源基础设施建设，<strong>十余年来为超过 1350 万名开发者提供服务，托管仓库数超过 3600 万</strong>，覆盖操作系统、数据库、中间件、安全工具、AI 模型等多个关键领域。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-635e875eee.png"></p>
<p>开源的核心意义，是让技术可见、可复用、可持续。在供应链不确定的时代，这是保持研发连续性的关键机制。</p>
<p><span><strong>自主可控只是起点</strong></span></p>
<p>这场可能到来的软件出口封锁再次提醒我们：<strong>任何建立在他人授权上的便利，终有一天要付出代价</strong>。</p>
<p>从开发工具到模型算力，从代码托管到版本控制，从开源生态到数据安全，中国必须拥有自己的技术底座。</p>
<p>Gitee 将持续完善自主可控的研发基础设施，与开发者、企业、科研机构共同打造一个不惧封锁的开源生态系统。</p>
<p>在这一进程中，<strong>自主可控只是起点，能在开放中持续创新，才是长期的安全</strong>。</p>
<blockquote>
<p><a href="https://gitee.com/" target="_blank">https://gitee.com/</a></p>
</blockquote>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ea2773debc.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ea2773debc.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 19:44:24 +0800</pubDate>
  </item><item>
    <title><![CDATA[苹果“天线门”事件 15 年后被发现是信号显示算法存在问题]]></title>
    <link>https://www.oschina.net/news/377210</link>
    <itunes:title><![CDATA[苹果“天线门”事件 15 年后被发现是信号显示算法存在问题]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>困扰业界15年的iPhone 4“天线门”事件迎来反转。<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2F9to5mac.com%2F2025%2F10%2F08%2Fa-15-year-mystery-solved-the-20-bytes-of-code-that-fixed-antennagate%2F" target="_blank">据科技媒体9to5Mac报道</a>，软件工程师 Sam Henri Gold通过分析固件代码发现，<strong>信号骤降问题并非硬件缺陷，而是苹果信号显示算法存在严重偏差</strong>。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-055024f4a5.png"></p>
<p>原算法因“查找表”数值过于乐观，常虚标2至3格信号，导致用户握持手机时信号从虚假高位骤降至真实水平，产生断崖式下跌错觉。</p>
<p>Sam Henri Gold对比固件后发现，苹果在iOS 4.0.1中仅修改了CommCenter框架中的一张查找表，总计20字节数据，调整信号格转换逻辑，并优化前两格显示高度，使信号反馈更真实平稳。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a669772a07.png"> <img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-9dcfacedde.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b067e746a6.png"></p>
<p>不过，彭博社记者Mark Gurman直言“这完全是胡说八道”，他认为该更新并未真正修复问题，只是掩盖了信号栏的显示，<strong>真正的解决方案是iPhone 4s重新设计的天线系统</strong>。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>困扰业界15年的iPhone 4“天线门”事件迎来反转。<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2F9to5mac.com%2F2025%2F10%2F08%2Fa-15-year-mystery-solved-the-20-bytes-of-code-that-fixed-antennagate%2F" target="_blank">据科技媒体9to5Mac报道</a>，软件工程师 Sam Henri Gold通过分析固件代码发现，<strong>信号骤降问题并非硬件缺陷，而是苹果信号显示算法存在严重偏差</strong>。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-055024f4a5.png"></p>
<p>原算法因“查找表”数值过于乐观，常虚标2至3格信号，导致用户握持手机时信号从虚假高位骤降至真实水平，产生断崖式下跌错觉。</p>
<p>Sam Henri Gold对比固件后发现，苹果在iOS 4.0.1中仅修改了CommCenter框架中的一张查找表，总计20字节数据，调整信号格转换逻辑，并优化前两格显示高度，使信号反馈更真实平稳。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a669772a07.png"> <img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-9dcfacedde.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b067e746a6.png"></p>
<p>不过，彭博社记者Mark Gurman直言“这完全是胡说八道”，他认为该更新并未真正修复问题，只是掩盖了信号栏的显示，<strong>真正的解决方案是iPhone 4s重新设计的天线系统</strong>。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>困扰业界15年的iPhone 4“天线门”事件迎来反转。<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2F9to5mac.com%2F2025%2F10%2F08%2Fa-15-year-mystery-solved-the-20-bytes-of-code-that-fixed-antennagate%2F" target="_blank">据科技媒体9to5Mac报道</a>，软件工程师 Sam Henri Gold通过分析固件代码发现，<strong>信号骤降问题并非硬件缺陷，而是苹果信号显示算法存在严重偏差</strong>。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-055024f4a5.png"></p>
<p>原算法因“查找表”数值过于乐观，常虚标2至3格信号，导致用户握持手机时信号从虚假高位骤降至真实水平，产生断崖式下跌错觉。</p>
<p>Sam Henri Gold对比固件后发现，苹果在iOS 4.0.1中仅修改了CommCenter框架中的一张查找表，总计20字节数据，调整信号格转换逻辑，并优化前两格显示高度，使信号反馈更真实平稳。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a669772a07.png"> <img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-9dcfacedde.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b067e746a6.png"></p>
<p>不过，彭博社记者Mark Gurman直言“这完全是胡说八道”，他认为该更新并未真正修复问题，只是掩盖了信号栏的显示，<strong>真正的解决方案是iPhone 4s重新设计的天线系统</strong>。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-055024f4a5.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-055024f4a5.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 19:32:16 +0800</pubDate>
  </item><item>
    <title><![CDATA[Firefox “摇晃总结”荣获《时代》2025 年度最佳发明特别提及]]></title>
    <link>https://www.oschina.net/news/377208/firefox-shake-to-summarize-time-best-inventions</link>
    <itunes:title><![CDATA[Firefox “摇晃总结”荣获《时代》2025 年度最佳发明特别提及]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><strong>&nbsp;“摇晃总结”（Shake to Summarize）</strong>是 Firefox 于 2025 年 9 月面向 iPhone 推出的一项新功能，旨在帮助用户快速获取网页内容的精华摘要，尤其适用于在移动设备上浏览时节省时间和精力。</p>
<blockquote>
<p><a href="https://www.oschina.net/news/371176/firefox-shake-to-summarize-ios-ai-launch" target="_blank">Firefox 浏览器在 iPhone 上线 AI 新功能：“摇晃总结”</a></p>
</blockquote>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-5ba110d3ae.png"></p>
<p>据介绍，Firefox “摇晃总结”功能目前仅限于系统语言设置为英语的美国地区 iOS 用户。在技术实现上，它利用了苹果的端侧 AI 能力：</p>
<ul>
<li> <p>iPhone 15 Pro 或更新机型，运行 iOS 26 及以上版本的用户，摘要由 Apple Intelligence 本地生成；</p> </li>
<li> <p>其他设备（iOS 16 及以上版本）则通过 Mozilla 云端 AI 生成摘要。</p> </li>
</ul>
<p>该功能在 2025 年 10 月 9 日被《时代》杂志评为“2025 年度最佳发明特别提及”，表彰其在解决信息过载问题方面的创新贡献。Firefox 总经理 Anthony Enzor-DeMeo 对此表示：“这种认可是对我们 UX、设计、产品和工程团队所做出色工作的证明，他们使这一创新成为现实。”</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><strong>&nbsp;“摇晃总结”（Shake to Summarize）</strong>是 Firefox 于 2025 年 9 月面向 iPhone 推出的一项新功能，旨在帮助用户快速获取网页内容的精华摘要，尤其适用于在移动设备上浏览时节省时间和精力。</p>
<blockquote>
<p><a href="https://www.oschina.net/news/371176/firefox-shake-to-summarize-ios-ai-launch" target="_blank">Firefox 浏览器在 iPhone 上线 AI 新功能：“摇晃总结”</a></p>
</blockquote>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-5ba110d3ae.png"></p>
<p>据介绍，Firefox “摇晃总结”功能目前仅限于系统语言设置为英语的美国地区 iOS 用户。在技术实现上，它利用了苹果的端侧 AI 能力：</p>
<ul>
<li> <p>iPhone 15 Pro 或更新机型，运行 iOS 26 及以上版本的用户，摘要由 Apple Intelligence 本地生成；</p> </li>
<li> <p>其他设备（iOS 16 及以上版本）则通过 Mozilla 云端 AI 生成摘要。</p> </li>
</ul>
<p>该功能在 2025 年 10 月 9 日被《时代》杂志评为“2025 年度最佳发明特别提及”，表彰其在解决信息过载问题方面的创新贡献。Firefox 总经理 Anthony Enzor-DeMeo 对此表示：“这种认可是对我们 UX、设计、产品和工程团队所做出色工作的证明，他们使这一创新成为现实。”</p>]]>
    </description>
    <content:encoded><![CDATA[<p><strong>&nbsp;“摇晃总结”（Shake to Summarize）</strong>是 Firefox 于 2025 年 9 月面向 iPhone 推出的一项新功能，旨在帮助用户快速获取网页内容的精华摘要，尤其适用于在移动设备上浏览时节省时间和精力。</p>
<blockquote>
<p><a href="https://www.oschina.net/news/371176/firefox-shake-to-summarize-ios-ai-launch" target="_blank">Firefox 浏览器在 iPhone 上线 AI 新功能：“摇晃总结”</a></p>
</blockquote>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-5ba110d3ae.png"></p>
<p>据介绍，Firefox “摇晃总结”功能目前仅限于系统语言设置为英语的美国地区 iOS 用户。在技术实现上，它利用了苹果的端侧 AI 能力：</p>
<ul>
<li> <p>iPhone 15 Pro 或更新机型，运行 iOS 26 及以上版本的用户，摘要由 Apple Intelligence 本地生成；</p> </li>
<li> <p>其他设备（iOS 16 及以上版本）则通过 Mozilla 云端 AI 生成摘要。</p> </li>
</ul>
<p>该功能在 2025 年 10 月 9 日被《时代》杂志评为“2025 年度最佳发明特别提及”，表彰其在解决信息过载问题方面的创新贡献。Firefox 总经理 Anthony Enzor-DeMeo 对此表示：“这种认可是对我们 UX、设计、产品和工程团队所做出色工作的证明，他们使这一创新成为现实。”</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-5ba110d3ae.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-5ba110d3ae.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 19:04:08 +0800</pubDate>
  </item><item>
    <title><![CDATA[MinerU - PDF 转化为机器可读格式的工具]]></title>
    <link>https://www.oschina.net/p/mineru</link>
    <itunes:title><![CDATA[MinerU - PDF 转化为机器可读格式的工具]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>MinerU是一款将PDF转化为机器可读格式的工具（如markdown、json），可以很方便地抽取为任意格式。 MinerU诞生于书生-浦语的预训练过程中，我们将会集中精力解决科技文献中的符号转化问题，希望在大模型时代为科技发展做出贡献。 相比国内外知名商用产品MinerU还很年轻，如果遇到问题或者结果不及预期请到issue提交问题，同时附上相关PDF。</p>
<div>
<h2>主要功能</h2>
</div>
<ul>
<li>删除页眉、页脚、脚注、页码等元素，确保语义连贯</li>
<li>输出符合人类阅读顺序的文本，适用于单栏、多栏及复杂排版</li>
<li>保留原文档的结构，包括标题、段落、列表等</li>
<li>提取图像、图片描述、表格、表格标题及脚注</li>
<li>自动识别并转换文档中的公式为LaTeX格式</li>
<li>自动识别并转换文档中的表格为HTML格式</li>
<li>自动检测扫描版PDF和乱码PDF，并启用OCR功能</li>
<li>OCR支持84种语言的检测与识别</li>
<li>支持多种输出格式，如多模态与NLP的Markdown、按阅读顺序排序的JSON、含有丰富信息的中间格式等</li>
<li>支持多种可视化结果，包括layout可视化、span可视化等，便于高效确认输出效果与质检</li>
<li>支持纯CPU环境运行，并支持 GPU(CUDA)/NPU(CANN)/MPS 加速</li>
<li>兼容Windows、Linux和Mac平台</li>
</ul>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>MinerU是一款将PDF转化为机器可读格式的工具（如markdown、json），可以很方便地抽取为任意格式。 MinerU诞生于书生-浦语的预训练过程中，我们将会集中精力解决科技文献中的符号转化问题，希望在大模型时代为科技发展做出贡献。 相比国内外知名商用产品MinerU还很年轻，如果遇到问题或者结果不及预期请到issue提交问题，同时附上相关PDF。</p>
<div>
<h2>主要功能</h2>
</div>
<ul>
<li>删除页眉、页脚、脚注、页码等元素，确保语义连贯</li>
<li>输出符合人类阅读顺序的文本，适用于单栏、多栏及复杂排版</li>
<li>保留原文档的结构，包括标题、段落、列表等</li>
<li>提取图像、图片描述、表格、表格标题及脚注</li>
<li>自动识别并转换文档中的公式为LaTeX格式</li>
<li>自动识别并转换文档中的表格为HTML格式</li>
<li>自动检测扫描版PDF和乱码PDF，并启用OCR功能</li>
<li>OCR支持84种语言的检测与识别</li>
<li>支持多种输出格式，如多模态与NLP的Markdown、按阅读顺序排序的JSON、含有丰富信息的中间格式等</li>
<li>支持多种可视化结果，包括layout可视化、span可视化等，便于高效确认输出效果与质检</li>
<li>支持纯CPU环境运行，并支持 GPU(CUDA)/NPU(CANN)/MPS 加速</li>
<li>兼容Windows、Linux和Mac平台</li>
</ul>]]>
    </description>
    <content:encoded><![CDATA[<p>MinerU是一款将PDF转化为机器可读格式的工具（如markdown、json），可以很方便地抽取为任意格式。 MinerU诞生于书生-浦语的预训练过程中，我们将会集中精力解决科技文献中的符号转化问题，希望在大模型时代为科技发展做出贡献。 相比国内外知名商用产品MinerU还很年轻，如果遇到问题或者结果不及预期请到issue提交问题，同时附上相关PDF。</p>
<div>
<h2>主要功能</h2>
</div>
<ul>
<li>删除页眉、页脚、脚注、页码等元素，确保语义连贯</li>
<li>输出符合人类阅读顺序的文本，适用于单栏、多栏及复杂排版</li>
<li>保留原文档的结构，包括标题、段落、列表等</li>
<li>提取图像、图片描述、表格、表格标题及脚注</li>
<li>自动识别并转换文档中的公式为LaTeX格式</li>
<li>自动识别并转换文档中的表格为HTML格式</li>
<li>自动检测扫描版PDF和乱码PDF，并启用OCR功能</li>
<li>OCR支持84种语言的检测与识别</li>
<li>支持多种输出格式，如多模态与NLP的Markdown、按阅读顺序排序的JSON、含有丰富信息的中间格式等</li>
<li>支持多种可视化结果，包括layout可视化、span可视化等，便于高效确认输出效果与质检</li>
<li>支持纯CPU环境运行，并支持 GPU(CUDA)/NPU(CANN)/MPS 加速</li>
<li>兼容Windows、Linux和Mac平台</li>
</ul>]]></content:encoded>
    
    <pubDate>Mon, 13 Oct 2025 18:47:51 +0800</pubDate>
  </item><item>
    <title><![CDATA[Linus Torvalds 批评 Rust 代码格式化工具：称其“完全疯狂”]]></title>
    <link>https://www.oschina.net/news/377205</link>
    <itunes:title><![CDATA[Linus Torvalds 批评 Rust 代码格式化工具：称其“完全疯狂”]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>近日，Linux之父Linus Torvalds在Linux内核邮件列表上<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flore.kernel.org%2Flkml%2FCAHk-%3DwgO7S_FZUSBbngG5vtejWOpzDfTTBkVvP3_yjJmFddbzA%40mail.gmail.com%2FT%2F%23me533a148abe97c29e0e7150508c42345b2a64e13" target="_blank">公开批评</a>Rust的代码格式化工具rustfmt，直言其行为“完全疯狂”。</p>
<blockquote>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-36dbe77762.png"></p>
</blockquote>
<p>他指出，Rust中<code>use crate::xyz;</code>语句在自动格式化后常被压缩成单行，难以保持清晰的结构，降低了代码可读性，并增加了未来合并冲突的风险。</p>
<p>Torvalds还批评rustfmt在多行与单行格式处理上不够一致，给维护大型项目带来困难。他建议重新审视Rust的格式化规则，尤其是针对独立的<code>use</code>语句，避免将它们压缩成单行格式，以提高代码的可读性和可维护性。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>近日，Linux之父Linus Torvalds在Linux内核邮件列表上<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flore.kernel.org%2Flkml%2FCAHk-%3DwgO7S_FZUSBbngG5vtejWOpzDfTTBkVvP3_yjJmFddbzA%40mail.gmail.com%2FT%2F%23me533a148abe97c29e0e7150508c42345b2a64e13" target="_blank">公开批评</a>Rust的代码格式化工具rustfmt，直言其行为“完全疯狂”。</p>
<blockquote>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-36dbe77762.png"></p>
</blockquote>
<p>他指出，Rust中<code>use crate::xyz;</code>语句在自动格式化后常被压缩成单行，难以保持清晰的结构，降低了代码可读性，并增加了未来合并冲突的风险。</p>
<p>Torvalds还批评rustfmt在多行与单行格式处理上不够一致，给维护大型项目带来困难。他建议重新审视Rust的格式化规则，尤其是针对独立的<code>use</code>语句，避免将它们压缩成单行格式，以提高代码的可读性和可维护性。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>近日，Linux之父Linus Torvalds在Linux内核邮件列表上<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flore.kernel.org%2Flkml%2FCAHk-%3DwgO7S_FZUSBbngG5vtejWOpzDfTTBkVvP3_yjJmFddbzA%40mail.gmail.com%2FT%2F%23me533a148abe97c29e0e7150508c42345b2a64e13" target="_blank">公开批评</a>Rust的代码格式化工具rustfmt，直言其行为“完全疯狂”。</p>
<blockquote>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-36dbe77762.png"></p>
</blockquote>
<p>他指出，Rust中<code>use crate::xyz;</code>语句在自动格式化后常被压缩成单行，难以保持清晰的结构，降低了代码可读性，并增加了未来合并冲突的风险。</p>
<p>Torvalds还批评rustfmt在多行与单行格式处理上不够一致，给维护大型项目带来困难。他建议重新审视Rust的格式化规则，尤其是针对独立的<code>use</code>语句，避免将它们压缩成单行格式，以提高代码的可读性和可维护性。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-36dbe77762.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-36dbe77762.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 18:45:44 +0800</pubDate>
  </item><item>
    <title><![CDATA[FreeBSD 15.0 Beta 1 发布，优化系统性能和用户体验]]></title>
    <link>https://www.oschina.net/news/377203</link>
    <itunes:title><![CDATA[FreeBSD 15.0 Beta 1 发布，优化系统性能和用户体验]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>FreeBSD 15.0 Beta 1 已发布，此次更新引入了多个关键改进，提升了系统性能、硬件兼容性和用户体验。</p>
<p><strong>更新亮点</strong></p>
<ul>
<li> <p><strong>OpenZFS 升级至 2.4.0-rc2</strong>：增强了文件系统的稳定性和性能，支持更先进的存储特性。</p> </li>
<li> <p><strong>TCP LRO 性能修复</strong>：针对部分网络接口，优化了 TCP 大接收卸载（LRO）机制，提升了网络吞吐量。</p> </li>
<li> <p><strong>构建流程改进</strong>：</p>
<ul>
<li> <p>修复了“no-root”模式下的发布构建问题。</p> </li>
<li> <p>优化了 OCI 容器镜像、虚拟机和云镜像的构建流程。</p> </li>
<li> <p>修正了 pkgbase-repo.tar 文件的构建问题。</p> </li>
</ul> </li>
<li> <p><strong>硬件兼容性增强</strong>：</p>
<ul>
<li> <p>修复了 AHCI 控制器在无法分配 MSI-X BAR 时的附加失败问题。</p> </li>
<li> <p>改进了对新硬件的支持，特别是在笔记本电脑上的 Wi-Fi、GPU 和电源管理方面。</p> </li>
</ul> </li>
</ul>
<p>FreeBSD 15.0 正在努力改善桌面用户体验，特别是对笔记本电脑的支持。此次更新新增了在安装程序中选择 KDE Plasma 6 桌面环境的选项，提供更友好的用户界面。</p>
<p>此外，FreeBSD 还在优化对异构 CPU 核心的调度、AMD 笔记本的 S0ix 电源管理、Wi-Fi 驱动、外接 HDMI 显示器热插拔和其他电源管理功能的支持。</p>
<p>下载地址 &amp; 发布说明：<em>https://lists.freebsd.org/archives/freebsd-stable/2025-October/003383.html</em></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>FreeBSD 15.0 Beta 1 已发布，此次更新引入了多个关键改进，提升了系统性能、硬件兼容性和用户体验。</p>
<p><strong>更新亮点</strong></p>
<ul>
<li> <p><strong>OpenZFS 升级至 2.4.0-rc2</strong>：增强了文件系统的稳定性和性能，支持更先进的存储特性。</p> </li>
<li> <p><strong>TCP LRO 性能修复</strong>：针对部分网络接口，优化了 TCP 大接收卸载（LRO）机制，提升了网络吞吐量。</p> </li>
<li> <p><strong>构建流程改进</strong>：</p>
<ul>
<li> <p>修复了“no-root”模式下的发布构建问题。</p> </li>
<li> <p>优化了 OCI 容器镜像、虚拟机和云镜像的构建流程。</p> </li>
<li> <p>修正了 pkgbase-repo.tar 文件的构建问题。</p> </li>
</ul> </li>
<li> <p><strong>硬件兼容性增强</strong>：</p>
<ul>
<li> <p>修复了 AHCI 控制器在无法分配 MSI-X BAR 时的附加失败问题。</p> </li>
<li> <p>改进了对新硬件的支持，特别是在笔记本电脑上的 Wi-Fi、GPU 和电源管理方面。</p> </li>
</ul> </li>
</ul>
<p>FreeBSD 15.0 正在努力改善桌面用户体验，特别是对笔记本电脑的支持。此次更新新增了在安装程序中选择 KDE Plasma 6 桌面环境的选项，提供更友好的用户界面。</p>
<p>此外，FreeBSD 还在优化对异构 CPU 核心的调度、AMD 笔记本的 S0ix 电源管理、Wi-Fi 驱动、外接 HDMI 显示器热插拔和其他电源管理功能的支持。</p>
<p>下载地址 &amp; 发布说明：<em>https://lists.freebsd.org/archives/freebsd-stable/2025-October/003383.html</em></p>]]>
    </description>
    <content:encoded><![CDATA[<p>FreeBSD 15.0 Beta 1 已发布，此次更新引入了多个关键改进，提升了系统性能、硬件兼容性和用户体验。</p>
<p><strong>更新亮点</strong></p>
<ul>
<li> <p><strong>OpenZFS 升级至 2.4.0-rc2</strong>：增强了文件系统的稳定性和性能，支持更先进的存储特性。</p> </li>
<li> <p><strong>TCP LRO 性能修复</strong>：针对部分网络接口，优化了 TCP 大接收卸载（LRO）机制，提升了网络吞吐量。</p> </li>
<li> <p><strong>构建流程改进</strong>：</p>
<ul>
<li> <p>修复了“no-root”模式下的发布构建问题。</p> </li>
<li> <p>优化了 OCI 容器镜像、虚拟机和云镜像的构建流程。</p> </li>
<li> <p>修正了 pkgbase-repo.tar 文件的构建问题。</p> </li>
</ul> </li>
<li> <p><strong>硬件兼容性增强</strong>：</p>
<ul>
<li> <p>修复了 AHCI 控制器在无法分配 MSI-X BAR 时的附加失败问题。</p> </li>
<li> <p>改进了对新硬件的支持，特别是在笔记本电脑上的 Wi-Fi、GPU 和电源管理方面。</p> </li>
</ul> </li>
</ul>
<p>FreeBSD 15.0 正在努力改善桌面用户体验，特别是对笔记本电脑的支持。此次更新新增了在安装程序中选择 KDE Plasma 6 桌面环境的选项，提供更友好的用户界面。</p>
<p>此外，FreeBSD 还在优化对异构 CPU 核心的调度、AMD 笔记本的 S0ix 电源管理、Wi-Fi 驱动、外接 HDMI 显示器热插拔和其他电源管理功能的支持。</p>
<p>下载地址 &amp; 发布说明：<em>https://lists.freebsd.org/archives/freebsd-stable/2025-October/003383.html</em></p>]]></content:encoded>
    
    <pubDate>Mon, 13 Oct 2025 18:30:13 +0800</pubDate>
  </item><item>
    <title><![CDATA[Radical Numerics 开源 30B 参数 RND1]]></title>
    <link>https://www.oschina.net/news/377199</link>
    <itunes:title><![CDATA[Radical Numerics 开源 30B 参数 RND1]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><span>AI 研究机构 Radical Numerics 发布了 RND1-Base，这是迄今为止最大规模的开源扩散语言模型，参数规模达到30B，其中活跃参数为3B，采用稀疏专家混合架构。该模型不仅在基准测试中表现出色，还开源了完整权重、训练配方和推理代码，旨在加速扩散语言模型领域的后训练与推理研究。</span></p>
<p><span>RND1-Base基于Qwen3-30BA3B的自回归基础模型，通过简单的连续预训练实现了向扩散范式的无缝转型。转换过程采用双向掩码机制和特定于层的学习率以保留原有知识，并使用高达8M标记的大批量训练确保稳定性，最终在500B标记上完成预训练。这一高效方案避免了从零开始训练带来的资源浪费，体现了Radical Numerics在模型重用方面的创新思路。</span></p>
<p><span>与传统自回归语言模型逐个生成token的顺序模式不同，RND1将文本生成视作类似图像去噪的过程，从噪声中并行精炼整个序列，支持双向注意力机制。这不仅提升了生成的灵活性和可控性，还显著降低了推理延迟，特别适合复杂推理和代码生成任务。</span></p>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2e558ce0c2.png"></span></p>
<p><span>在通用基准测试中，RND1-Base展现出强劲实力，超越了Dream-7B和LLaDA-8B等开源扩散语言模型前辈。具体成绩包括MMLU多任务语言理解57.2%、GSM8K数学推理72.1%、MBPP代码生成51.3%。这些指标覆盖推理、STEM和编程领域，证明该模型在保持自回归基础优势的同时，实现了扩散架构的性能提升。</span></p>
<p><span>RND1的稀疏专家混合设计在30B总参数中仅激活3B参数，优化了计算效率，适合大规模部署。该模型尚未进行后训练，贪婪采样时可能偶尔出现重复，但开源代码已集成FlashInfer和SGLang后端，支持快速推理迭代。</span></p>
<p><span>Radical Numerics将自身定位为下一代AI实验室，专注于构建递归自我改进引擎。RND1正是这一愿景的产物，通过自动化AI研究平台，让模型参与优化下一代AI。该团队由来自DeepMind、Meta、Liquid和斯坦福等顶尖机构的研究员与工程师组成，目标是让AI自主设计AI，推动科学与工业发现加速。</span></p>
<p><span>开源RND1的目的在于激发社区探索扩散语言模型的推理优化和后训练潜力。当前，扩散模型在语言领域的应用正从实验阶段转向实用阶段，特别是在并行生成长序列任务方面展现出优势。业内预计，此举将刺激更多自回归模型向扩散模型转换的实验，填补开源生态在高效生成模型方面的空白。</span></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><span>AI 研究机构 Radical Numerics 发布了 RND1-Base，这是迄今为止最大规模的开源扩散语言模型，参数规模达到30B，其中活跃参数为3B，采用稀疏专家混合架构。该模型不仅在基准测试中表现出色，还开源了完整权重、训练配方和推理代码，旨在加速扩散语言模型领域的后训练与推理研究。</span></p>
<p><span>RND1-Base基于Qwen3-30BA3B的自回归基础模型，通过简单的连续预训练实现了向扩散范式的无缝转型。转换过程采用双向掩码机制和特定于层的学习率以保留原有知识，并使用高达8M标记的大批量训练确保稳定性，最终在500B标记上完成预训练。这一高效方案避免了从零开始训练带来的资源浪费，体现了Radical Numerics在模型重用方面的创新思路。</span></p>
<p><span>与传统自回归语言模型逐个生成token的顺序模式不同，RND1将文本生成视作类似图像去噪的过程，从噪声中并行精炼整个序列，支持双向注意力机制。这不仅提升了生成的灵活性和可控性，还显著降低了推理延迟，特别适合复杂推理和代码生成任务。</span></p>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2e558ce0c2.png"></span></p>
<p><span>在通用基准测试中，RND1-Base展现出强劲实力，超越了Dream-7B和LLaDA-8B等开源扩散语言模型前辈。具体成绩包括MMLU多任务语言理解57.2%、GSM8K数学推理72.1%、MBPP代码生成51.3%。这些指标覆盖推理、STEM和编程领域，证明该模型在保持自回归基础优势的同时，实现了扩散架构的性能提升。</span></p>
<p><span>RND1的稀疏专家混合设计在30B总参数中仅激活3B参数，优化了计算效率，适合大规模部署。该模型尚未进行后训练，贪婪采样时可能偶尔出现重复，但开源代码已集成FlashInfer和SGLang后端，支持快速推理迭代。</span></p>
<p><span>Radical Numerics将自身定位为下一代AI实验室，专注于构建递归自我改进引擎。RND1正是这一愿景的产物，通过自动化AI研究平台，让模型参与优化下一代AI。该团队由来自DeepMind、Meta、Liquid和斯坦福等顶尖机构的研究员与工程师组成，目标是让AI自主设计AI，推动科学与工业发现加速。</span></p>
<p><span>开源RND1的目的在于激发社区探索扩散语言模型的推理优化和后训练潜力。当前，扩散模型在语言领域的应用正从实验阶段转向实用阶段，特别是在并行生成长序列任务方面展现出优势。业内预计，此举将刺激更多自回归模型向扩散模型转换的实验，填补开源生态在高效生成模型方面的空白。</span></p>]]>
    </description>
    <content:encoded><![CDATA[<p><span>AI 研究机构 Radical Numerics 发布了 RND1-Base，这是迄今为止最大规模的开源扩散语言模型，参数规模达到30B，其中活跃参数为3B，采用稀疏专家混合架构。该模型不仅在基准测试中表现出色，还开源了完整权重、训练配方和推理代码，旨在加速扩散语言模型领域的后训练与推理研究。</span></p>
<p><span>RND1-Base基于Qwen3-30BA3B的自回归基础模型，通过简单的连续预训练实现了向扩散范式的无缝转型。转换过程采用双向掩码机制和特定于层的学习率以保留原有知识，并使用高达8M标记的大批量训练确保稳定性，最终在500B标记上完成预训练。这一高效方案避免了从零开始训练带来的资源浪费，体现了Radical Numerics在模型重用方面的创新思路。</span></p>
<p><span>与传统自回归语言模型逐个生成token的顺序模式不同，RND1将文本生成视作类似图像去噪的过程，从噪声中并行精炼整个序列，支持双向注意力机制。这不仅提升了生成的灵活性和可控性，还显著降低了推理延迟，特别适合复杂推理和代码生成任务。</span></p>
<p><span><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2e558ce0c2.png"></span></p>
<p><span>在通用基准测试中，RND1-Base展现出强劲实力，超越了Dream-7B和LLaDA-8B等开源扩散语言模型前辈。具体成绩包括MMLU多任务语言理解57.2%、GSM8K数学推理72.1%、MBPP代码生成51.3%。这些指标覆盖推理、STEM和编程领域，证明该模型在保持自回归基础优势的同时，实现了扩散架构的性能提升。</span></p>
<p><span>RND1的稀疏专家混合设计在30B总参数中仅激活3B参数，优化了计算效率，适合大规模部署。该模型尚未进行后训练，贪婪采样时可能偶尔出现重复，但开源代码已集成FlashInfer和SGLang后端，支持快速推理迭代。</span></p>
<p><span>Radical Numerics将自身定位为下一代AI实验室，专注于构建递归自我改进引擎。RND1正是这一愿景的产物，通过自动化AI研究平台，让模型参与优化下一代AI。该团队由来自DeepMind、Meta、Liquid和斯坦福等顶尖机构的研究员与工程师组成，目标是让AI自主设计AI，推动科学与工业发现加速。</span></p>
<p><span>开源RND1的目的在于激发社区探索扩散语言模型的推理优化和后训练潜力。当前，扩散模型在语言领域的应用正从实验阶段转向实用阶段，特别是在并行生成长序列任务方面展现出优势。业内预计，此举将刺激更多自回归模型向扩散模型转换的实验，填补开源生态在高效生成模型方面的空白。</span></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2e558ce0c2.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2e558ce0c2.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 18:14:05 +0800</pubDate>
  </item><item>
    <title><![CDATA[Ubuntu 26.04 LTS 代号公布：Resolute Raccoon]]></title>
    <link>https://www.oschina.net/news/377192/ubuntu-26-04-lts-codename-resolute-raccoon</link>
    <itunes:title><![CDATA[Ubuntu 26.04 LTS 代号公布：Resolute Raccoon]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>Ubuntu <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.omgubuntu.co.uk%2F2025%2F10%2Fubuntu-26-04-lts-codename-resolute-raccoon" target="_blank">公布</a>了下一代长期支持版本（LTS）的代号 ——&nbsp;<strong>Resolute Raccoon</strong>，预计将在 2026 年 4 月发布。</p>
<p>这个名称延续了 Ubuntu 的传统双词命名方式，其中 “Resolute” 代表坚定可靠，寓意此次 LTS 版本将成为稳定、安全的系统基石；“Raccoon（浣熊）” 则象征灵巧与适应力，呼应 Ubuntu 在多平台环境下的灵活性与坚韧性。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3529bbee73.png"></p>
<p>作为 LTS 版本，Ubuntu 26.04 将提供 5 年桌面支持 与 3 年硬件更新，并通过 Ubuntu Pro 获得额外安全维护。预计系统将搭载 GNOME 50 桌面环境，强化 TPM 绑定加密 安全特性，并可能更换默认媒体播放器为 ShowTime。</p>
<p>值得一提的是，“Resolute Raccoon”的代号由已故的 Debian 与 Ubuntu 资深工程师 Steve Langasek 提名，成为对这位长期贡献者的纪念。</p>
<blockquote>
<p><a href="https://www.oschina.net/news/328947/remembering-and-thanking-steve-langasek" target="_blank">Ubuntu 核心贡献者 Steve Langasek 去世</a></p>
</blockquote>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>Ubuntu <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.omgubuntu.co.uk%2F2025%2F10%2Fubuntu-26-04-lts-codename-resolute-raccoon" target="_blank">公布</a>了下一代长期支持版本（LTS）的代号 ——&nbsp;<strong>Resolute Raccoon</strong>，预计将在 2026 年 4 月发布。</p>
<p>这个名称延续了 Ubuntu 的传统双词命名方式，其中 “Resolute” 代表坚定可靠，寓意此次 LTS 版本将成为稳定、安全的系统基石；“Raccoon（浣熊）” 则象征灵巧与适应力，呼应 Ubuntu 在多平台环境下的灵活性与坚韧性。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3529bbee73.png"></p>
<p>作为 LTS 版本，Ubuntu 26.04 将提供 5 年桌面支持 与 3 年硬件更新，并通过 Ubuntu Pro 获得额外安全维护。预计系统将搭载 GNOME 50 桌面环境，强化 TPM 绑定加密 安全特性，并可能更换默认媒体播放器为 ShowTime。</p>
<p>值得一提的是，“Resolute Raccoon”的代号由已故的 Debian 与 Ubuntu 资深工程师 Steve Langasek 提名，成为对这位长期贡献者的纪念。</p>
<blockquote>
<p><a href="https://www.oschina.net/news/328947/remembering-and-thanking-steve-langasek" target="_blank">Ubuntu 核心贡献者 Steve Langasek 去世</a></p>
</blockquote>]]>
    </description>
    <content:encoded><![CDATA[<p>Ubuntu <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.omgubuntu.co.uk%2F2025%2F10%2Fubuntu-26-04-lts-codename-resolute-raccoon" target="_blank">公布</a>了下一代长期支持版本（LTS）的代号 ——&nbsp;<strong>Resolute Raccoon</strong>，预计将在 2026 年 4 月发布。</p>
<p>这个名称延续了 Ubuntu 的传统双词命名方式，其中 “Resolute” 代表坚定可靠，寓意此次 LTS 版本将成为稳定、安全的系统基石；“Raccoon（浣熊）” 则象征灵巧与适应力，呼应 Ubuntu 在多平台环境下的灵活性与坚韧性。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3529bbee73.png"></p>
<p>作为 LTS 版本，Ubuntu 26.04 将提供 5 年桌面支持 与 3 年硬件更新，并通过 Ubuntu Pro 获得额外安全维护。预计系统将搭载 GNOME 50 桌面环境，强化 TPM 绑定加密 安全特性，并可能更换默认媒体播放器为 ShowTime。</p>
<p>值得一提的是，“Resolute Raccoon”的代号由已故的 Debian 与 Ubuntu 资深工程师 Steve Langasek 提名，成为对这位长期贡献者的纪念。</p>
<blockquote>
<p><a href="https://www.oschina.net/news/328947/remembering-and-thanking-steve-langasek" target="_blank">Ubuntu 核心贡献者 Steve Langasek 去世</a></p>
</blockquote>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3529bbee73.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3529bbee73.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 17:51:37 +0800</pubDate>
  </item><item>
    <title><![CDATA[蚂蚁开源业内首个高性能扩散语言模型推理框架 dInfer]]></title>
    <link>https://www.oschina.net/news/377191</link>
    <itunes:title><![CDATA[蚂蚁开源业内首个高性能扩散语言模型推理框架 dInfer]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>蚂蚁集团正式开源业界首个高性能扩散语言模型推理框架dInfer。</p>
<p>在基准测试中，dInfer将扩散语言模型的推理速度相比于英伟达扩散模型框架Fast-dLLM提升了10.7倍；在代码生成任务HumanEval上，dInfer在单批次推理中创造了1011Tokens/秒的速度，<span>首次</span>在开源社区中实现扩散语言模型的单批次推理速度显著超越自回归模型。dInfer的工作表明，扩散语言模型具备显著的效率潜力，可以通过系统性的创新工程兑现，为通往AGI的架构路径提供极具竞争力的选项。</p>
<p>扩散语言模型，作为一种全新的范式将文本生成视为一个“从随机噪声中逐步恢复完整序列”的去噪过程，具有高度并行、全局视野、结构灵活三大优势。凭借这些优势，以蚂蚁集团和人大发布的LLaDA-MoE为代表的模型已在多个基准测试中，展现出与<span>顶尖</span>AR模型相媲美的准确性。然而在推理效率方面，dLLM理论上的强大潜能，却长期被残酷的现实“枷锁”所束缚。</p>
<p>dLLM的高效推理面临计算成本高、KV缓存失效、并行解码三大挑战。这些瓶颈使得扩散语言模型的推理速度一直不尽人意，如何打破枷锁释放扩散语言模型在推理效率上的潜能，成为整个领域亟待解决的难题。</p>
<p>dInfer是一款专为扩散语言模型设计的、算法与系统深度协同的高性能推理框架 ，可支持多种扩散语言模型，包括 LLaDA、 LLaDA-MoE、LLaDA-MoE-TD 等。</p>
<p>dInfer包含四大核心模块：模型接入（Model）、KV缓存管理器(KV-Cache Manager)，扩散迭代管理器(Iteration Manager)，和解码策略(Decoder)。这种可插拔的架构，允许开发者像搭乐高一样，进一步组合和探索不同模块的优化策略，并在统一的平台上进行标准化评测 。更重要的是，dInfer针对上述三大挑战，在每个模块中都集成了针对性的解决方案。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-42042f5607.png"></p>
<p>在配备8块NVIDIA H800GPU的节点上，dInfer的性能表现令人瞩目：</p>
<p>在与先前的dLLM推理方案Fast-dLLM的对比中，dInfer在模型效果持平的情况下，平均推理速度（avg TPS）实现了10.7倍的巨大提升(681vs63.6)；在代码生成任务HumanEval上，dInfer在单批次推理中创造了1011tokens/秒的速度；与在业界<span>顶尖</span>的推理服务框架vLLM上运行的、参数量和性能相当的AR模型Qwen2.5-3B相比，dInfer的平均推理速度是其2.5倍(681vs277) 。</p>
<p>蚂蚁集团介绍，dInfer连接了前沿研究与产业落地，标志着扩散语言模型从“理论可行”迈向“实践高效”的关键一步。此次开预案，也是诚邀全球的开发者与研究者共同探索扩散语言模型的巨大潜能，构建更加高效、开放的AI新生态。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>蚂蚁集团正式开源业界首个高性能扩散语言模型推理框架dInfer。</p>
<p>在基准测试中，dInfer将扩散语言模型的推理速度相比于英伟达扩散模型框架Fast-dLLM提升了10.7倍；在代码生成任务HumanEval上，dInfer在单批次推理中创造了1011Tokens/秒的速度，<span>首次</span>在开源社区中实现扩散语言模型的单批次推理速度显著超越自回归模型。dInfer的工作表明，扩散语言模型具备显著的效率潜力，可以通过系统性的创新工程兑现，为通往AGI的架构路径提供极具竞争力的选项。</p>
<p>扩散语言模型，作为一种全新的范式将文本生成视为一个“从随机噪声中逐步恢复完整序列”的去噪过程，具有高度并行、全局视野、结构灵活三大优势。凭借这些优势，以蚂蚁集团和人大发布的LLaDA-MoE为代表的模型已在多个基准测试中，展现出与<span>顶尖</span>AR模型相媲美的准确性。然而在推理效率方面，dLLM理论上的强大潜能，却长期被残酷的现实“枷锁”所束缚。</p>
<p>dLLM的高效推理面临计算成本高、KV缓存失效、并行解码三大挑战。这些瓶颈使得扩散语言模型的推理速度一直不尽人意，如何打破枷锁释放扩散语言模型在推理效率上的潜能，成为整个领域亟待解决的难题。</p>
<p>dInfer是一款专为扩散语言模型设计的、算法与系统深度协同的高性能推理框架 ，可支持多种扩散语言模型，包括 LLaDA、 LLaDA-MoE、LLaDA-MoE-TD 等。</p>
<p>dInfer包含四大核心模块：模型接入（Model）、KV缓存管理器(KV-Cache Manager)，扩散迭代管理器(Iteration Manager)，和解码策略(Decoder)。这种可插拔的架构，允许开发者像搭乐高一样，进一步组合和探索不同模块的优化策略，并在统一的平台上进行标准化评测 。更重要的是，dInfer针对上述三大挑战，在每个模块中都集成了针对性的解决方案。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-42042f5607.png"></p>
<p>在配备8块NVIDIA H800GPU的节点上，dInfer的性能表现令人瞩目：</p>
<p>在与先前的dLLM推理方案Fast-dLLM的对比中，dInfer在模型效果持平的情况下，平均推理速度（avg TPS）实现了10.7倍的巨大提升(681vs63.6)；在代码生成任务HumanEval上，dInfer在单批次推理中创造了1011tokens/秒的速度；与在业界<span>顶尖</span>的推理服务框架vLLM上运行的、参数量和性能相当的AR模型Qwen2.5-3B相比，dInfer的平均推理速度是其2.5倍(681vs277) 。</p>
<p>蚂蚁集团介绍，dInfer连接了前沿研究与产业落地，标志着扩散语言模型从“理论可行”迈向“实践高效”的关键一步。此次开预案，也是诚邀全球的开发者与研究者共同探索扩散语言模型的巨大潜能，构建更加高效、开放的AI新生态。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>蚂蚁集团正式开源业界首个高性能扩散语言模型推理框架dInfer。</p>
<p>在基准测试中，dInfer将扩散语言模型的推理速度相比于英伟达扩散模型框架Fast-dLLM提升了10.7倍；在代码生成任务HumanEval上，dInfer在单批次推理中创造了1011Tokens/秒的速度，<span>首次</span>在开源社区中实现扩散语言模型的单批次推理速度显著超越自回归模型。dInfer的工作表明，扩散语言模型具备显著的效率潜力，可以通过系统性的创新工程兑现，为通往AGI的架构路径提供极具竞争力的选项。</p>
<p>扩散语言模型，作为一种全新的范式将文本生成视为一个“从随机噪声中逐步恢复完整序列”的去噪过程，具有高度并行、全局视野、结构灵活三大优势。凭借这些优势，以蚂蚁集团和人大发布的LLaDA-MoE为代表的模型已在多个基准测试中，展现出与<span>顶尖</span>AR模型相媲美的准确性。然而在推理效率方面，dLLM理论上的强大潜能，却长期被残酷的现实“枷锁”所束缚。</p>
<p>dLLM的高效推理面临计算成本高、KV缓存失效、并行解码三大挑战。这些瓶颈使得扩散语言模型的推理速度一直不尽人意，如何打破枷锁释放扩散语言模型在推理效率上的潜能，成为整个领域亟待解决的难题。</p>
<p>dInfer是一款专为扩散语言模型设计的、算法与系统深度协同的高性能推理框架 ，可支持多种扩散语言模型，包括 LLaDA、 LLaDA-MoE、LLaDA-MoE-TD 等。</p>
<p>dInfer包含四大核心模块：模型接入（Model）、KV缓存管理器(KV-Cache Manager)，扩散迭代管理器(Iteration Manager)，和解码策略(Decoder)。这种可插拔的架构，允许开发者像搭乐高一样，进一步组合和探索不同模块的优化策略，并在统一的平台上进行标准化评测 。更重要的是，dInfer针对上述三大挑战，在每个模块中都集成了针对性的解决方案。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-42042f5607.png"></p>
<p>在配备8块NVIDIA H800GPU的节点上，dInfer的性能表现令人瞩目：</p>
<p>在与先前的dLLM推理方案Fast-dLLM的对比中，dInfer在模型效果持平的情况下，平均推理速度（avg TPS）实现了10.7倍的巨大提升(681vs63.6)；在代码生成任务HumanEval上，dInfer在单批次推理中创造了1011tokens/秒的速度；与在业界<span>顶尖</span>的推理服务框架vLLM上运行的、参数量和性能相当的AR模型Qwen2.5-3B相比，dInfer的平均推理速度是其2.5倍(681vs277) 。</p>
<p>蚂蚁集团介绍，dInfer连接了前沿研究与产业落地，标志着扩散语言模型从“理论可行”迈向“实践高效”的关键一步。此次开预案，也是诚邀全球的开发者与研究者共同探索扩散语言模型的巨大潜能，构建更加高效、开放的AI新生态。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-42042f5607.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-42042f5607.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 17:50:14 +0800</pubDate>
  </item><item>
    <title><![CDATA[Reflection AI 完成 20 亿美元融资，打造“开放智能”]]></title>
    <link>https://www.oschina.net/news/377187</link>
    <itunes:title><![CDATA[Reflection AI 完成 20 亿美元融资，打造“开放智能”]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>美国 AI 初创公司 Reflection AI&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Freflection_ai%2Fstatus%2F1976304405369520242" target="_blank">宣布</a>完成 20 亿美元融资，英伟达领投 8 亿美元，公司估值达到 80 亿美元。这家成立仅一年的公司目标是构建 “全民可访问的前沿开放智能（Open Intelligence）”。</p>
<p>Reflection AI 称已组建顶尖 AI 团队、打造了前沿级大模型训练平台，并筹集 20 亿美元资金用于推进开放 AI 基础设施的建设。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-15a5f822bf.png"></p>
<p>Reflection 认为，科技和科学的进步源于开放与协作。互联网、Linux 以及现代计算的底层协议皆是开放的产物。AI 的突破（如自注意力机制、下一词预测、强化学习）也得益于公开的研究分享。</p>
<p>然而，如今的 AI 前沿能力高度集中在少数封闭实验室中。如果这种趋势持续，AI 的资本、算力和人才将被垄断，形成 “封闭循环”。Reflection 希望通过开源、开放的高能力模型打破这一格局，让 AI 的基础层保持开放与可访问。</p>
<p>过去一年中，Reflection 聚集了曾参与 PaLM、Gemini、AlphaGo、AlphaCode、ChatGPT、Character AI 等项目的顶级人才。</p>
<p>他们构建了一个可训练大规模专家 Mixture-of-Experts（MoE） 模型的 LLM 与强化学习平台 —— 这一能力此前仅存在于全球顶尖实验室。团队首先在 “自主编程（autonomous coding）” 领域验证了该体系的有效性，接下来将拓展至 “通用智能体推理（agentic reasoning）”。公司还获得了可持续的商业模式支持，以便长期构建并开放发布前沿级模型。</p>
<p>Reflection AI 强调，开放智能将使安全研究更具透明性，全球研究社区都能参与风险评估与防范，而非让少数封闭实验室独断专行。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>美国 AI 初创公司 Reflection AI&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Freflection_ai%2Fstatus%2F1976304405369520242" target="_blank">宣布</a>完成 20 亿美元融资，英伟达领投 8 亿美元，公司估值达到 80 亿美元。这家成立仅一年的公司目标是构建 “全民可访问的前沿开放智能（Open Intelligence）”。</p>
<p>Reflection AI 称已组建顶尖 AI 团队、打造了前沿级大模型训练平台，并筹集 20 亿美元资金用于推进开放 AI 基础设施的建设。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-15a5f822bf.png"></p>
<p>Reflection 认为，科技和科学的进步源于开放与协作。互联网、Linux 以及现代计算的底层协议皆是开放的产物。AI 的突破（如自注意力机制、下一词预测、强化学习）也得益于公开的研究分享。</p>
<p>然而，如今的 AI 前沿能力高度集中在少数封闭实验室中。如果这种趋势持续，AI 的资本、算力和人才将被垄断，形成 “封闭循环”。Reflection 希望通过开源、开放的高能力模型打破这一格局，让 AI 的基础层保持开放与可访问。</p>
<p>过去一年中，Reflection 聚集了曾参与 PaLM、Gemini、AlphaGo、AlphaCode、ChatGPT、Character AI 等项目的顶级人才。</p>
<p>他们构建了一个可训练大规模专家 Mixture-of-Experts（MoE） 模型的 LLM 与强化学习平台 —— 这一能力此前仅存在于全球顶尖实验室。团队首先在 “自主编程（autonomous coding）” 领域验证了该体系的有效性，接下来将拓展至 “通用智能体推理（agentic reasoning）”。公司还获得了可持续的商业模式支持，以便长期构建并开放发布前沿级模型。</p>
<p>Reflection AI 强调，开放智能将使安全研究更具透明性，全球研究社区都能参与风险评估与防范，而非让少数封闭实验室独断专行。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>美国 AI 初创公司 Reflection AI&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx.com%2Freflection_ai%2Fstatus%2F1976304405369520242" target="_blank">宣布</a>完成 20 亿美元融资，英伟达领投 8 亿美元，公司估值达到 80 亿美元。这家成立仅一年的公司目标是构建 “全民可访问的前沿开放智能（Open Intelligence）”。</p>
<p>Reflection AI 称已组建顶尖 AI 团队、打造了前沿级大模型训练平台，并筹集 20 亿美元资金用于推进开放 AI 基础设施的建设。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-15a5f822bf.png"></p>
<p>Reflection 认为，科技和科学的进步源于开放与协作。互联网、Linux 以及现代计算的底层协议皆是开放的产物。AI 的突破（如自注意力机制、下一词预测、强化学习）也得益于公开的研究分享。</p>
<p>然而，如今的 AI 前沿能力高度集中在少数封闭实验室中。如果这种趋势持续，AI 的资本、算力和人才将被垄断，形成 “封闭循环”。Reflection 希望通过开源、开放的高能力模型打破这一格局，让 AI 的基础层保持开放与可访问。</p>
<p>过去一年中，Reflection 聚集了曾参与 PaLM、Gemini、AlphaGo、AlphaCode、ChatGPT、Character AI 等项目的顶级人才。</p>
<p>他们构建了一个可训练大规模专家 Mixture-of-Experts（MoE） 模型的 LLM 与强化学习平台 —— 这一能力此前仅存在于全球顶尖实验室。团队首先在 “自主编程（autonomous coding）” 领域验证了该体系的有效性，接下来将拓展至 “通用智能体推理（agentic reasoning）”。公司还获得了可持续的商业模式支持，以便长期构建并开放发布前沿级模型。</p>
<p>Reflection AI 强调，开放智能将使安全研究更具透明性，全球研究社区都能参与风险评估与防范，而非让少数封闭实验室独断专行。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-15a5f822bf.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-15a5f822bf.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 17:36:20 +0800</pubDate>
  </item><item>
    <title><![CDATA[赛力斯与字节跳动合作研发具身智能机器人技术]]></title>
    <link>https://www.oschina.net/news/377185</link>
    <itunes:title><![CDATA[赛力斯与字节跳动合作研发具身智能机器人技术]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>赛力斯集团子公司重庆凤凰技术与字节跳动旗下火山引擎签署合作协议，共同研发具身智能机器人技术。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-9c1dbcaec3.jpg"></p>
<p>据了解，双方聚焦“多模态云边协同的智能机器人决策、控制与人机增强技术”，构建“技术研发-场景验证”闭环机制。火山引擎提供AI算法、多模态模型及算力支持，赛力斯负责技术产业化落地，推动智能机器人在汽车制造等场景的应用。</p>
<p>公告指出，火山引擎将发挥其在语言、视觉、多模态及通用人工智能模型等领域的技术积累，提供智能算法与算力支持；赛力斯则依托其产业资源与实践场景，推动人工智能技术的产业化落地，共同开发更智慧、安全、可信赖的具身智能系统，使其成为未来用户的“移动智能体与生活伙伴”。</p>
<p>赛力斯此前已成立多家子公司储备机器人技术，并获重庆市政府政策支持，目标推动具身智能在智能制造、社会服务等领域的落地。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>赛力斯集团子公司重庆凤凰技术与字节跳动旗下火山引擎签署合作协议，共同研发具身智能机器人技术。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-9c1dbcaec3.jpg"></p>
<p>据了解，双方聚焦“多模态云边协同的智能机器人决策、控制与人机增强技术”，构建“技术研发-场景验证”闭环机制。火山引擎提供AI算法、多模态模型及算力支持，赛力斯负责技术产业化落地，推动智能机器人在汽车制造等场景的应用。</p>
<p>公告指出，火山引擎将发挥其在语言、视觉、多模态及通用人工智能模型等领域的技术积累，提供智能算法与算力支持；赛力斯则依托其产业资源与实践场景，推动人工智能技术的产业化落地，共同开发更智慧、安全、可信赖的具身智能系统，使其成为未来用户的“移动智能体与生活伙伴”。</p>
<p>赛力斯此前已成立多家子公司储备机器人技术，并获重庆市政府政策支持，目标推动具身智能在智能制造、社会服务等领域的落地。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>赛力斯集团子公司重庆凤凰技术与字节跳动旗下火山引擎签署合作协议，共同研发具身智能机器人技术。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-9c1dbcaec3.jpg"></p>
<p>据了解，双方聚焦“多模态云边协同的智能机器人决策、控制与人机增强技术”，构建“技术研发-场景验证”闭环机制。火山引擎提供AI算法、多模态模型及算力支持，赛力斯负责技术产业化落地，推动智能机器人在汽车制造等场景的应用。</p>
<p>公告指出，火山引擎将发挥其在语言、视觉、多模态及通用人工智能模型等领域的技术积累，提供智能算法与算力支持；赛力斯则依托其产业资源与实践场景，推动人工智能技术的产业化落地，共同开发更智慧、安全、可信赖的具身智能系统，使其成为未来用户的“移动智能体与生活伙伴”。</p>
<p>赛力斯此前已成立多家子公司储备机器人技术，并获重庆市政府政策支持，目标推动具身智能在智能制造、社会服务等领域的落地。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-9c1dbcaec3.jpg"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-9c1dbcaec3.jpg" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 17:23:56 +0800</pubDate>
  </item><item>
    <title><![CDATA[Windows 11 截图工具更新，支持即时标注]]></title>
    <link>https://www.oschina.net/news/377184</link>
    <itunes:title><![CDATA[Windows 11 截图工具更新，支持即时标注]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>微软截图功能迎来了一项重大更新，开始支持即时标注，这一改进正在向所有用户推出。</p>
<p>微软此前已经通过截图工具和画图等提供了截图和插入文本框、对象等功能，但这些都需要先保存截图或打开应用程序才能使用标注功能。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b11bbd78f.png"></p>
<p>通常情况下，当用户拖动并选择一个区域时，截图工具会自动捕获截图并将其保存到库文件夹中。</p>
<p>而随着Snipping Tool v11.2508.28.0版本的更新，用户无需保存截图或打开任何应用程序即可直接进行标注，用户只需通过Win + Shift + S快捷键进入Windows 11的截图模式。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b9e440f569.png"></p>
<p>此时截图工具不会直接捕获截图，而是会在选定区域下方显示一个浮动工具栏，其中包含多个按钮和选项。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-358719c37b.png"></p>
<p>这个浮动工具栏提供了多种功能，包括Bing反向图像搜索、将捕获的屏幕发送到Windows 11的Copilot应用程序、在屏幕上绘图、突出显示重要文本以及插入选项等。就像QQ和微信的截图那样。</p>
<p>事实上，如果保存截图并尝试使用截图工具进行编辑，也会看到这些选项，只是现在微软让在Windows 11上标注屏幕变得更加简单。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>微软截图功能迎来了一项重大更新，开始支持即时标注，这一改进正在向所有用户推出。</p>
<p>微软此前已经通过截图工具和画图等提供了截图和插入文本框、对象等功能，但这些都需要先保存截图或打开应用程序才能使用标注功能。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b11bbd78f.png"></p>
<p>通常情况下，当用户拖动并选择一个区域时，截图工具会自动捕获截图并将其保存到库文件夹中。</p>
<p>而随着Snipping Tool v11.2508.28.0版本的更新，用户无需保存截图或打开任何应用程序即可直接进行标注，用户只需通过Win + Shift + S快捷键进入Windows 11的截图模式。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b9e440f569.png"></p>
<p>此时截图工具不会直接捕获截图，而是会在选定区域下方显示一个浮动工具栏，其中包含多个按钮和选项。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-358719c37b.png"></p>
<p>这个浮动工具栏提供了多种功能，包括Bing反向图像搜索、将捕获的屏幕发送到Windows 11的Copilot应用程序、在屏幕上绘图、突出显示重要文本以及插入选项等。就像QQ和微信的截图那样。</p>
<p>事实上，如果保存截图并尝试使用截图工具进行编辑，也会看到这些选项，只是现在微软让在Windows 11上标注屏幕变得更加简单。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>微软截图功能迎来了一项重大更新，开始支持即时标注，这一改进正在向所有用户推出。</p>
<p>微软此前已经通过截图工具和画图等提供了截图和插入文本框、对象等功能，但这些都需要先保存截图或打开应用程序才能使用标注功能。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b11bbd78f.png"></p>
<p>通常情况下，当用户拖动并选择一个区域时，截图工具会自动捕获截图并将其保存到库文件夹中。</p>
<p>而随着Snipping Tool v11.2508.28.0版本的更新，用户无需保存截图或打开任何应用程序即可直接进行标注，用户只需通过Win + Shift + S快捷键进入Windows 11的截图模式。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-b9e440f569.png"></p>
<p>此时截图工具不会直接捕获截图，而是会在选定区域下方显示一个浮动工具栏，其中包含多个按钮和选项。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-358719c37b.png"></p>
<p>这个浮动工具栏提供了多种功能，包括Bing反向图像搜索、将捕获的屏幕发送到Windows 11的Copilot应用程序、在屏幕上绘图、突出显示重要文本以及插入选项等。就像QQ和微信的截图那样。</p>
<p>事实上，如果保存截图并尝试使用截图工具进行编辑，也会看到这些选项，只是现在微软让在Windows 11上标注屏幕变得更加简单。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b11bbd78f.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b11bbd78f.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 17:16:03 +0800</pubDate>
  </item><item>
    <title><![CDATA[聚焦六大功能：PostgreSQL 18 新特性深度解析]]></title>
    <link>https://my.oschina.net/u/5729420/blog/18693388</link>
    <itunes:title><![CDATA[聚焦六大功能：PostgreSQL 18 新特性深度解析]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>PostgreSQL 全球开发组于 2025 年 5 月 8 日发布了 PostgreSQL 18 的首个 Beta 版本，正式版也已于 9 月 25 日正式上线。本文 IvorySQL 社区将为大家拆解 PostgreSQL 18 的六大亮点特性。</p>
<h2>一、PG 异步 I/O（AIO）框架：迈出打破同步阻塞瓶颈的第一步</h2>
<p>PostgreSQL 18 全新引入异步 I/O 子系统。新机制允许<strong>特定场景下</strong>并行执行多个异步预读操作，CPU 无需等待数据返回即可继续推进查询，一定程度上降低了等待损耗。此框架为 PG 未来更深入更彻底的异步 I/O 性能优化奠定基础，迈出了第一步。</p>
<h3>核心提升场景</h3>
<p><strong>【当前仅实现异步读，没有实现异步写】</strong> 所有 Seq Scan 场景下通过适配过异步 I/O 的 ReadStream 设施可实现并行化顺序预读，提升 Seq Scan 的性能，效果好于原有 <code>posix_fadvice</code> 的建议性预读。尤其在云存储场景下单次阻塞读 I/O 相比本地 I/O 所需时间更长，异步 I/O 加持下的并行化预读的优势更加明显。目前异步 I/O 已支持顺序扫描、位图堆扫描和 VACUUM 操作的异步读取，早期测试显示，读取密集型查询性能可提升 2-3 倍。</p>
<p>如图所示</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-caf40f3143.png" alt="aio.drawio.png"></p>
<p>使用了异步 I/O 的 ReadStream 机制可以在收到读请求后异步地预读后续可能使用的 buffer。而在使用同步 I/O 方式在每次请求读取 buffer 时，都要等待 I/O 操作完成，这样降低了系统吞吐量。</p>
<h3>使用方法</h3>
<pre><code># - I/O -
#backend_flush_after = 0		# measured in pages, 0 disables
effective_io_concurrency = 300		# 1-1000; 0 disables issuing multiple simultaneous IO requests
maintenance_io_concurrency = 300	# 1-1000; same as effective_io_concurrency
io_max_combine_limit = 256kB		# usually 1-128 blocks (depends on OS)
# (change requires restart)
io_combine_limit = 256kB		# usually 1-128 blocks (depends on OS)
io_method = io_uring # worker, io_uring, sync
# (change requires restart)
io_max_concurrency = 128		# Max number of IOs that one process
# can execute simultaneously
# -1 sets based on shared_buffers
# (change requires restart)
#io_workers = 3				# 1-32;
</code></pre>
<p>用户可选择三种不同的 <code>io_method</code> 启用异步 I/O，分别是：</p>
<ul>
<li><code>worker</code> 若干后台 I/O workers 接收处理后端进程的 I/O 请求。</li>
<li><code>io_uring</code> Linux 系统中 io_uring 子系统通过操作系统内核线程处理 PG 的 I/O 请求。</li>
<li><code>sync</code> 满足异步 I/O 框架接口要求的同步 I/O。</li>
</ul>
<p>要启用异步 I/O，用户需要根据自身情况设定上述 GUC 参数。其中每个进程能够拥有的最大异步 I/O 句柄为 <code>io_max_concurrency</code>，用户可以将其置 <code>-1</code>，使数据库自行选择合适的值。若自行选择的值太大，则可能因为异步 I/O 所占空间太大而无法启动数据库；若自行选择的值太小则无法完全发挥出异步 I/O 性能。</p>
<p>启动数据库后，用户可通过 <code>pg_aios</code> 视图实时地获取当前系统异步 I/O 执行状况:</p>
<pre><code>postgres=# select * from pg_aios;
-[ RECORD 1 ]---+-------------------------------------------
pid             | 85834
io_id           | 14208
io_generation   | 204
state           | SUBMITTED
operation       | readv
off             | 116252672
length          | 8192
target          | smgr
handle_data_len | 1
raw_result      |
result          | UNKNOWN
target_desc     | block 14191 in file "base/5/16427"
f_sync          | f
f_localmem      | f
f_buffered      | t
...
</code></pre>
<p>各列含义详见<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.postgresql.org%2Fdocs%2F18%2Fview-pg-aios.html" target="_blank">官方文档</a>。</p>
<h3>框架设计</h3>
<p>PG 18 引入异步 I/O 框架，支持通过 GUC 参数灵活配置异步 I/O：包括实现方式（<code>io_method</code>，可选 worker、io_uring 或 sync）、并发规模（如 <code>*_io_concurrency</code>、<code>io_max_concurrency</code>）及实现相关参数（如 <code>io_workers</code>）。</p>
<p>该框架对 I/O 目标（当前支持 smgr，未来计划支持 WAL）和不同阶段、不同数据源(shared buffer/local buffer)的行为进行了抽象（通过 <code>PgAioHandleCallbacks</code> 结构），以支持后续扩展。相关内存于启动时在共享内存中分配，后续不再缩放。进程按编号访问所属异步 I/O 资源，句柄通过 generation 号标记复用。</p>
<p>目前该版本异步 I/O 主要提供对 smgr 的异步读支持，尚不支持 WAL 异步读写，smgr 的异步写入功能仍在开发中。</p>
<h3>对原有设施的修改</h3>
<ol>
<li><strong>扩展 smgr 接口</strong>：新增 <code>smgr_startreadv</code> 方法以支持异步读取。</li>
<li><strong>实现回调结构</strong>：smgr 需实现 <code>PgAioTargetInfo</code> 和 <code>PgAioHandleCallBacks</code> 回调结构。</li>
<li><strong>适配现有模块</strong>：smgr 和 buffer manager 等模块需填充异步 I/O 抽象结构以兼容框架。</li>
<li><strong>PG 临界区处理</strong>：同步 I/O 可在 PG 临界区内发起；异步 I/O 因分段执行且带回调，需移除回调中可能失败的操作（临界区内一切操作不能失败，如用<code>RelPathStr</code>替代 palloc 的<code>char*</code>字符串）以确保安全。</li>
<li><strong>优化上层接口</strong>：利用异步 I/O 改造 ReadStream 等接口，实现真预读，大幅提升顺序扫描、pg_prewarm 及 ANALYZE 等操作的 I/O 性能，效果优于原有 posix_fadvise 方案。</li>
</ol>
<h3>使用注意及未来展望</h3>
<ol>
<li>**io_uring 需要较新内核：**旧版 Linux Kernel 不支持 io_uring。某些早期版本内核虽然支持 io_uring，但功能和性能表现与新内核有一定差距。</li>
<li><strong>架构限制并发粒度</strong>：受多进程架构所限，PG 异步 I/O 期间可并行运行的计算任务较少，难以实现更细粒度的任务级异步。当前主要性能收益集中于<strong>ReadStream 顺序预读</strong>及等并行 I/O 操作。</li>
<li><strong>未来可能的性能提升</strong>：Linux io_uring 支持直接 I/O（DIO）特性，为在 PG 中启用 DIO 奠定基础。未来启用直接 I/O 可免除双重 buffer(OS 层面对 I/O 数据进行 buffer，PG 层面对 I/O 进行 buffer)以减少不必要的数据复制。在高速 NVMe 上还配合 DIO 启用<strong>IORING_SETUP_IOPOLL</strong>选项使用轮训方式检查 I/O 完成情况，还可以进一步提升性能。</li>
<li><strong>未来更多的异步 I/O 后端</strong>：除了<code>sync</code>模式，正式版的 PostgreSQL 18 仅支持 <code>worker</code> 和 <code>io_uring</code> 两种异步 I/O 后端。目前异步 I/O 框架设计已基本完备，未来版本有望支持 Windows IORing，IOCP，以及 Posix 异步 I/O 等 I/O 后端，为用户提供更多选择。</li>
</ol>
<h3>小结</h3>
<p>PostgreSQL 18 异步 I/O 框架提升数据库系统 I/O 能力，同时也增强了 PostgreSQL 架构的可扩展性，用户只需要根据自身情况修改 GUC 参数即可获取到异步 I/O 带来的好处。目前异步 I/O 框架设计基本完备，后期支持其他异步 I/O 后端也将非常方便。任意平台 Postgres 用户可以尝试 <code>io_method = worker</code>或者<code>sync</code> 若要在非官方适配 io_uring 的旧 Linux 内核发行版上使用 io_uring 后端，需要进行充分测试后再使用。</p>
<h2>二、 跳跃式扫描：让 B 树索引 “提速换挡”</h2>
<p>在 PostgreSQL 18 之前的版本中，多列 B 树索引可用于包含该索引中任意子集列的查询条件，在对起始（最左侧）列施加约束时最为高效。对前导列的等式约束，加上对第一个不带有等式约束的列的任何不等式约束，用于限制要扫描的索引部分。</p>
<p>例如，给定一个基于（a，b，c）非空字段的升序索引和查询条件 WHERE a = 5 AND b &gt;= 42 AND c &lt; 77，该索引将从具有 a = 5 和 b = 42 的第一个条目开始扫描，一直扫描到最后具有 a = 5 的条目。 具有 c &gt;= 77 的索引条目将被跳过，但它们仍需扫描。</p>
<p>原则上，这种索引可以用于对 b 和/或 c 有约束条件而对 a 没有约束条件的查询——但必须扫描整个索引，所以在大多数情况下优化器更倾向于对表进行顺序扫描表，而非利用索引扫描。</p>
<h3>核心提升场景</h3>
<p>从 PostgreSQL 18 开始：</p>
<p>如果 B 树索引扫描能够应用跳跃式扫描（SKIP SCAN），在遍历索引时应用每个列的约束，可以减少索引的读取。跳跃式扫描的工作原理是内部生成一个动态等式约束，该约束与索引列中的每个可能值相匹配。</p>
<h3>效果测试</h3>
<p>对比版本：</p>
<p><strong>PostgreSQL 17 vs PostgreSQL 18</strong></p>
<h4>表结构与索引</h4>
<pre><code>CREATE TABLE t1
(
c1 int
c2 int,
c3 float
)
WITH (fillfactor=80);
CREATE INDEX idx_t1_c1c2 ON t1(c1, c2);
</code></pre>
<h4>数据生成</h4>
<pre><code>INSERT INTO t1
SELECT (random()*1000)::int, (random()*10000)::int, random()
FROM generate_series(1,1000000) g;
</code></pre>
<h4>通过 COPY 导入数据</h4>
<pre><code>COPY t1 FROM '/.../t1.csv' WITH (FORMAT csv);
</code></pre>
<h4>查询语句 使用复合索引的第二个列</h4>
<pre><code>EXPLAIN ANALYZE SELECT * FROM t1 WHERE c2=100;
</code></pre>
<h4>PostgreSQL 17 执行计划 选择使用并行顺序扫描</h4>
<pre><code>                                                    QUERY PLAN
-------------------------------------------------------------------------------------------------------------------
Gather  (cost=1000.00..12986.33 rows=100 width=16) (actual time=1.125..76.076 rows=90 loops=1)
Workers Planned: 2
Workers Launched: 2
-&gt;  Parallel Seq Scan on t1  (cost=0.00..11976.33 rows=42 width=16) (actual time=1.414..68.624 rows=30 loops=3)
Filter: (c2 = 100)
Rows Removed by Filter: 333303
Planning Time: 0.792 ms
Execution Time: 76.165 ms
(8 rows)
</code></pre>
<p>关闭顺序扫描强制选择索引扫描，并非最优计划，执行更慢。</p>
<pre><code>                                                        QUERY PLAN
--------------------------------------------------------------------------------------------------------------------------
Index Scan using idx_t1_c1c2 on t1  (cost=0.42..18773.42 rows=100 width=16) (actual time=1.846..100.758 rows=90 loops=1)
Index Cond: (c2 = 100)
Planning Time: 0.147 ms
Execution Time: 100.806 ms
(4 rows)
</code></pre>
<h4>PostgreSQL 18 执行计划选择使用索引扫描，可以看出跳跃式扫描执行效率提升幅度非常大</h4>
<pre><code>                                                        QUERY PLAN
---------------------------------------------------------------------------------------------------------------------------
Index Scan using idx_t1_c1c2 on t1  (cost=0.42..3900.84 rows=100 width=16) (actual time=0.225..11.464 rows=90.00 loops=1)
Index Cond: (c2 = 100)
Index Searches: 1002
Buffers: shared hit=3096
Planning Time: 0.141 ms
Execution Time: 11.522 ms
(6 rows)
</code></pre>
<p>关闭索引扫描和位图扫描强制选择顺序扫描。</p>
<pre><code>postgres=# EXPLAIN ANALYZE SELECT * FROM t1 WHERE c2=100;
QUERY PLAN
----------------------------------------------------------------------------------------------------------------------
Gather  (cost=1000.00..12986.33 rows=100 width=16) (actual time=1.486..86.881 rows=90.00 loops=1)
Workers Planned: 2
Workers Launched: 2
Buffers: shared hit=6768
-&gt;  Parallel Seq Scan on t1  (cost=0.00..11976.33 rows=42 width=16) (actual time=2.758..78.712 rows=30.00 loops=3)
Filter: (c2 = 100)
Rows Removed by Filter: 333303
Buffers: shared hit=6768
Planning Time: 0.141 ms
Execution Time: 86.926 ms
(10 rows)
</code></pre>
<h3>使用注意</h3>
<p>跳跃式扫描目前只能支持等值比较条件。</p>
<h3>小结</h3>
<p>PostgreSQL 18 的索引跳跃式扫描，使得多列 BTREE 索引能够被那些仅对第二个或之后的索引列进行等值引用的查询使用，大幅减少索引扫描需要访问的条目，使其效率得到明显提升。</p>
<h2>三、 虚拟生成列：存储与计算的 “灵活平衡”</h2>
<p>PostgreSQL 18 开发体验相关的特性，聚焦于简化开发流程、提升代码灵活性，让开发者更高效地利用 PostgreSQL 能力。</p>
<p>IvorySQL 数据库长期致力于 Oracle 特性兼容，其中包含了一项虚拟列的语法兼容：</p>
<p><code>column [datatype][generated always] AS (column_expression)[VIRTUAL]</code></p>
<p>这次 PostgreSQL 18 终于也带来了虚拟列功能。虚拟列是一种不存储数据的表列，其值在查询时通过动态计算得出。与存储列相比，虚拟列节省了列存储空间，查询虚拟列值时通过计算虚拟列表达式的值作为该列的值。</p>
<h3>基本语法</h3>
<p>PostgreSQL 18 中虚拟列的语法和存储列的语法相似，新增加关键字<strong>VIRTUAL</strong>，当省略 STORED 和 VIRTUAL 关键字时默认为虚拟列。其语法如下所示：</p>
<p><code>GENERATED ALWAYS AS ( generation_expr ) [ STORED | VIRTUAL ]</code></p>
<h3>虚拟列用例</h3>
<p>虚拟列的标识是在列的限制条件中表示的，通过虚拟列的限制语法标识列为虚拟列，以下为虚拟列表的创建、查询和新增虚拟列：</p>
<pre><code>-- 创建包含虚拟列的表，其中price_with_tax为虚拟列
CREATE TABLE products (
id SERIAL PRIMARY KEY,
name TEXT NOT NULL,
price NUMERIC(10,2) NOT NULL,
tax_rate NUMERIC(5,2) DEFAULT 0.20,
price_with_tax NUMERIC(10,2) GENERATED ALWAYS AS (price * (1 + tax_rate)) VIRTUAL
);
-- 插入数据
INSERT INTO products (name, price, tax_rate)
VALUES ('Laptop', 1000.00, 0.20);
-- 查询数据（虚拟列自动计算）
SELECT name, price, tax_rate, price_with_tax FROM products;
name  |  price  | tax_rate | price_with_tax
--------+---------+----------+----------------
Laptop | 1000.00 |     0.20 |        1200.00
(1 row)
--为表添加虚拟列
ALTER TABLE products ADD COLUMN selling_price NUMERIC(10,2)
GENERATED ALWAYS AS (
price * (1 - 0.2) * (1 + tax_rate)
) virtual;
</code></pre>
<h3>实现原理浅析</h3>
<h4>虚拟列的创建</h4>
<p>创建的表中虚拟列的存储方式和普通列的存储方式类似，其列信息都存储在 pg_attribute 系统表中，其中 attgenerated 列存储生成列信息，如果该列的值为's'，表示该列为存储列。PostgreSQL 18 新增的虚拟列在该字段中的标识符为'v'，并且将虚拟列的表达式存储于 pg_attrdef 系统表中。</p>
<pre><code>--查看虚拟列信息，其attgenerated为v表示该列为虚拟列
postgres=# select * from pg_attribute where attname='price_with_tax';
-[ RECORD 1 ]--+---------------
attrelid       | 16388
attname        | price_with_tax
atttypid       | 1700
attlen         | -1
attnum         | 5
atttypmod      | 655366
attndims       | 0
attbyval       | f
attalign       | i
attstorage     | m
attcompression |
attnotnull     | f
atthasdef      | t
atthasmissing  | f
attidentity    |
attgenerated   | v
attisdropped   | f
attislocal     | t
attinhcount    | 0
attcollation   | 0
attstattarget  |
attacl         |
attoptions     |
attfdwoptions  |
attmissingval  |
--查看虚拟列表达式存储，可以看到以下表达式为虚拟列表达式
postgres=# select pg_get_expr(adbin, adrelid) from pg_attrdef where adnum = 5;
pg_get_expr
-------------------------------------
(price * ((1)::numeric + tax_rate))
(1 row)
</code></pre>
<h4>虚拟列的插入或更新</h4>
<p>由于虚拟列的数据不占据存储空间，所以任何指定更新或插入虚拟列的操作都将被限制。</p>
<pre><code>--指定插入虚拟列
INSERT INTO products (name, price, tax_rate, price_with_tax) VALUES ('Laptop', 1000.00, 0.20, 1);
ERROR:  cannot insert a non-DEFAULT value into column "price_with_tax"
DETAIL:  Column "price_with_tax" is a generated column.
--指定更新虚拟列
update products set price_with_tax = 1 where name = 'Laptop';
ERROR:  column "price_with_tax" can only be updated to DEFAULT
DETAIL:  Column "price_with_tax" is a generated column.
</code></pre>
<h4>虚拟列的查询</h4>
<p>PostgreSQL 18 中查询虚拟列的实现是在生成执行计划阶段完成。在逻辑重写优化阶段，判断查询的范围表中是否包含虚拟列，如果包含虚拟列，则将该虚拟列的表达式从 pg_attrdef 中获取出来并替换原虚拟列名。这样查询虚拟列的值就相当于计算其表达式的值，即 <code>select price_with_tax</code> 相当于 <code>select (price \* ('1'::numeric + tax_rate)) as price_with_tax</code>。可以看到以下虚拟列被替换成了其表达式：</p>
<pre><code>postgres=# explain verbose SELECT name, price, tax_rate, price_with_tax
FROM products;
QUERY PLAN
----------------------------------------------------------------------
Seq Scan on public.products  (cost=0.00..23.12 rows=750 width=76)
Output: name, price, tax_rate, (price * ('1'::numeric + tax_rate))
(2 rows)
</code></pre>
<h3>适用场景</h3>
<ul>
<li>当考虑存储空间时，可以使用虚拟列，因为虚拟列不占用磁盘空间。</li>
<li>当列的值需要根据依赖的列变化而变化时，需要使用虚拟列。因为虚拟列的值是动态获取的。</li>
<li>当虚拟列表达式简单时，可以使用虚拟列。因为查询虚拟列需消耗 CPU 资源，表达式复杂会消耗太多 CPU 资源。</li>
<li>因为 Oracle 中有虚拟列功能，更加方便 Oracle 的虚拟列迁移至 PostgreSQL 中。</li>
</ul>
<h3>待完善部分</h3>
<p>一些功能目前尚不支持，但可能会作为增量功能在后续的版本中添加：</p>
<ul>
<li>在虚拟列上创建索引或使用虚拟列。</li>
<li>虚拟列上也没有唯一约束。</li>
<li>虚拟列上的扩展统计信息。</li>
<li>虚拟列上的外键约束。</li>
<li>虚拟列上的非空约束（支持检查约束）。</li>
<li>ALTER TABLE / DROP EXPRESSION。</li>
<li>虚拟列不能具有域类型。</li>
<li>逻辑复制不支持虚拟列。</li>
</ul>
<h3>小结</h3>
<p>虚拟列与普通列和存储列有着本质的不同，因为虚拟列的值不占磁盘空间，其获取值的方式也与普通列和存储列不同，普通列或存储列需要从磁盘获取数据，而虚拟列是通过动态计算获取虚拟列的值。</p>
<h2>四、UUID 功能增强：有序性与易用性提升</h2>
<h3>传统 UUID 的无序性是其用作主键的主要痛点：</h3>
<p>传统 UUID（尤其是 v4）的完全随机性是其作为数据库主键的痛点：</p>
<ul>
<li>UUID 随机生成，插入位置不确定，导致索引树频繁分裂和重组，大幅降低写入性能。</li>
<li>破坏聚簇索引（如 InnoDB）的物理存储顺序，增加磁盘 I/O。</li>
<li>范围查询和排序效率低下，性能低下。</li>
</ul>
<h3>UUIDv7 的关键突破：时间有序性架构设计</h3>
<p>UUIDv7 通过在 UUID 的高位部分引入时间戳来解决生成 UUID 完全随机的问题，使新生成的 UUID 能够按照创建时间自然排序。这样，B 树索引可以像自增整数一样进行顺序插入，同时仍然保持 UUID 的全局唯一性和分布式生成优势。</p>
<p>该特性使 UUIDv7 作为主键具备以下突出优势：</p>
<ul>
<li>严格按照创建时间先后顺序递增。</li>
<li>减少索引碎片。</li>
<li>提高缓存命中率。</li>
<li>适合高并发插入和高效查询的场景。</li>
</ul>
<h3>UUIDv7 的结构设计</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-c53e5aa31c.png" alt="uuidv7.png"></p>
<table>
<thead>
<tr>
<th>字段</th>
<th>位数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>毫秒级 Unix 时间戳</td>
<td>48 位</td>
<td>Unix 时间戳（毫秒）</td>
</tr>
<tr>
<td>亚毫秒级时间戳分数（用于额外排序）</td>
<td>12 位</td>
<td>时间戳的微秒精度扩展</td>
</tr>
<tr>
<td>随机数</td>
<td>62 位</td>
<td>随机数或计数器</td>
</tr>
<tr>
<td>版本号</td>
<td>4 位</td>
<td>固定为 0111（v7）</td>
</tr>
<tr>
<td>变体</td>
<td>2 位</td>
<td>固定为 10（RFC 4122）</td>
</tr>
</tbody>
</table>
<p><strong>设计关键点解析：</strong></p>
<ul>
<li><strong>高精度时间前缀(48 位):</strong> 精确到毫秒的 Unix 时间戳，确保 ID 严格按时间递增（需 NTP 时钟同步）。</li>
<li><strong>尾部随机位(62 位):</strong> 保证分布式唯一性，避免 v1 版本的 MAC 地址泄漏风险。</li>
</ul>
<p><strong>有序性如何解决性能问题？</strong></p>
<ul>
<li><strong>B-Tree 索引优化:</strong> 新生成的 UUIDv7 总是大于之前的值，因此被追加到索引尾部，避免中间节点分裂。</li>
<li><strong>缓冲池友好:</strong> 顺序写入使新记录集中在少数数据页。当页写满时，数据库只需分配新页追加，减少旧页淘汰与磁盘 I/O。</li>
<li><strong>范围查询加速:</strong> 时间有序性使 WHERE id &gt; '2025-06-01' 可转化为时间戳范围过滤，大幅降低扫描范围。</li>
</ul>
<h3>如何在 PostgreSQL18 中应用 UUIDv7</h3>
<p>PostgreSQL 18 引入了多个新函数来支持 UUIDv7，方便生成、操作和提取 UUID 信息。</p>
<ol>
<li><code>uuidv7()</code>函数：用于生成新的 UUIDv7 值</li>
</ol>
<pre><code>-- 使用当前时间戳生成 UUIDv7
SELECT uuidv7();
-- 输出示例: 0197f96c-b278-7f64-a32f-dae3cabe1ff0
-- 生成 1 小时前的 UUIDv7
SELECT uuidv7(INTERVAL '-1 hour');
-- 生成 30 分钟后的 UUIDv7
SELECT uuidv7(INTERVAL '30 minutes');
</code></pre>
<ol start="2">
<li><code>uuidv4()</code>函数：作为已有函数 <code>gen_random_uuid()</code> 的别名 ，便于和 uuidv7 一起使用</li>
</ol>
<pre><code>-- 两者等价
SELECT gen_random_uuid();
SELECT uuidv4();
</code></pre>
<ol start="3">
<li><code>uuid_extract_timestamp()</code>函数 : 该函数现在支持 UUIDv7（原本只支持 UUIDv1）</li>
</ol>
<pre><code>-- 从 UUIDv7 提取时间戳
SELECT uuid_extract_timestamp(uuidv7());
-- 示例输出: 2025-09-18 12:20:49.409+00
</code></pre>
<ol start="4">
<li><code>uuid_extract_version()</code>函数：用于检测 UUID 的版本：</li>
</ol>
<pre><code>-- 检查 UUID 版本
SELECT uuid_extract_version(uuidv7());  -- 返回 7
SELECT uuid_extract_version(uuidv4());  -- 返回 4
</code></pre>
<p>PostgreSQL 数据库中使用 UUIDv7 作为主键：</p>
<pre><code>--创建带 UUIDv7 主键的表
CREATE TABLE users (
id UUID PRIMARY KEY DEFAULT uuidv7(),
username VARCHAR(50) UNIQUE NOT NULL,
email VARCHAR(100) NOT NULL,
created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
</code></pre>
<p>这样，每条新记录的 <code>id</code> 都会自动分配一个按时间戳排序的 UUID。</p>
<pre><code>--插入数据
INSERT INTO users (username, email)
VALUES ('alice', 'alice@example.com');
INSERT INTO users (username, email)
VALUES ('bob', 'bob@example.com');
-- 按 UUID 时间顺序查看
SELECT id, username, uuid_extract_timestamp(id) as uuid_timestamp
FROM users
ORDER BY id;
</code></pre>
<h3>性能优势：</h3>
<ol>
<li>UUIDv7 的时间戳顺序能显著减少页分裂和缓存失效，有效提升 B 树索引效率。</li>
</ol>
<pre><code>--创建性能测试表
CREATE TABLE performance_test (
id_v4 UUID DEFAULT uuidv4(),
id_v7 UUID DEFAULT uuidv7(),
data TEXT DEFAULT 'sample data'
);
--使用UUIDv7作为索引
CREATE INDEX idx_v4 ON performance_test (id_v4);
CREATE INDEX idx_v7 ON performance_test (id_v7);
</code></pre>
<p>批量插入后，你可以用 <code>pg_statio_user_indexes</code> 查看索引命中情况，UUIDv7 通常表现更优。</p>
<ol start="2">
<li>UUIDv7 自带时间排序，大部分场景下显著提升排序性能。</li>
</ol>
<pre><code>-- 利用 UUIDv7 自然排序
SELECT id_v7, data
FROM performance_test
ORDER BY id_v7
LIMIT 10;
</code></pre>
<p>相比 UUIDv4 的随机顺序，UUIDv7 查询结果按创建顺序返回，更直观。</p>
<h3>UUIDv7 最佳实践</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-13baf49df9.png" alt="uuidv7_2.png"></p>
<p>适合使用 UUIDv7 的场景：</p>
<ul>
<li> <p><strong>多租户应用</strong>：可用 UUIDv7 做主键，并为 <code>(tenant_id, id)</code> 创建复合索引，既保持唯一性又能按时间排序。</p> </li>
<li> <p><strong>分布式系统</strong>：多个服务可独立生成 UUIDv7，并且在全局范围内仍能保持时间顺序。</p> </li>
</ul>
<h3>限制与注意事项</h3>
<ul>
<li><strong>依赖系统时钟</strong>：需启用 NTP 等时间同步机制，避免时钟漂移。</li>
<li><strong>时间戳精度</strong>：UUIDv7 以毫秒为单位，在同一毫秒内生成多个 UUID，顺序可能无法完全反映真实创建顺序，但仍保持唯一性。</li>
<li><strong>迁移规划</strong>：从 UUIDv4 迁移到 UUIDv7 时，需要检查应用逻辑、索引和外部依赖。</li>
</ul>
<h3>小结</h3>
<p>PostgreSQL 18 对 UUIDv7 的支持，解决了 UUID 作为主键的性能瓶颈。UUIDv7 在保持全局唯一性的同时，具备类似自增整数的顺序性，使 B 树插入更高效，查询更快。</p>
<p>对于需要分布式、高并发和高性能的现代应用，UUIDv7 提供了一种兼顾唯一性和性能的实用解决方案。</p>
<h2>五、EXPLAIN 增强：直观呈现执行细节</h2>
<p>PostgreSQL 18 对 EXPLAIN 命令进行了重大升级，通过提供更丰富、更直观的执行计划信息，让数据库开发者和 DBA 能够更加轻松地进行查询性能分析与优化。</p>
<h3>自动缓冲区分析</h3>
<p>EXPLAIN ANALYZE 现在默认包含 BUFFER 统计信息，无需手动添加 BUFFERS 选项：</p>
<ul>
<li><strong>共享命中(Shared Hits)</strong>：显示从缓存中读取的数据块数量，反映内存使用效率。</li>
<li><strong>共享读取(Shared Reads)</strong>：标识必须从磁盘读取的数据块，帮助识别 I/O 瓶颈。</li>
<li><strong>共享脏块(Shared Dirtied)</strong>：针对数据修改操作，显示被更改的块数量。</li>
</ul>
<h3>精细化索引监控</h3>
<p>新增索引扫描次数统计，让开发者能够精确了解索引使用效率：</p>
<pre><code>-- 示例输出显示索引使用情况
Index Scan using orders_pkey on orders
Index Searches: 1  -- 明确显示索引查找次数
Buffers: shared hit=2 read=2
</code></pre>
<h3>增强的统计信息</h3>
<ul>
<li>支持小数行（ fractional row counts），提供更精确的行数估计。</li>
<li>为 Material、Window Aggregate、CTE 节点输出内存和磁盘使用详情。</li>
<li>在窗口函数中显示详细的参数信息。</li>
<li>为 Parallel Bitmap Heap Scan 显示 worker 缓存统计。</li>
<li>输出禁用节点。</li>
<li>输出 WAL 缓冲区信息。</li>
</ul>
<h3>基础查询分析</h3>
<pre><code>-- 创建测试表
CREATE TABLE orders (
order_id SERIAL PRIMARY KEY,
customer_id INTEGER NOT NULL,
order_date DATE NOT NULL,
total_amount DECIMAL(10, 2) NOT NULL
);
CREATE INDEX idx_orders_customer_id ON orders(customer_id);
-- 插入测试数据
INSERT INTO orders (customer_id, order_date, total_amount)
SELECT
(n % 10) + 1,
CURRENT_DATE - (n % 365),
(50 + (random() * 950))::decimal(10,2)
FROM generate_series(1, 50000) n;
-- 查看增强的执行计划
EXPLAIN ANALYZE
SELECT *,sum(total_amount) OVER (PARTITION BY customer_id)
FROM orders WHERE order_id&gt;49900;
</code></pre>
<p>执行计划：</p>
<pre><code>QUERY PLAN
----------------------------------------------------------------------------------------------------------------------------------------
WindowAgg  (cost=13.46..15.04 rows=99 width=50) (actual time=0.630..0.745 rows=100.00 loops=1)
Window: w1 AS (PARTITION BY customer_id)
Storage: Memory  Maximum Storage: 17kB
Buffers: shared hit=5 read=2
-&gt;  Sort  (cost=13.30..13.55 rows=99 width=18) (actual time=0.307..0.333 rows=100.00 loops=1)
Sort Key: customer_id
Sort Method: quicksort  Memory: 28kB
Buffers: shared hit=5 read=2
-&gt;  Index Scan using orders_pkey on orders  (cost=0.29..10.02 rows=99 width=18) (actual time=0.089..0.174 rows=100.00 loops=1)
Index Cond: (order_id &gt; 49900)
Index Searches: 1
Buffers: shared hit=2 read=2
Planning:
Buffers: shared hit=64 read=22
Planning Time: 2.080 ms
Execution Time: 1.343 ms
</code></pre>
<p>执行计划输出洞察：</p>
<ul>
<li>缓冲区使用情况（缓存命中 vs 磁盘读取）：生成执行计划时从缓存中访问了 64 个共享缓冲区，从磁盘中读取了 22 个缓冲区，执行时从缓冲区访问了 5 个共享缓冲区，从磁盘读取了 2 个缓冲区。</li>
<li>索引效率统计：执行了 1 次 orders_pkey 索引扫描，并从缓存中访问了 2 个共享缓冲区，清晰地显示了索引的使用效率。</li>
<li>窗口函数内存使用详情：使用的 17kB 磁盘空间。</li>
<li>精确的行统计信息。</li>
<li>窗口函数的详细参数。</li>
</ul>
<h3>WAL 日志分析</h3>
<pre><code>EXPLAIN (ANALYZE, WAL)
INSERT INTO orders (customer_id, order_date, total_amount)
SELECT
(n % 10) + 1,
CURRENT_DATE - (n % 365),
(50 + (random() * 950))::decimal(10,2)
FROM generate_series(1, 50000) n;
</code></pre>
<p>执行计划：</p>
<pre><code>QUERY PLAN
---------------------------------------------------------------------------------------------------------------------------------------------
Insert on orders  (cost=0.00..2000.00 rows=0 width=0) (actual time=767.116..767.118 rows=0.00 loops=1)
Buffers: shared hit=299156 read=2 dirtied=500 written=501
WAL: records=152158 bytes=10427828 buffers full=139
-&gt;  Subquery Scan on "*SELECT*"  (cost=0.00..2000.00 rows=50000 width=28) (actual time=5.742..336.699 rows=50000.00 loops=1)
Buffers: shared hit=50013
WAL: records=1516 bytes=150084 buffers full=2
-&gt;  Function Scan on generate_series n  (cost=0.00..1750.00 rows=50000 width=24) (actual time=5.460..227.650 rows=50000.00 loops=1)
Planning Time: 0.114 ms
Execution Time: 767.179 ms
</code></pre>
<p>WAL 统计：</p>
<ul>
<li>监控写入负载的日志生成量：WAL 缓冲区生成 1516 条日志，共 150084 个字节的数据。</li>
<li>诊断写入性能瓶颈：缓冲区被写满了 2 次。</li>
</ul>
<h3>并行查询优化</h3>
<pre><code>EXPLAIN (ANALYZE)
SELECT * FROM orders WHERE customer_id IN (1, 2, 3, 4, 5, 6);
</code></pre>
<p>执行计划：</p>
<pre><code>QUERY PLAN
-------------------------------------------------------------------------------------------------------------------------------------------------------
Gather  (cost=2752.40..10357.99 rows=327855 width=18) (actual time=22.375..121.296 rows=330000.00 loops=1)
Workers Planned: 2
Workers Launched: 2
Buffers: shared hit=3810
-&gt;  Parallel Bitmap Heap Scan on orders  (cost=2751.40..10029.13 rows=136606 width=18) (actual time=12.868..88.329 rows=110000.00 loops=3)
Recheck Cond: (customer_id = ANY ('{1,2,3,4,5,6}'::integer[]))
Rows Removed by Index Recheck: 53967
Heap Blocks: exact=170 lossy=566
Buffers: shared hit=3810
Worker 0:  Heap Blocks: exact=387 lossy=957
Worker 1:  Heap Blocks: exact=369 lossy=1055
-&gt;  Bitmap Index Scan on idx_orders_customer_id  (cost=0.00..2669.44 rows=327855 width=0) (actual time=21.219..21.220 rows=330000.00 loops=1)
Index Cond: (customer_id = ANY ('{1,2,3,4,5,6}'::integer[]))
Index Searches: 1
Buffers: shared hit=266
Planning:
Buffers: shared hit=30
Planning Time: 0.510 ms
Execution Time: 158.523 ms
</code></pre>
<p>并行执行效率洞察：</p>
<ul>
<li>每个工作进程的缓存统计详情：worker 0 命中 387 个精确块和 957 个有损块，worker 1 命中 369 个精确块和 1055 个有损块。</li>
<li>精确块与有损块分析：出现有损块说明可能 work_mem 太小导致 bitmap 无法精准定位元组。</li>
</ul>
<h3>技术优势与价值</h3>
<h4>即时性能诊断</h4>
<ul>
<li><strong>降低门槛</strong>：自动化的缓冲区统计让初学者快速识别 I/O 问题。</li>
<li><strong>深度洞察</strong>：为专家级用户提供更细粒度的性能数据。</li>
<li><strong>全面覆盖</strong>：单条命令获取执行计划、缓存使用、索引效率等多维信息。</li>
</ul>
<h4>优化指导</h4>
<ul>
<li><strong>索引优化</strong>：通过精确的索引使用统计，避免过度索引或索引不足。</li>
<li><strong>内存调优</strong>：根据有损块出现频率指导 work_mem 参数调整</li>
<li><strong>查询重写</strong>：基于详细的执行成本数据优化 SQL 语句结构</li>
</ul>
<h3>功能未来展望</h3>
<p>尽管 PostgreSQL 18 的 EXPLAIN 增强带来了显著改进，但仍有一些方面可以进一步完善：</p>
<ol>
<li><strong>输出可读性</strong>：随着信息量的增加，输出变得更加复杂，可能需要更好的格式化或可视化工具支持</li>
<li><strong>历史对比</strong>：缺乏直接与历史执行计划对比的内置机制，使得性能回归分析仍需依赖外部工具</li>
<li><strong>阈值警报</strong>：没有内置机制对异常值（如异常高的缓冲区读取）发出警告，需要手动分析</li>
<li><strong>执行计划可视化</strong>：文本形式的输出在复杂查询中仍难以直观理解，需要第三方工具补充</li>
</ol>
<h3>小结</h3>
<p>PostgreSQL 18 的 EXPLAIN 增强代表了数据库可观测性的重大进步。通过自动化收集关键性能指标并提供更深入的执行洞察，它显著降低了查询优化的门槛，同时为经验丰富的 DBA 提供了更强大的分析能力。</p>
<h2>六、OAuth 2.0 认证支持：筑牢数据防护壁垒</h2>
<p>在安全方面，IvorySQL 致力于加入多种国密认证功能来保障数据安全。而这次 PostgreSQL18 在身份认证方面继续加强，引入对 OAuth 2 的支持。这是一种开放标准的授权协议，用于授权一个应用程序或服务访问用户在另一个应用程序中的资源，而无需提供用户名和密码。</p>
<p>该特性主要包含以下几个核心要素：</p>
<ul>
<li><strong>OAuth2 验证器框架</strong>：提供了一个可扩展的框架，使 PostgreSQL 能够与 OAuth 2.0 提供程序集成。PostgreSQL 本身不实现具体的令牌验证算法（如 JWT 验证），而是将这项工作委托给一个外部共享库 (<code>*.so</code> 文件)。</li>
<li><strong>客户端认证支持</strong>：libpq（PostgreSQL 的 C 客户端库）现在支持 OAuth 2.0 认证流程。</li>
<li><strong>自定义验证逻辑</strong>：通过回调机制允许实现自定义的令牌验证和用户映射逻辑。</li>
</ul>
<h3>配置 OAuth 认证方式</h3>
<h4>服务端配置</h4>
<ol>
<li> <p>选择 OAuth 认证的方式与瀚高数据库选择国密认证的方式类似，需要通过在 pg_hba.conf 文件中指定 METHOD 为 oauth，开启 OAuth 认证。</p> <p>同时 OPTIONS 必须指定 issuer 和 scope 参数，除此之外还有几个可选参数：validator、map、delegate_ident_mapping，以下是一个最简配置示例：</p> <pre><code>local all test oauth issuer="http://127.0.0.1:9000" scope="openid postgre"
</code></pre> </li>
<li> <p>指定外部 OAuth 验证器，在 postgresql.conf 文件中配置新提供的 oauth_validator_libraries 参数，配置内容为 OAuth 验证器提供的库文件。</p> </li>
</ol>
<h4>客户端配置</h4>
<p>客户端在连接时需要指定以下连接参数从而实现连接：</p>
<ul>
<li>oauth_issuer：必要参数，HTTPS URL，是授权服务器的颁发者标识符。</li>
<li>oauth_client_id：必要参数，由授权服务器颁发的 OAuth 2.0 客户端标识符。</li>
<li>oauth_client_secret：可选参数，访问 OAuth 授权服务器时要使用的客户端密码。</li>
<li>oauth_scope：可选参数，发送到授权服务器的访问请求的范围，指定为 OAuth 范围标识符的空格分隔列表。</li>
</ul>
<h3>认证实现原理</h3>
<p>oauth 整体认证流程大致如下图所示：</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-c473a4e1d2.png" alt="oauth.png"></p>
<h4>客户端（libpq）</h4>
<p>PostgreSQL 实现了一个<strong>非阻塞的、基于状态机的异步网络客户端</strong>。状态机包含 <code>OAUTH_STEP_INIT</code>、<code>OAUTH_STEP_DISCOVERY</code>、<code>OAUTH_STEP_DEVICE_AUTHORIZATION</code>、<code>OAUTH_STEP_TOKEN_REQUEST</code>、<code>OAUTH_STEP_WAIT_INTERVAL</code> 这几个状态。其核心原理包含以下几个部分：</p>
<ul>
<li><strong>DISCOVERY</strong>：客户端从用户请求中获取授权服务器元信息。</li>
<li><strong>DEVICE_AUTHORIZATION</strong>：客户端向授权服务器发送请求，授权服务器返回 device_code 和 verification_uri。客户端输出信息"Visit xxxxxx and enter the code: xxxxxx"，提示用户进行操作。</li>
<li><strong>TOKEN_REQUEST 和 WAIT_INTERVAL</strong>：轮询访问授权服务器，直到用户完成授权，授权服务器返回 access_token 给客户端。</li>
<li>将获取到的 <code>access_token</code> 设置到连接对象中。<code>libpq</code> 会将它作为密码发送给 PostgreSQL 服务器，服务器端的 OAuth 验证器会负责校验这个令牌。</li>
</ul>
<h4>服务端</h4>
<p>以下 PostgreSQL 服务端处理 OAuth 认证流程，同样通过状态机实现，但要比客户端简单得多，总共分为 <code>OAUTH_STATE_INIT</code>、<code>OAUTH_STATE_ERROR</code>、<code>OAUTH_STATE_FINISHED</code> 三个状态。以下是核心步骤：</p>
<ul>
<li>首先解析客户端发送的消息，该消息格式遵循 RFC 7628 第 3.1 节部分。</li>
<li>从客户端消息中提取出纯粹的 Bearer Token，并验证其格式（是否为合法的 Base64 字符串）。</li>
<li>将提取出的令牌传递给验证器模块进行实质性的验证。
<ul>
<li>验证成功：状态转为 <code>OAUTH_STATE_FINISHED</code>，返回 <code>PG_SASL_EXCHANGE_SUCCESS</code>。进行建立连接的后续操作。</li>
<li>验证失败：生成一个符合 RFC 7628 第 3.2.2 节的 JSON 错误响应，告知客户端所需的 <code>scope</code> 和到哪里获取令牌。状态转为 <code>OAUTH_STATE_ERROR</code>，并返回 <code>PG_SASL_EXCHANGE_CONTINUE</code>，等待客户端发送最终的 <code>KVSEP</code> 来结束失败的握手。</li>
</ul> </li>
</ul>
<h4>外部验证器</h4>
<p>外部验证器通常需要处理以下事项：</p>
<ul>
<li>令牌验证：可以通过在线验证和本地验证两种方式，由验证器自行决定。在线验证下验证器通常将令牌发送到授权服务器专门的 <code>Introspection Endpoint</code>，授权服务器会返回一个 JSON 响应，告知令牌是否有效。本地验证则需要验证器内部实现一套验证流程，本地验证令牌的签名和有效期。本地验证的好处在于能够快速响应，但缺点是无法实时检测令牌撤销。</li>
<li>身份映射：在验证通过后，验证器需要提取令牌中的唯一用户标识，并转换为数据库可理解的身份标识，也就是数据库用户。</li>
<li>连接决策：如果令牌处于有效期并且存在相应的数据库用户映射关系，则以该用户的身份创建会话连接。</li>
</ul>
<h3>优缺点剖析</h3>
<p><strong>优点：</strong></p>
<ol>
<li>OAuth2 提供了现代、标准化的身份验证机制，提高了安全性。通过 OAuth2 认证，规避了传统密码认证在数据传输过程中暴露密码导致的安全风险。</li>
<li>简化了数据库用户管理，支持统一的身份策略和访问控制，提高了管理效率。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>相较于传统的密码认证，实现更加复杂，需要额外的配置和维护工作，包括 OAuth2 提供程序的设置和管理，提高了运维的复杂度。比如瀚高数据库的国密认证功能，同样在保障口令安全的同时仅需要通过非常简单的配置即可使用。</li>
<li>依赖于外部 OAuth 提供程序的可用性和可靠性，若 OAuth 提供程序出现问题，可能会影响数据库访问。</li>
<li>每次连接可能需要额外的网络请求来验证令牌，可能会增加连接建立的时间，特别是在高并发场景下。</li>
</ol>
<h3>小结</h3>
<p>PostgreSQL 18 引入的 OAuth2 支持是一个重要的安全增强功能，它允许组织使用现代的身份验证机制来保护数据库访问。通过提供灵活的验证器框架和回调机制，PostgreSQL 18 可以适应各种 OAuth2 部署场景和业务需求。</p>
<p>虽然 OAuth2 认证增加了系统的复杂性，但它提供了显著的安全优势，特别是在集中式身份管理和单点登录方面。对于希望采用现代安全最佳实践的组织来说，这是一个值得考虑的新功能。OAuth 认证并不与已有的认证方式冲突，在不希望进行繁琐的配置时，仍然可以选择类似于瀚高数据库提供的国密认证等方式。</p>
<p>在实施 OAuth2 认证时，组织应该仔细评估其需求、现有基础设施和技术能力，以确保成功部署和运维。同时应该考虑到性能、可用性和兼容性等因素，以提供最佳的用户体验和系统可靠性。</p>
<h1>总结</h1>
<p>PostgreSQL 18 凭借六大核心特性实现了性能、功能与安全性的全方位升级：</p>
<ul>
<li>异步 I/O 突破了同步阻塞瓶颈，提升读取密集型场景的吞吐量。</li>
<li>跳跃式扫描让多列 B 树索引在非前导列查询中发挥高效作用。</li>
<li>虚拟生成列在存储与计算间找到了灵活平衡，优化了开发体验。</li>
<li>UUIDv7 解决了传统 UUID 无序性带来的性能痛点，兼顾唯一性与顺序性。</li>
<li>EXPLAIN 增强为查询优化提供了更直观、细致的执行洞察。</li>
<li>OAuth 2.0 认证则为数据安全筑牢了现代防护屏障。</li>
</ul>
<p>这些特性不仅满足了当前数据库在高性能、高并发、易开发、强安全等方面的需求，也为未来在跨平台适配、功能扩展等方向奠定了坚实基础，进一步巩固了 PostgreSQL 在开源数据库领域的领先地位，为各类应用场景提供了更强大、更灵活的技术支撑。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>PostgreSQL 全球开发组于 2025 年 5 月 8 日发布了 PostgreSQL 18 的首个 Beta 版本，正式版也已于 9 月 25 日正式上线。本文 IvorySQL 社区将为大家拆解 PostgreSQL 18 的六大亮点特性。</p>
<h2>一、PG 异步 I/O（AIO）框架：迈出打破同步阻塞瓶颈的第一步</h2>
<p>PostgreSQL 18 全新引入异步 I/O 子系统。新机制允许<strong>特定场景下</strong>并行执行多个异步预读操作，CPU 无需等待数据返回即可继续推进查询，一定程度上降低了等待损耗。此框架为 PG 未来更深入更彻底的异步 I/O 性能优化奠定基础，迈出了第一步。</p>
<h3>核心提升场景</h3>
<p><strong>【当前仅实现异步读，没有实现异步写】</strong> 所有 Seq Scan 场景下通过适配过异步 I/O 的 ReadStream 设施可实现并行化顺序预读，提升 Seq Scan 的性能，效果好于原有 <code>posix_fadvice</code> 的建议性预读。尤其在云存储场景下单次阻塞读 I/O 相比本地 I/O 所需时间更长，异步 I/O 加持下的并行化预读的优势更加明显。目前异步 I/O 已支持顺序扫描、位图堆扫描和 VACUUM 操作的异步读取，早期测试显示，读取密集型查询性能可提升 2-3 倍。</p>
<p>如图所示</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-caf40f3143.png" alt="aio.drawio.png"></p>
<p>使用了异步 I/O 的 ReadStream 机制可以在收到读请求后异步地预读后续可能使用的 buffer。而在使用同步 I/O 方式在每次请求读取 buffer 时，都要等待 I/O 操作完成，这样降低了系统吞吐量。</p>
<h3>使用方法</h3>
<pre><code># - I/O -
#backend_flush_after = 0		# measured in pages, 0 disables
effective_io_concurrency = 300		# 1-1000; 0 disables issuing multiple simultaneous IO requests
maintenance_io_concurrency = 300	# 1-1000; same as effective_io_concurrency
io_max_combine_limit = 256kB		# usually 1-128 blocks (depends on OS)
# (change requires restart)
io_combine_limit = 256kB		# usually 1-128 blocks (depends on OS)
io_method = io_uring # worker, io_uring, sync
# (change requires restart)
io_max_concurrency = 128		# Max number of IOs that one process
# can execute simultaneously
# -1 sets based on shared_buffers
# (change requires restart)
#io_workers = 3				# 1-32;
</code></pre>
<p>用户可选择三种不同的 <code>io_method</code> 启用异步 I/O，分别是：</p>
<ul>
<li><code>worker</code> 若干后台 I/O workers 接收处理后端进程的 I/O 请求。</li>
<li><code>io_uring</code> Linux 系统中 io_uring 子系统通过操作系统内核线程处理 PG 的 I/O 请求。</li>
<li><code>sync</code> 满足异步 I/O 框架接口要求的同步 I/O。</li>
</ul>
<p>要启用异步 I/O，用户需要根据自身情况设定上述 GUC 参数。其中每个进程能够拥有的最大异步 I/O 句柄为 <code>io_max_concurrency</code>，用户可以将其置 <code>-1</code>，使数据库自行选择合适的值。若自行选择的值太大，则可能因为异步 I/O 所占空间太大而无法启动数据库；若自行选择的值太小则无法完全发挥出异步 I/O 性能。</p>
<p>启动数据库后，用户可通过 <code>pg_aios</code> 视图实时地获取当前系统异步 I/O 执行状况:</p>
<pre><code>postgres=# select * from pg_aios;
-[ RECORD 1 ]---+-------------------------------------------
pid             | 85834
io_id           | 14208
io_generation   | 204
state           | SUBMITTED
operation       | readv
off             | 116252672
length          | 8192
target          | smgr
handle_data_len | 1
raw_result      |
result          | UNKNOWN
target_desc     | block 14191 in file "base/5/16427"
f_sync          | f
f_localmem      | f
f_buffered      | t
...
</code></pre>
<p>各列含义详见<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.postgresql.org%2Fdocs%2F18%2Fview-pg-aios.html" target="_blank">官方文档</a>。</p>
<h3>框架设计</h3>
<p>PG 18 引入异步 I/O 框架，支持通过 GUC 参数灵活配置异步 I/O：包括实现方式（<code>io_method</code>，可选 worker、io_uring 或 sync）、并发规模（如 <code>*_io_concurrency</code>、<code>io_max_concurrency</code>）及实现相关参数（如 <code>io_workers</code>）。</p>
<p>该框架对 I/O 目标（当前支持 smgr，未来计划支持 WAL）和不同阶段、不同数据源(shared buffer/local buffer)的行为进行了抽象（通过 <code>PgAioHandleCallbacks</code> 结构），以支持后续扩展。相关内存于启动时在共享内存中分配，后续不再缩放。进程按编号访问所属异步 I/O 资源，句柄通过 generation 号标记复用。</p>
<p>目前该版本异步 I/O 主要提供对 smgr 的异步读支持，尚不支持 WAL 异步读写，smgr 的异步写入功能仍在开发中。</p>
<h3>对原有设施的修改</h3>
<ol>
<li><strong>扩展 smgr 接口</strong>：新增 <code>smgr_startreadv</code> 方法以支持异步读取。</li>
<li><strong>实现回调结构</strong>：smgr 需实现 <code>PgAioTargetInfo</code> 和 <code>PgAioHandleCallBacks</code> 回调结构。</li>
<li><strong>适配现有模块</strong>：smgr 和 buffer manager 等模块需填充异步 I/O 抽象结构以兼容框架。</li>
<li><strong>PG 临界区处理</strong>：同步 I/O 可在 PG 临界区内发起；异步 I/O 因分段执行且带回调，需移除回调中可能失败的操作（临界区内一切操作不能失败，如用<code>RelPathStr</code>替代 palloc 的<code>char*</code>字符串）以确保安全。</li>
<li><strong>优化上层接口</strong>：利用异步 I/O 改造 ReadStream 等接口，实现真预读，大幅提升顺序扫描、pg_prewarm 及 ANALYZE 等操作的 I/O 性能，效果优于原有 posix_fadvise 方案。</li>
</ol>
<h3>使用注意及未来展望</h3>
<ol>
<li>**io_uring 需要较新内核：**旧版 Linux Kernel 不支持 io_uring。某些早期版本内核虽然支持 io_uring，但功能和性能表现与新内核有一定差距。</li>
<li><strong>架构限制并发粒度</strong>：受多进程架构所限，PG 异步 I/O 期间可并行运行的计算任务较少，难以实现更细粒度的任务级异步。当前主要性能收益集中于<strong>ReadStream 顺序预读</strong>及等并行 I/O 操作。</li>
<li><strong>未来可能的性能提升</strong>：Linux io_uring 支持直接 I/O（DIO）特性，为在 PG 中启用 DIO 奠定基础。未来启用直接 I/O 可免除双重 buffer(OS 层面对 I/O 数据进行 buffer，PG 层面对 I/O 进行 buffer)以减少不必要的数据复制。在高速 NVMe 上还配合 DIO 启用<strong>IORING_SETUP_IOPOLL</strong>选项使用轮训方式检查 I/O 完成情况，还可以进一步提升性能。</li>
<li><strong>未来更多的异步 I/O 后端</strong>：除了<code>sync</code>模式，正式版的 PostgreSQL 18 仅支持 <code>worker</code> 和 <code>io_uring</code> 两种异步 I/O 后端。目前异步 I/O 框架设计已基本完备，未来版本有望支持 Windows IORing，IOCP，以及 Posix 异步 I/O 等 I/O 后端，为用户提供更多选择。</li>
</ol>
<h3>小结</h3>
<p>PostgreSQL 18 异步 I/O 框架提升数据库系统 I/O 能力，同时也增强了 PostgreSQL 架构的可扩展性，用户只需要根据自身情况修改 GUC 参数即可获取到异步 I/O 带来的好处。目前异步 I/O 框架设计基本完备，后期支持其他异步 I/O 后端也将非常方便。任意平台 Postgres 用户可以尝试 <code>io_method = worker</code>或者<code>sync</code> 若要在非官方适配 io_uring 的旧 Linux 内核发行版上使用 io_uring 后端，需要进行充分测试后再使用。</p>
<h2>二、 跳跃式扫描：让 B 树索引 “提速换挡”</h2>
<p>在 PostgreSQL 18 之前的版本中，多列 B 树索引可用于包含该索引中任意子集列的查询条件，在对起始（最左侧）列施加约束时最为高效。对前导列的等式约束，加上对第一个不带有等式约束的列的任何不等式约束，用于限制要扫描的索引部分。</p>
<p>例如，给定一个基于（a，b，c）非空字段的升序索引和查询条件 WHERE a = 5 AND b &gt;= 42 AND c &lt; 77，该索引将从具有 a = 5 和 b = 42 的第一个条目开始扫描，一直扫描到最后具有 a = 5 的条目。 具有 c &gt;= 77 的索引条目将被跳过，但它们仍需扫描。</p>
<p>原则上，这种索引可以用于对 b 和/或 c 有约束条件而对 a 没有约束条件的查询——但必须扫描整个索引，所以在大多数情况下优化器更倾向于对表进行顺序扫描表，而非利用索引扫描。</p>
<h3>核心提升场景</h3>
<p>从 PostgreSQL 18 开始：</p>
<p>如果 B 树索引扫描能够应用跳跃式扫描（SKIP SCAN），在遍历索引时应用每个列的约束，可以减少索引的读取。跳跃式扫描的工作原理是内部生成一个动态等式约束，该约束与索引列中的每个可能值相匹配。</p>
<h3>效果测试</h3>
<p>对比版本：</p>
<p><strong>PostgreSQL 17 vs PostgreSQL 18</strong></p>
<h4>表结构与索引</h4>
<pre><code>CREATE TABLE t1
(
c1 int
c2 int,
c3 float
)
WITH (fillfactor=80);
CREATE INDEX idx_t1_c1c2 ON t1(c1, c2);
</code></pre>
<h4>数据生成</h4>
<pre><code>INSERT INTO t1
SELECT (random()*1000)::int, (random()*10000)::int, random()
FROM generate_series(1,1000000) g;
</code></pre>
<h4>通过 COPY 导入数据</h4>
<pre><code>COPY t1 FROM '/.../t1.csv' WITH (FORMAT csv);
</code></pre>
<h4>查询语句 使用复合索引的第二个列</h4>
<pre><code>EXPLAIN ANALYZE SELECT * FROM t1 WHERE c2=100;
</code></pre>
<h4>PostgreSQL 17 执行计划 选择使用并行顺序扫描</h4>
<pre><code>                                                    QUERY PLAN
-------------------------------------------------------------------------------------------------------------------
Gather  (cost=1000.00..12986.33 rows=100 width=16) (actual time=1.125..76.076 rows=90 loops=1)
Workers Planned: 2
Workers Launched: 2
-&gt;  Parallel Seq Scan on t1  (cost=0.00..11976.33 rows=42 width=16) (actual time=1.414..68.624 rows=30 loops=3)
Filter: (c2 = 100)
Rows Removed by Filter: 333303
Planning Time: 0.792 ms
Execution Time: 76.165 ms
(8 rows)
</code></pre>
<p>关闭顺序扫描强制选择索引扫描，并非最优计划，执行更慢。</p>
<pre><code>                                                        QUERY PLAN
--------------------------------------------------------------------------------------------------------------------------
Index Scan using idx_t1_c1c2 on t1  (cost=0.42..18773.42 rows=100 width=16) (actual time=1.846..100.758 rows=90 loops=1)
Index Cond: (c2 = 100)
Planning Time: 0.147 ms
Execution Time: 100.806 ms
(4 rows)
</code></pre>
<h4>PostgreSQL 18 执行计划选择使用索引扫描，可以看出跳跃式扫描执行效率提升幅度非常大</h4>
<pre><code>                                                        QUERY PLAN
---------------------------------------------------------------------------------------------------------------------------
Index Scan using idx_t1_c1c2 on t1  (cost=0.42..3900.84 rows=100 width=16) (actual time=0.225..11.464 rows=90.00 loops=1)
Index Cond: (c2 = 100)
Index Searches: 1002
Buffers: shared hit=3096
Planning Time: 0.141 ms
Execution Time: 11.522 ms
(6 rows)
</code></pre>
<p>关闭索引扫描和位图扫描强制选择顺序扫描。</p>
<pre><code>postgres=# EXPLAIN ANALYZE SELECT * FROM t1 WHERE c2=100;
QUERY PLAN
----------------------------------------------------------------------------------------------------------------------
Gather  (cost=1000.00..12986.33 rows=100 width=16) (actual time=1.486..86.881 rows=90.00 loops=1)
Workers Planned: 2
Workers Launched: 2
Buffers: shared hit=6768
-&gt;  Parallel Seq Scan on t1  (cost=0.00..11976.33 rows=42 width=16) (actual time=2.758..78.712 rows=30.00 loops=3)
Filter: (c2 = 100)
Rows Removed by Filter: 333303
Buffers: shared hit=6768
Planning Time: 0.141 ms
Execution Time: 86.926 ms
(10 rows)
</code></pre>
<h3>使用注意</h3>
<p>跳跃式扫描目前只能支持等值比较条件。</p>
<h3>小结</h3>
<p>PostgreSQL 18 的索引跳跃式扫描，使得多列 BTREE 索引能够被那些仅对第二个或之后的索引列进行等值引用的查询使用，大幅减少索引扫描需要访问的条目，使其效率得到明显提升。</p>
<h2>三、 虚拟生成列：存储与计算的 “灵活平衡”</h2>
<p>PostgreSQL 18 开发体验相关的特性，聚焦于简化开发流程、提升代码灵活性，让开发者更高效地利用 PostgreSQL 能力。</p>
<p>IvorySQL 数据库长期致力于 Oracle 特性兼容，其中包含了一项虚拟列的语法兼容：</p>
<p><code>column [datatype][generated always] AS (column_expression)[VIRTUAL]</code></p>
<p>这次 PostgreSQL 18 终于也带来了虚拟列功能。虚拟列是一种不存储数据的表列，其值在查询时通过动态计算得出。与存储列相比，虚拟列节省了列存储空间，查询虚拟列值时通过计算虚拟列表达式的值作为该列的值。</p>
<h3>基本语法</h3>
<p>PostgreSQL 18 中虚拟列的语法和存储列的语法相似，新增加关键字<strong>VIRTUAL</strong>，当省略 STORED 和 VIRTUAL 关键字时默认为虚拟列。其语法如下所示：</p>
<p><code>GENERATED ALWAYS AS ( generation_expr ) [ STORED | VIRTUAL ]</code></p>
<h3>虚拟列用例</h3>
<p>虚拟列的标识是在列的限制条件中表示的，通过虚拟列的限制语法标识列为虚拟列，以下为虚拟列表的创建、查询和新增虚拟列：</p>
<pre><code>-- 创建包含虚拟列的表，其中price_with_tax为虚拟列
CREATE TABLE products (
id SERIAL PRIMARY KEY,
name TEXT NOT NULL,
price NUMERIC(10,2) NOT NULL,
tax_rate NUMERIC(5,2) DEFAULT 0.20,
price_with_tax NUMERIC(10,2) GENERATED ALWAYS AS (price * (1 + tax_rate)) VIRTUAL
);
-- 插入数据
INSERT INTO products (name, price, tax_rate)
VALUES ('Laptop', 1000.00, 0.20);
-- 查询数据（虚拟列自动计算）
SELECT name, price, tax_rate, price_with_tax FROM products;
name  |  price  | tax_rate | price_with_tax
--------+---------+----------+----------------
Laptop | 1000.00 |     0.20 |        1200.00
(1 row)
--为表添加虚拟列
ALTER TABLE products ADD COLUMN selling_price NUMERIC(10,2)
GENERATED ALWAYS AS (
price * (1 - 0.2) * (1 + tax_rate)
) virtual;
</code></pre>
<h3>实现原理浅析</h3>
<h4>虚拟列的创建</h4>
<p>创建的表中虚拟列的存储方式和普通列的存储方式类似，其列信息都存储在 pg_attribute 系统表中，其中 attgenerated 列存储生成列信息，如果该列的值为's'，表示该列为存储列。PostgreSQL 18 新增的虚拟列在该字段中的标识符为'v'，并且将虚拟列的表达式存储于 pg_attrdef 系统表中。</p>
<pre><code>--查看虚拟列信息，其attgenerated为v表示该列为虚拟列
postgres=# select * from pg_attribute where attname='price_with_tax';
-[ RECORD 1 ]--+---------------
attrelid       | 16388
attname        | price_with_tax
atttypid       | 1700
attlen         | -1
attnum         | 5
atttypmod      | 655366
attndims       | 0
attbyval       | f
attalign       | i
attstorage     | m
attcompression |
attnotnull     | f
atthasdef      | t
atthasmissing  | f
attidentity    |
attgenerated   | v
attisdropped   | f
attislocal     | t
attinhcount    | 0
attcollation   | 0
attstattarget  |
attacl         |
attoptions     |
attfdwoptions  |
attmissingval  |
--查看虚拟列表达式存储，可以看到以下表达式为虚拟列表达式
postgres=# select pg_get_expr(adbin, adrelid) from pg_attrdef where adnum = 5;
pg_get_expr
-------------------------------------
(price * ((1)::numeric + tax_rate))
(1 row)
</code></pre>
<h4>虚拟列的插入或更新</h4>
<p>由于虚拟列的数据不占据存储空间，所以任何指定更新或插入虚拟列的操作都将被限制。</p>
<pre><code>--指定插入虚拟列
INSERT INTO products (name, price, tax_rate, price_with_tax) VALUES ('Laptop', 1000.00, 0.20, 1);
ERROR:  cannot insert a non-DEFAULT value into column "price_with_tax"
DETAIL:  Column "price_with_tax" is a generated column.
--指定更新虚拟列
update products set price_with_tax = 1 where name = 'Laptop';
ERROR:  column "price_with_tax" can only be updated to DEFAULT
DETAIL:  Column "price_with_tax" is a generated column.
</code></pre>
<h4>虚拟列的查询</h4>
<p>PostgreSQL 18 中查询虚拟列的实现是在生成执行计划阶段完成。在逻辑重写优化阶段，判断查询的范围表中是否包含虚拟列，如果包含虚拟列，则将该虚拟列的表达式从 pg_attrdef 中获取出来并替换原虚拟列名。这样查询虚拟列的值就相当于计算其表达式的值，即 <code>select price_with_tax</code> 相当于 <code>select (price \* ('1'::numeric + tax_rate)) as price_with_tax</code>。可以看到以下虚拟列被替换成了其表达式：</p>
<pre><code>postgres=# explain verbose SELECT name, price, tax_rate, price_with_tax
FROM products;
QUERY PLAN
----------------------------------------------------------------------
Seq Scan on public.products  (cost=0.00..23.12 rows=750 width=76)
Output: name, price, tax_rate, (price * ('1'::numeric + tax_rate))
(2 rows)
</code></pre>
<h3>适用场景</h3>
<ul>
<li>当考虑存储空间时，可以使用虚拟列，因为虚拟列不占用磁盘空间。</li>
<li>当列的值需要根据依赖的列变化而变化时，需要使用虚拟列。因为虚拟列的值是动态获取的。</li>
<li>当虚拟列表达式简单时，可以使用虚拟列。因为查询虚拟列需消耗 CPU 资源，表达式复杂会消耗太多 CPU 资源。</li>
<li>因为 Oracle 中有虚拟列功能，更加方便 Oracle 的虚拟列迁移至 PostgreSQL 中。</li>
</ul>
<h3>待完善部分</h3>
<p>一些功能目前尚不支持，但可能会作为增量功能在后续的版本中添加：</p>
<ul>
<li>在虚拟列上创建索引或使用虚拟列。</li>
<li>虚拟列上也没有唯一约束。</li>
<li>虚拟列上的扩展统计信息。</li>
<li>虚拟列上的外键约束。</li>
<li>虚拟列上的非空约束（支持检查约束）。</li>
<li>ALTER TABLE / DROP EXPRESSION。</li>
<li>虚拟列不能具有域类型。</li>
<li>逻辑复制不支持虚拟列。</li>
</ul>
<h3>小结</h3>
<p>虚拟列与普通列和存储列有着本质的不同，因为虚拟列的值不占磁盘空间，其获取值的方式也与普通列和存储列不同，普通列或存储列需要从磁盘获取数据，而虚拟列是通过动态计算获取虚拟列的值。</p>
<h2>四、UUID 功能增强：有序性与易用性提升</h2>
<h3>传统 UUID 的无序性是其用作主键的主要痛点：</h3>
<p>传统 UUID（尤其是 v4）的完全随机性是其作为数据库主键的痛点：</p>
<ul>
<li>UUID 随机生成，插入位置不确定，导致索引树频繁分裂和重组，大幅降低写入性能。</li>
<li>破坏聚簇索引（如 InnoDB）的物理存储顺序，增加磁盘 I/O。</li>
<li>范围查询和排序效率低下，性能低下。</li>
</ul>
<h3>UUIDv7 的关键突破：时间有序性架构设计</h3>
<p>UUIDv7 通过在 UUID 的高位部分引入时间戳来解决生成 UUID 完全随机的问题，使新生成的 UUID 能够按照创建时间自然排序。这样，B 树索引可以像自增整数一样进行顺序插入，同时仍然保持 UUID 的全局唯一性和分布式生成优势。</p>
<p>该特性使 UUIDv7 作为主键具备以下突出优势：</p>
<ul>
<li>严格按照创建时间先后顺序递增。</li>
<li>减少索引碎片。</li>
<li>提高缓存命中率。</li>
<li>适合高并发插入和高效查询的场景。</li>
</ul>
<h3>UUIDv7 的结构设计</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-c53e5aa31c.png" alt="uuidv7.png"></p>
<table>
<thead>
<tr>
<th>字段</th>
<th>位数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>毫秒级 Unix 时间戳</td>
<td>48 位</td>
<td>Unix 时间戳（毫秒）</td>
</tr>
<tr>
<td>亚毫秒级时间戳分数（用于额外排序）</td>
<td>12 位</td>
<td>时间戳的微秒精度扩展</td>
</tr>
<tr>
<td>随机数</td>
<td>62 位</td>
<td>随机数或计数器</td>
</tr>
<tr>
<td>版本号</td>
<td>4 位</td>
<td>固定为 0111（v7）</td>
</tr>
<tr>
<td>变体</td>
<td>2 位</td>
<td>固定为 10（RFC 4122）</td>
</tr>
</tbody>
</table>
<p><strong>设计关键点解析：</strong></p>
<ul>
<li><strong>高精度时间前缀(48 位):</strong> 精确到毫秒的 Unix 时间戳，确保 ID 严格按时间递增（需 NTP 时钟同步）。</li>
<li><strong>尾部随机位(62 位):</strong> 保证分布式唯一性，避免 v1 版本的 MAC 地址泄漏风险。</li>
</ul>
<p><strong>有序性如何解决性能问题？</strong></p>
<ul>
<li><strong>B-Tree 索引优化:</strong> 新生成的 UUIDv7 总是大于之前的值，因此被追加到索引尾部，避免中间节点分裂。</li>
<li><strong>缓冲池友好:</strong> 顺序写入使新记录集中在少数数据页。当页写满时，数据库只需分配新页追加，减少旧页淘汰与磁盘 I/O。</li>
<li><strong>范围查询加速:</strong> 时间有序性使 WHERE id &gt; '2025-06-01' 可转化为时间戳范围过滤，大幅降低扫描范围。</li>
</ul>
<h3>如何在 PostgreSQL18 中应用 UUIDv7</h3>
<p>PostgreSQL 18 引入了多个新函数来支持 UUIDv7，方便生成、操作和提取 UUID 信息。</p>
<ol>
<li><code>uuidv7()</code>函数：用于生成新的 UUIDv7 值</li>
</ol>
<pre><code>-- 使用当前时间戳生成 UUIDv7
SELECT uuidv7();
-- 输出示例: 0197f96c-b278-7f64-a32f-dae3cabe1ff0
-- 生成 1 小时前的 UUIDv7
SELECT uuidv7(INTERVAL '-1 hour');
-- 生成 30 分钟后的 UUIDv7
SELECT uuidv7(INTERVAL '30 minutes');
</code></pre>
<ol start="2">
<li><code>uuidv4()</code>函数：作为已有函数 <code>gen_random_uuid()</code> 的别名 ，便于和 uuidv7 一起使用</li>
</ol>
<pre><code>-- 两者等价
SELECT gen_random_uuid();
SELECT uuidv4();
</code></pre>
<ol start="3">
<li><code>uuid_extract_timestamp()</code>函数 : 该函数现在支持 UUIDv7（原本只支持 UUIDv1）</li>
</ol>
<pre><code>-- 从 UUIDv7 提取时间戳
SELECT uuid_extract_timestamp(uuidv7());
-- 示例输出: 2025-09-18 12:20:49.409+00
</code></pre>
<ol start="4">
<li><code>uuid_extract_version()</code>函数：用于检测 UUID 的版本：</li>
</ol>
<pre><code>-- 检查 UUID 版本
SELECT uuid_extract_version(uuidv7());  -- 返回 7
SELECT uuid_extract_version(uuidv4());  -- 返回 4
</code></pre>
<p>PostgreSQL 数据库中使用 UUIDv7 作为主键：</p>
<pre><code>--创建带 UUIDv7 主键的表
CREATE TABLE users (
id UUID PRIMARY KEY DEFAULT uuidv7(),
username VARCHAR(50) UNIQUE NOT NULL,
email VARCHAR(100) NOT NULL,
created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
</code></pre>
<p>这样，每条新记录的 <code>id</code> 都会自动分配一个按时间戳排序的 UUID。</p>
<pre><code>--插入数据
INSERT INTO users (username, email)
VALUES ('alice', 'alice@example.com');
INSERT INTO users (username, email)
VALUES ('bob', 'bob@example.com');
-- 按 UUID 时间顺序查看
SELECT id, username, uuid_extract_timestamp(id) as uuid_timestamp
FROM users
ORDER BY id;
</code></pre>
<h3>性能优势：</h3>
<ol>
<li>UUIDv7 的时间戳顺序能显著减少页分裂和缓存失效，有效提升 B 树索引效率。</li>
</ol>
<pre><code>--创建性能测试表
CREATE TABLE performance_test (
id_v4 UUID DEFAULT uuidv4(),
id_v7 UUID DEFAULT uuidv7(),
data TEXT DEFAULT 'sample data'
);
--使用UUIDv7作为索引
CREATE INDEX idx_v4 ON performance_test (id_v4);
CREATE INDEX idx_v7 ON performance_test (id_v7);
</code></pre>
<p>批量插入后，你可以用 <code>pg_statio_user_indexes</code> 查看索引命中情况，UUIDv7 通常表现更优。</p>
<ol start="2">
<li>UUIDv7 自带时间排序，大部分场景下显著提升排序性能。</li>
</ol>
<pre><code>-- 利用 UUIDv7 自然排序
SELECT id_v7, data
FROM performance_test
ORDER BY id_v7
LIMIT 10;
</code></pre>
<p>相比 UUIDv4 的随机顺序，UUIDv7 查询结果按创建顺序返回，更直观。</p>
<h3>UUIDv7 最佳实践</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-13baf49df9.png" alt="uuidv7_2.png"></p>
<p>适合使用 UUIDv7 的场景：</p>
<ul>
<li> <p><strong>多租户应用</strong>：可用 UUIDv7 做主键，并为 <code>(tenant_id, id)</code> 创建复合索引，既保持唯一性又能按时间排序。</p> </li>
<li> <p><strong>分布式系统</strong>：多个服务可独立生成 UUIDv7，并且在全局范围内仍能保持时间顺序。</p> </li>
</ul>
<h3>限制与注意事项</h3>
<ul>
<li><strong>依赖系统时钟</strong>：需启用 NTP 等时间同步机制，避免时钟漂移。</li>
<li><strong>时间戳精度</strong>：UUIDv7 以毫秒为单位，在同一毫秒内生成多个 UUID，顺序可能无法完全反映真实创建顺序，但仍保持唯一性。</li>
<li><strong>迁移规划</strong>：从 UUIDv4 迁移到 UUIDv7 时，需要检查应用逻辑、索引和外部依赖。</li>
</ul>
<h3>小结</h3>
<p>PostgreSQL 18 对 UUIDv7 的支持，解决了 UUID 作为主键的性能瓶颈。UUIDv7 在保持全局唯一性的同时，具备类似自增整数的顺序性，使 B 树插入更高效，查询更快。</p>
<p>对于需要分布式、高并发和高性能的现代应用，UUIDv7 提供了一种兼顾唯一性和性能的实用解决方案。</p>
<h2>五、EXPLAIN 增强：直观呈现执行细节</h2>
<p>PostgreSQL 18 对 EXPLAIN 命令进行了重大升级，通过提供更丰富、更直观的执行计划信息，让数据库开发者和 DBA 能够更加轻松地进行查询性能分析与优化。</p>
<h3>自动缓冲区分析</h3>
<p>EXPLAIN ANALYZE 现在默认包含 BUFFER 统计信息，无需手动添加 BUFFERS 选项：</p>
<ul>
<li><strong>共享命中(Shared Hits)</strong>：显示从缓存中读取的数据块数量，反映内存使用效率。</li>
<li><strong>共享读取(Shared Reads)</strong>：标识必须从磁盘读取的数据块，帮助识别 I/O 瓶颈。</li>
<li><strong>共享脏块(Shared Dirtied)</strong>：针对数据修改操作，显示被更改的块数量。</li>
</ul>
<h3>精细化索引监控</h3>
<p>新增索引扫描次数统计，让开发者能够精确了解索引使用效率：</p>
<pre><code>-- 示例输出显示索引使用情况
Index Scan using orders_pkey on orders
Index Searches: 1  -- 明确显示索引查找次数
Buffers: shared hit=2 read=2
</code></pre>
<h3>增强的统计信息</h3>
<ul>
<li>支持小数行（ fractional row counts），提供更精确的行数估计。</li>
<li>为 Material、Window Aggregate、CTE 节点输出内存和磁盘使用详情。</li>
<li>在窗口函数中显示详细的参数信息。</li>
<li>为 Parallel Bitmap Heap Scan 显示 worker 缓存统计。</li>
<li>输出禁用节点。</li>
<li>输出 WAL 缓冲区信息。</li>
</ul>
<h3>基础查询分析</h3>
<pre><code>-- 创建测试表
CREATE TABLE orders (
order_id SERIAL PRIMARY KEY,
customer_id INTEGER NOT NULL,
order_date DATE NOT NULL,
total_amount DECIMAL(10, 2) NOT NULL
);
CREATE INDEX idx_orders_customer_id ON orders(customer_id);
-- 插入测试数据
INSERT INTO orders (customer_id, order_date, total_amount)
SELECT
(n % 10) + 1,
CURRENT_DATE - (n % 365),
(50 + (random() * 950))::decimal(10,2)
FROM generate_series(1, 50000) n;
-- 查看增强的执行计划
EXPLAIN ANALYZE
SELECT *,sum(total_amount) OVER (PARTITION BY customer_id)
FROM orders WHERE order_id&gt;49900;
</code></pre>
<p>执行计划：</p>
<pre><code>QUERY PLAN
----------------------------------------------------------------------------------------------------------------------------------------
WindowAgg  (cost=13.46..15.04 rows=99 width=50) (actual time=0.630..0.745 rows=100.00 loops=1)
Window: w1 AS (PARTITION BY customer_id)
Storage: Memory  Maximum Storage: 17kB
Buffers: shared hit=5 read=2
-&gt;  Sort  (cost=13.30..13.55 rows=99 width=18) (actual time=0.307..0.333 rows=100.00 loops=1)
Sort Key: customer_id
Sort Method: quicksort  Memory: 28kB
Buffers: shared hit=5 read=2
-&gt;  Index Scan using orders_pkey on orders  (cost=0.29..10.02 rows=99 width=18) (actual time=0.089..0.174 rows=100.00 loops=1)
Index Cond: (order_id &gt; 49900)
Index Searches: 1
Buffers: shared hit=2 read=2
Planning:
Buffers: shared hit=64 read=22
Planning Time: 2.080 ms
Execution Time: 1.343 ms
</code></pre>
<p>执行计划输出洞察：</p>
<ul>
<li>缓冲区使用情况（缓存命中 vs 磁盘读取）：生成执行计划时从缓存中访问了 64 个共享缓冲区，从磁盘中读取了 22 个缓冲区，执行时从缓冲区访问了 5 个共享缓冲区，从磁盘读取了 2 个缓冲区。</li>
<li>索引效率统计：执行了 1 次 orders_pkey 索引扫描，并从缓存中访问了 2 个共享缓冲区，清晰地显示了索引的使用效率。</li>
<li>窗口函数内存使用详情：使用的 17kB 磁盘空间。</li>
<li>精确的行统计信息。</li>
<li>窗口函数的详细参数。</li>
</ul>
<h3>WAL 日志分析</h3>
<pre><code>EXPLAIN (ANALYZE, WAL)
INSERT INTO orders (customer_id, order_date, total_amount)
SELECT
(n % 10) + 1,
CURRENT_DATE - (n % 365),
(50 + (random() * 950))::decimal(10,2)
FROM generate_series(1, 50000) n;
</code></pre>
<p>执行计划：</p>
<pre><code>QUERY PLAN
---------------------------------------------------------------------------------------------------------------------------------------------
Insert on orders  (cost=0.00..2000.00 rows=0 width=0) (actual time=767.116..767.118 rows=0.00 loops=1)
Buffers: shared hit=299156 read=2 dirtied=500 written=501
WAL: records=152158 bytes=10427828 buffers full=139
-&gt;  Subquery Scan on "*SELECT*"  (cost=0.00..2000.00 rows=50000 width=28) (actual time=5.742..336.699 rows=50000.00 loops=1)
Buffers: shared hit=50013
WAL: records=1516 bytes=150084 buffers full=2
-&gt;  Function Scan on generate_series n  (cost=0.00..1750.00 rows=50000 width=24) (actual time=5.460..227.650 rows=50000.00 loops=1)
Planning Time: 0.114 ms
Execution Time: 767.179 ms
</code></pre>
<p>WAL 统计：</p>
<ul>
<li>监控写入负载的日志生成量：WAL 缓冲区生成 1516 条日志，共 150084 个字节的数据。</li>
<li>诊断写入性能瓶颈：缓冲区被写满了 2 次。</li>
</ul>
<h3>并行查询优化</h3>
<pre><code>EXPLAIN (ANALYZE)
SELECT * FROM orders WHERE customer_id IN (1, 2, 3, 4, 5, 6);
</code></pre>
<p>执行计划：</p>
<pre><code>QUERY PLAN
-------------------------------------------------------------------------------------------------------------------------------------------------------
Gather  (cost=2752.40..10357.99 rows=327855 width=18) (actual time=22.375..121.296 rows=330000.00 loops=1)
Workers Planned: 2
Workers Launched: 2
Buffers: shared hit=3810
-&gt;  Parallel Bitmap Heap Scan on orders  (cost=2751.40..10029.13 rows=136606 width=18) (actual time=12.868..88.329 rows=110000.00 loops=3)
Recheck Cond: (customer_id = ANY ('{1,2,3,4,5,6}'::integer[]))
Rows Removed by Index Recheck: 53967
Heap Blocks: exact=170 lossy=566
Buffers: shared hit=3810
Worker 0:  Heap Blocks: exact=387 lossy=957
Worker 1:  Heap Blocks: exact=369 lossy=1055
-&gt;  Bitmap Index Scan on idx_orders_customer_id  (cost=0.00..2669.44 rows=327855 width=0) (actual time=21.219..21.220 rows=330000.00 loops=1)
Index Cond: (customer_id = ANY ('{1,2,3,4,5,6}'::integer[]))
Index Searches: 1
Buffers: shared hit=266
Planning:
Buffers: shared hit=30
Planning Time: 0.510 ms
Execution Time: 158.523 ms
</code></pre>
<p>并行执行效率洞察：</p>
<ul>
<li>每个工作进程的缓存统计详情：worker 0 命中 387 个精确块和 957 个有损块，worker 1 命中 369 个精确块和 1055 个有损块。</li>
<li>精确块与有损块分析：出现有损块说明可能 work_mem 太小导致 bitmap 无法精准定位元组。</li>
</ul>
<h3>技术优势与价值</h3>
<h4>即时性能诊断</h4>
<ul>
<li><strong>降低门槛</strong>：自动化的缓冲区统计让初学者快速识别 I/O 问题。</li>
<li><strong>深度洞察</strong>：为专家级用户提供更细粒度的性能数据。</li>
<li><strong>全面覆盖</strong>：单条命令获取执行计划、缓存使用、索引效率等多维信息。</li>
</ul>
<h4>优化指导</h4>
<ul>
<li><strong>索引优化</strong>：通过精确的索引使用统计，避免过度索引或索引不足。</li>
<li><strong>内存调优</strong>：根据有损块出现频率指导 work_mem 参数调整</li>
<li><strong>查询重写</strong>：基于详细的执行成本数据优化 SQL 语句结构</li>
</ul>
<h3>功能未来展望</h3>
<p>尽管 PostgreSQL 18 的 EXPLAIN 增强带来了显著改进，但仍有一些方面可以进一步完善：</p>
<ol>
<li><strong>输出可读性</strong>：随着信息量的增加，输出变得更加复杂，可能需要更好的格式化或可视化工具支持</li>
<li><strong>历史对比</strong>：缺乏直接与历史执行计划对比的内置机制，使得性能回归分析仍需依赖外部工具</li>
<li><strong>阈值警报</strong>：没有内置机制对异常值（如异常高的缓冲区读取）发出警告，需要手动分析</li>
<li><strong>执行计划可视化</strong>：文本形式的输出在复杂查询中仍难以直观理解，需要第三方工具补充</li>
</ol>
<h3>小结</h3>
<p>PostgreSQL 18 的 EXPLAIN 增强代表了数据库可观测性的重大进步。通过自动化收集关键性能指标并提供更深入的执行洞察，它显著降低了查询优化的门槛，同时为经验丰富的 DBA 提供了更强大的分析能力。</p>
<h2>六、OAuth 2.0 认证支持：筑牢数据防护壁垒</h2>
<p>在安全方面，IvorySQL 致力于加入多种国密认证功能来保障数据安全。而这次 PostgreSQL18 在身份认证方面继续加强，引入对 OAuth 2 的支持。这是一种开放标准的授权协议，用于授权一个应用程序或服务访问用户在另一个应用程序中的资源，而无需提供用户名和密码。</p>
<p>该特性主要包含以下几个核心要素：</p>
<ul>
<li><strong>OAuth2 验证器框架</strong>：提供了一个可扩展的框架，使 PostgreSQL 能够与 OAuth 2.0 提供程序集成。PostgreSQL 本身不实现具体的令牌验证算法（如 JWT 验证），而是将这项工作委托给一个外部共享库 (<code>*.so</code> 文件)。</li>
<li><strong>客户端认证支持</strong>：libpq（PostgreSQL 的 C 客户端库）现在支持 OAuth 2.0 认证流程。</li>
<li><strong>自定义验证逻辑</strong>：通过回调机制允许实现自定义的令牌验证和用户映射逻辑。</li>
</ul>
<h3>配置 OAuth 认证方式</h3>
<h4>服务端配置</h4>
<ol>
<li> <p>选择 OAuth 认证的方式与瀚高数据库选择国密认证的方式类似，需要通过在 pg_hba.conf 文件中指定 METHOD 为 oauth，开启 OAuth 认证。</p> <p>同时 OPTIONS 必须指定 issuer 和 scope 参数，除此之外还有几个可选参数：validator、map、delegate_ident_mapping，以下是一个最简配置示例：</p> <pre><code>local all test oauth issuer="http://127.0.0.1:9000" scope="openid postgre"
</code></pre> </li>
<li> <p>指定外部 OAuth 验证器，在 postgresql.conf 文件中配置新提供的 oauth_validator_libraries 参数，配置内容为 OAuth 验证器提供的库文件。</p> </li>
</ol>
<h4>客户端配置</h4>
<p>客户端在连接时需要指定以下连接参数从而实现连接：</p>
<ul>
<li>oauth_issuer：必要参数，HTTPS URL，是授权服务器的颁发者标识符。</li>
<li>oauth_client_id：必要参数，由授权服务器颁发的 OAuth 2.0 客户端标识符。</li>
<li>oauth_client_secret：可选参数，访问 OAuth 授权服务器时要使用的客户端密码。</li>
<li>oauth_scope：可选参数，发送到授权服务器的访问请求的范围，指定为 OAuth 范围标识符的空格分隔列表。</li>
</ul>
<h3>认证实现原理</h3>
<p>oauth 整体认证流程大致如下图所示：</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-c473a4e1d2.png" alt="oauth.png"></p>
<h4>客户端（libpq）</h4>
<p>PostgreSQL 实现了一个<strong>非阻塞的、基于状态机的异步网络客户端</strong>。状态机包含 <code>OAUTH_STEP_INIT</code>、<code>OAUTH_STEP_DISCOVERY</code>、<code>OAUTH_STEP_DEVICE_AUTHORIZATION</code>、<code>OAUTH_STEP_TOKEN_REQUEST</code>、<code>OAUTH_STEP_WAIT_INTERVAL</code> 这几个状态。其核心原理包含以下几个部分：</p>
<ul>
<li><strong>DISCOVERY</strong>：客户端从用户请求中获取授权服务器元信息。</li>
<li><strong>DEVICE_AUTHORIZATION</strong>：客户端向授权服务器发送请求，授权服务器返回 device_code 和 verification_uri。客户端输出信息"Visit xxxxxx and enter the code: xxxxxx"，提示用户进行操作。</li>
<li><strong>TOKEN_REQUEST 和 WAIT_INTERVAL</strong>：轮询访问授权服务器，直到用户完成授权，授权服务器返回 access_token 给客户端。</li>
<li>将获取到的 <code>access_token</code> 设置到连接对象中。<code>libpq</code> 会将它作为密码发送给 PostgreSQL 服务器，服务器端的 OAuth 验证器会负责校验这个令牌。</li>
</ul>
<h4>服务端</h4>
<p>以下 PostgreSQL 服务端处理 OAuth 认证流程，同样通过状态机实现，但要比客户端简单得多，总共分为 <code>OAUTH_STATE_INIT</code>、<code>OAUTH_STATE_ERROR</code>、<code>OAUTH_STATE_FINISHED</code> 三个状态。以下是核心步骤：</p>
<ul>
<li>首先解析客户端发送的消息，该消息格式遵循 RFC 7628 第 3.1 节部分。</li>
<li>从客户端消息中提取出纯粹的 Bearer Token，并验证其格式（是否为合法的 Base64 字符串）。</li>
<li>将提取出的令牌传递给验证器模块进行实质性的验证。
<ul>
<li>验证成功：状态转为 <code>OAUTH_STATE_FINISHED</code>，返回 <code>PG_SASL_EXCHANGE_SUCCESS</code>。进行建立连接的后续操作。</li>
<li>验证失败：生成一个符合 RFC 7628 第 3.2.2 节的 JSON 错误响应，告知客户端所需的 <code>scope</code> 和到哪里获取令牌。状态转为 <code>OAUTH_STATE_ERROR</code>，并返回 <code>PG_SASL_EXCHANGE_CONTINUE</code>，等待客户端发送最终的 <code>KVSEP</code> 来结束失败的握手。</li>
</ul> </li>
</ul>
<h4>外部验证器</h4>
<p>外部验证器通常需要处理以下事项：</p>
<ul>
<li>令牌验证：可以通过在线验证和本地验证两种方式，由验证器自行决定。在线验证下验证器通常将令牌发送到授权服务器专门的 <code>Introspection Endpoint</code>，授权服务器会返回一个 JSON 响应，告知令牌是否有效。本地验证则需要验证器内部实现一套验证流程，本地验证令牌的签名和有效期。本地验证的好处在于能够快速响应，但缺点是无法实时检测令牌撤销。</li>
<li>身份映射：在验证通过后，验证器需要提取令牌中的唯一用户标识，并转换为数据库可理解的身份标识，也就是数据库用户。</li>
<li>连接决策：如果令牌处于有效期并且存在相应的数据库用户映射关系，则以该用户的身份创建会话连接。</li>
</ul>
<h3>优缺点剖析</h3>
<p><strong>优点：</strong></p>
<ol>
<li>OAuth2 提供了现代、标准化的身份验证机制，提高了安全性。通过 OAuth2 认证，规避了传统密码认证在数据传输过程中暴露密码导致的安全风险。</li>
<li>简化了数据库用户管理，支持统一的身份策略和访问控制，提高了管理效率。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>相较于传统的密码认证，实现更加复杂，需要额外的配置和维护工作，包括 OAuth2 提供程序的设置和管理，提高了运维的复杂度。比如瀚高数据库的国密认证功能，同样在保障口令安全的同时仅需要通过非常简单的配置即可使用。</li>
<li>依赖于外部 OAuth 提供程序的可用性和可靠性，若 OAuth 提供程序出现问题，可能会影响数据库访问。</li>
<li>每次连接可能需要额外的网络请求来验证令牌，可能会增加连接建立的时间，特别是在高并发场景下。</li>
</ol>
<h3>小结</h3>
<p>PostgreSQL 18 引入的 OAuth2 支持是一个重要的安全增强功能，它允许组织使用现代的身份验证机制来保护数据库访问。通过提供灵活的验证器框架和回调机制，PostgreSQL 18 可以适应各种 OAuth2 部署场景和业务需求。</p>
<p>虽然 OAuth2 认证增加了系统的复杂性，但它提供了显著的安全优势，特别是在集中式身份管理和单点登录方面。对于希望采用现代安全最佳实践的组织来说，这是一个值得考虑的新功能。OAuth 认证并不与已有的认证方式冲突，在不希望进行繁琐的配置时，仍然可以选择类似于瀚高数据库提供的国密认证等方式。</p>
<p>在实施 OAuth2 认证时，组织应该仔细评估其需求、现有基础设施和技术能力，以确保成功部署和运维。同时应该考虑到性能、可用性和兼容性等因素，以提供最佳的用户体验和系统可靠性。</p>
<h1>总结</h1>
<p>PostgreSQL 18 凭借六大核心特性实现了性能、功能与安全性的全方位升级：</p>
<ul>
<li>异步 I/O 突破了同步阻塞瓶颈，提升读取密集型场景的吞吐量。</li>
<li>跳跃式扫描让多列 B 树索引在非前导列查询中发挥高效作用。</li>
<li>虚拟生成列在存储与计算间找到了灵活平衡，优化了开发体验。</li>
<li>UUIDv7 解决了传统 UUID 无序性带来的性能痛点，兼顾唯一性与顺序性。</li>
<li>EXPLAIN 增强为查询优化提供了更直观、细致的执行洞察。</li>
<li>OAuth 2.0 认证则为数据安全筑牢了现代防护屏障。</li>
</ul>
<p>这些特性不仅满足了当前数据库在高性能、高并发、易开发、强安全等方面的需求，也为未来在跨平台适配、功能扩展等方向奠定了坚实基础，进一步巩固了 PostgreSQL 在开源数据库领域的领先地位，为各类应用场景提供了更强大、更灵活的技术支撑。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>PostgreSQL 全球开发组于 2025 年 5 月 8 日发布了 PostgreSQL 18 的首个 Beta 版本，正式版也已于 9 月 25 日正式上线。本文 IvorySQL 社区将为大家拆解 PostgreSQL 18 的六大亮点特性。</p>
<h2>一、PG 异步 I/O（AIO）框架：迈出打破同步阻塞瓶颈的第一步</h2>
<p>PostgreSQL 18 全新引入异步 I/O 子系统。新机制允许<strong>特定场景下</strong>并行执行多个异步预读操作，CPU 无需等待数据返回即可继续推进查询，一定程度上降低了等待损耗。此框架为 PG 未来更深入更彻底的异步 I/O 性能优化奠定基础，迈出了第一步。</p>
<h3>核心提升场景</h3>
<p><strong>【当前仅实现异步读，没有实现异步写】</strong> 所有 Seq Scan 场景下通过适配过异步 I/O 的 ReadStream 设施可实现并行化顺序预读，提升 Seq Scan 的性能，效果好于原有 <code>posix_fadvice</code> 的建议性预读。尤其在云存储场景下单次阻塞读 I/O 相比本地 I/O 所需时间更长，异步 I/O 加持下的并行化预读的优势更加明显。目前异步 I/O 已支持顺序扫描、位图堆扫描和 VACUUM 操作的异步读取，早期测试显示，读取密集型查询性能可提升 2-3 倍。</p>
<p>如图所示</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-caf40f3143.png" alt="aio.drawio.png"></p>
<p>使用了异步 I/O 的 ReadStream 机制可以在收到读请求后异步地预读后续可能使用的 buffer。而在使用同步 I/O 方式在每次请求读取 buffer 时，都要等待 I/O 操作完成，这样降低了系统吞吐量。</p>
<h3>使用方法</h3>
<pre><code># - I/O -
#backend_flush_after = 0		# measured in pages, 0 disables
effective_io_concurrency = 300		# 1-1000; 0 disables issuing multiple simultaneous IO requests
maintenance_io_concurrency = 300	# 1-1000; same as effective_io_concurrency
io_max_combine_limit = 256kB		# usually 1-128 blocks (depends on OS)
# (change requires restart)
io_combine_limit = 256kB		# usually 1-128 blocks (depends on OS)
io_method = io_uring # worker, io_uring, sync
# (change requires restart)
io_max_concurrency = 128		# Max number of IOs that one process
# can execute simultaneously
# -1 sets based on shared_buffers
# (change requires restart)
#io_workers = 3				# 1-32;
</code></pre>
<p>用户可选择三种不同的 <code>io_method</code> 启用异步 I/O，分别是：</p>
<ul>
<li><code>worker</code> 若干后台 I/O workers 接收处理后端进程的 I/O 请求。</li>
<li><code>io_uring</code> Linux 系统中 io_uring 子系统通过操作系统内核线程处理 PG 的 I/O 请求。</li>
<li><code>sync</code> 满足异步 I/O 框架接口要求的同步 I/O。</li>
</ul>
<p>要启用异步 I/O，用户需要根据自身情况设定上述 GUC 参数。其中每个进程能够拥有的最大异步 I/O 句柄为 <code>io_max_concurrency</code>，用户可以将其置 <code>-1</code>，使数据库自行选择合适的值。若自行选择的值太大，则可能因为异步 I/O 所占空间太大而无法启动数据库；若自行选择的值太小则无法完全发挥出异步 I/O 性能。</p>
<p>启动数据库后，用户可通过 <code>pg_aios</code> 视图实时地获取当前系统异步 I/O 执行状况:</p>
<pre><code>postgres=# select * from pg_aios;
-[ RECORD 1 ]---+-------------------------------------------
pid             | 85834
io_id           | 14208
io_generation   | 204
state           | SUBMITTED
operation       | readv
off             | 116252672
length          | 8192
target          | smgr
handle_data_len | 1
raw_result      |
result          | UNKNOWN
target_desc     | block 14191 in file "base/5/16427"
f_sync          | f
f_localmem      | f
f_buffered      | t
...
</code></pre>
<p>各列含义详见<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.postgresql.org%2Fdocs%2F18%2Fview-pg-aios.html" target="_blank">官方文档</a>。</p>
<h3>框架设计</h3>
<p>PG 18 引入异步 I/O 框架，支持通过 GUC 参数灵活配置异步 I/O：包括实现方式（<code>io_method</code>，可选 worker、io_uring 或 sync）、并发规模（如 <code>*_io_concurrency</code>、<code>io_max_concurrency</code>）及实现相关参数（如 <code>io_workers</code>）。</p>
<p>该框架对 I/O 目标（当前支持 smgr，未来计划支持 WAL）和不同阶段、不同数据源(shared buffer/local buffer)的行为进行了抽象（通过 <code>PgAioHandleCallbacks</code> 结构），以支持后续扩展。相关内存于启动时在共享内存中分配，后续不再缩放。进程按编号访问所属异步 I/O 资源，句柄通过 generation 号标记复用。</p>
<p>目前该版本异步 I/O 主要提供对 smgr 的异步读支持，尚不支持 WAL 异步读写，smgr 的异步写入功能仍在开发中。</p>
<h3>对原有设施的修改</h3>
<ol>
<li><strong>扩展 smgr 接口</strong>：新增 <code>smgr_startreadv</code> 方法以支持异步读取。</li>
<li><strong>实现回调结构</strong>：smgr 需实现 <code>PgAioTargetInfo</code> 和 <code>PgAioHandleCallBacks</code> 回调结构。</li>
<li><strong>适配现有模块</strong>：smgr 和 buffer manager 等模块需填充异步 I/O 抽象结构以兼容框架。</li>
<li><strong>PG 临界区处理</strong>：同步 I/O 可在 PG 临界区内发起；异步 I/O 因分段执行且带回调，需移除回调中可能失败的操作（临界区内一切操作不能失败，如用<code>RelPathStr</code>替代 palloc 的<code>char*</code>字符串）以确保安全。</li>
<li><strong>优化上层接口</strong>：利用异步 I/O 改造 ReadStream 等接口，实现真预读，大幅提升顺序扫描、pg_prewarm 及 ANALYZE 等操作的 I/O 性能，效果优于原有 posix_fadvise 方案。</li>
</ol>
<h3>使用注意及未来展望</h3>
<ol>
<li>**io_uring 需要较新内核：**旧版 Linux Kernel 不支持 io_uring。某些早期版本内核虽然支持 io_uring，但功能和性能表现与新内核有一定差距。</li>
<li><strong>架构限制并发粒度</strong>：受多进程架构所限，PG 异步 I/O 期间可并行运行的计算任务较少，难以实现更细粒度的任务级异步。当前主要性能收益集中于<strong>ReadStream 顺序预读</strong>及等并行 I/O 操作。</li>
<li><strong>未来可能的性能提升</strong>：Linux io_uring 支持直接 I/O（DIO）特性，为在 PG 中启用 DIO 奠定基础。未来启用直接 I/O 可免除双重 buffer(OS 层面对 I/O 数据进行 buffer，PG 层面对 I/O 进行 buffer)以减少不必要的数据复制。在高速 NVMe 上还配合 DIO 启用<strong>IORING_SETUP_IOPOLL</strong>选项使用轮训方式检查 I/O 完成情况，还可以进一步提升性能。</li>
<li><strong>未来更多的异步 I/O 后端</strong>：除了<code>sync</code>模式，正式版的 PostgreSQL 18 仅支持 <code>worker</code> 和 <code>io_uring</code> 两种异步 I/O 后端。目前异步 I/O 框架设计已基本完备，未来版本有望支持 Windows IORing，IOCP，以及 Posix 异步 I/O 等 I/O 后端，为用户提供更多选择。</li>
</ol>
<h3>小结</h3>
<p>PostgreSQL 18 异步 I/O 框架提升数据库系统 I/O 能力，同时也增强了 PostgreSQL 架构的可扩展性，用户只需要根据自身情况修改 GUC 参数即可获取到异步 I/O 带来的好处。目前异步 I/O 框架设计基本完备，后期支持其他异步 I/O 后端也将非常方便。任意平台 Postgres 用户可以尝试 <code>io_method = worker</code>或者<code>sync</code> 若要在非官方适配 io_uring 的旧 Linux 内核发行版上使用 io_uring 后端，需要进行充分测试后再使用。</p>
<h2>二、 跳跃式扫描：让 B 树索引 “提速换挡”</h2>
<p>在 PostgreSQL 18 之前的版本中，多列 B 树索引可用于包含该索引中任意子集列的查询条件，在对起始（最左侧）列施加约束时最为高效。对前导列的等式约束，加上对第一个不带有等式约束的列的任何不等式约束，用于限制要扫描的索引部分。</p>
<p>例如，给定一个基于（a，b，c）非空字段的升序索引和查询条件 WHERE a = 5 AND b &gt;= 42 AND c &lt; 77，该索引将从具有 a = 5 和 b = 42 的第一个条目开始扫描，一直扫描到最后具有 a = 5 的条目。 具有 c &gt;= 77 的索引条目将被跳过，但它们仍需扫描。</p>
<p>原则上，这种索引可以用于对 b 和/或 c 有约束条件而对 a 没有约束条件的查询——但必须扫描整个索引，所以在大多数情况下优化器更倾向于对表进行顺序扫描表，而非利用索引扫描。</p>
<h3>核心提升场景</h3>
<p>从 PostgreSQL 18 开始：</p>
<p>如果 B 树索引扫描能够应用跳跃式扫描（SKIP SCAN），在遍历索引时应用每个列的约束，可以减少索引的读取。跳跃式扫描的工作原理是内部生成一个动态等式约束，该约束与索引列中的每个可能值相匹配。</p>
<h3>效果测试</h3>
<p>对比版本：</p>
<p><strong>PostgreSQL 17 vs PostgreSQL 18</strong></p>
<h4>表结构与索引</h4>
<pre><code>CREATE TABLE t1
(
c1 int
c2 int,
c3 float
)
WITH (fillfactor=80);
CREATE INDEX idx_t1_c1c2 ON t1(c1, c2);
</code></pre>
<h4>数据生成</h4>
<pre><code>INSERT INTO t1
SELECT (random()*1000)::int, (random()*10000)::int, random()
FROM generate_series(1,1000000) g;
</code></pre>
<h4>通过 COPY 导入数据</h4>
<pre><code>COPY t1 FROM '/.../t1.csv' WITH (FORMAT csv);
</code></pre>
<h4>查询语句 使用复合索引的第二个列</h4>
<pre><code>EXPLAIN ANALYZE SELECT * FROM t1 WHERE c2=100;
</code></pre>
<h4>PostgreSQL 17 执行计划 选择使用并行顺序扫描</h4>
<pre><code>                                                    QUERY PLAN
-------------------------------------------------------------------------------------------------------------------
Gather  (cost=1000.00..12986.33 rows=100 width=16) (actual time=1.125..76.076 rows=90 loops=1)
Workers Planned: 2
Workers Launched: 2
-&gt;  Parallel Seq Scan on t1  (cost=0.00..11976.33 rows=42 width=16) (actual time=1.414..68.624 rows=30 loops=3)
Filter: (c2 = 100)
Rows Removed by Filter: 333303
Planning Time: 0.792 ms
Execution Time: 76.165 ms
(8 rows)
</code></pre>
<p>关闭顺序扫描强制选择索引扫描，并非最优计划，执行更慢。</p>
<pre><code>                                                        QUERY PLAN
--------------------------------------------------------------------------------------------------------------------------
Index Scan using idx_t1_c1c2 on t1  (cost=0.42..18773.42 rows=100 width=16) (actual time=1.846..100.758 rows=90 loops=1)
Index Cond: (c2 = 100)
Planning Time: 0.147 ms
Execution Time: 100.806 ms
(4 rows)
</code></pre>
<h4>PostgreSQL 18 执行计划选择使用索引扫描，可以看出跳跃式扫描执行效率提升幅度非常大</h4>
<pre><code>                                                        QUERY PLAN
---------------------------------------------------------------------------------------------------------------------------
Index Scan using idx_t1_c1c2 on t1  (cost=0.42..3900.84 rows=100 width=16) (actual time=0.225..11.464 rows=90.00 loops=1)
Index Cond: (c2 = 100)
Index Searches: 1002
Buffers: shared hit=3096
Planning Time: 0.141 ms
Execution Time: 11.522 ms
(6 rows)
</code></pre>
<p>关闭索引扫描和位图扫描强制选择顺序扫描。</p>
<pre><code>postgres=# EXPLAIN ANALYZE SELECT * FROM t1 WHERE c2=100;
QUERY PLAN
----------------------------------------------------------------------------------------------------------------------
Gather  (cost=1000.00..12986.33 rows=100 width=16) (actual time=1.486..86.881 rows=90.00 loops=1)
Workers Planned: 2
Workers Launched: 2
Buffers: shared hit=6768
-&gt;  Parallel Seq Scan on t1  (cost=0.00..11976.33 rows=42 width=16) (actual time=2.758..78.712 rows=30.00 loops=3)
Filter: (c2 = 100)
Rows Removed by Filter: 333303
Buffers: shared hit=6768
Planning Time: 0.141 ms
Execution Time: 86.926 ms
(10 rows)
</code></pre>
<h3>使用注意</h3>
<p>跳跃式扫描目前只能支持等值比较条件。</p>
<h3>小结</h3>
<p>PostgreSQL 18 的索引跳跃式扫描，使得多列 BTREE 索引能够被那些仅对第二个或之后的索引列进行等值引用的查询使用，大幅减少索引扫描需要访问的条目，使其效率得到明显提升。</p>
<h2>三、 虚拟生成列：存储与计算的 “灵活平衡”</h2>
<p>PostgreSQL 18 开发体验相关的特性，聚焦于简化开发流程、提升代码灵活性，让开发者更高效地利用 PostgreSQL 能力。</p>
<p>IvorySQL 数据库长期致力于 Oracle 特性兼容，其中包含了一项虚拟列的语法兼容：</p>
<p><code>column [datatype][generated always] AS (column_expression)[VIRTUAL]</code></p>
<p>这次 PostgreSQL 18 终于也带来了虚拟列功能。虚拟列是一种不存储数据的表列，其值在查询时通过动态计算得出。与存储列相比，虚拟列节省了列存储空间，查询虚拟列值时通过计算虚拟列表达式的值作为该列的值。</p>
<h3>基本语法</h3>
<p>PostgreSQL 18 中虚拟列的语法和存储列的语法相似，新增加关键字<strong>VIRTUAL</strong>，当省略 STORED 和 VIRTUAL 关键字时默认为虚拟列。其语法如下所示：</p>
<p><code>GENERATED ALWAYS AS ( generation_expr ) [ STORED | VIRTUAL ]</code></p>
<h3>虚拟列用例</h3>
<p>虚拟列的标识是在列的限制条件中表示的，通过虚拟列的限制语法标识列为虚拟列，以下为虚拟列表的创建、查询和新增虚拟列：</p>
<pre><code>-- 创建包含虚拟列的表，其中price_with_tax为虚拟列
CREATE TABLE products (
id SERIAL PRIMARY KEY,
name TEXT NOT NULL,
price NUMERIC(10,2) NOT NULL,
tax_rate NUMERIC(5,2) DEFAULT 0.20,
price_with_tax NUMERIC(10,2) GENERATED ALWAYS AS (price * (1 + tax_rate)) VIRTUAL
);
-- 插入数据
INSERT INTO products (name, price, tax_rate)
VALUES ('Laptop', 1000.00, 0.20);
-- 查询数据（虚拟列自动计算）
SELECT name, price, tax_rate, price_with_tax FROM products;
name  |  price  | tax_rate | price_with_tax
--------+---------+----------+----------------
Laptop | 1000.00 |     0.20 |        1200.00
(1 row)
--为表添加虚拟列
ALTER TABLE products ADD COLUMN selling_price NUMERIC(10,2)
GENERATED ALWAYS AS (
price * (1 - 0.2) * (1 + tax_rate)
) virtual;
</code></pre>
<h3>实现原理浅析</h3>
<h4>虚拟列的创建</h4>
<p>创建的表中虚拟列的存储方式和普通列的存储方式类似，其列信息都存储在 pg_attribute 系统表中，其中 attgenerated 列存储生成列信息，如果该列的值为's'，表示该列为存储列。PostgreSQL 18 新增的虚拟列在该字段中的标识符为'v'，并且将虚拟列的表达式存储于 pg_attrdef 系统表中。</p>
<pre><code>--查看虚拟列信息，其attgenerated为v表示该列为虚拟列
postgres=# select * from pg_attribute where attname='price_with_tax';
-[ RECORD 1 ]--+---------------
attrelid       | 16388
attname        | price_with_tax
atttypid       | 1700
attlen         | -1
attnum         | 5
atttypmod      | 655366
attndims       | 0
attbyval       | f
attalign       | i
attstorage     | m
attcompression |
attnotnull     | f
atthasdef      | t
atthasmissing  | f
attidentity    |
attgenerated   | v
attisdropped   | f
attislocal     | t
attinhcount    | 0
attcollation   | 0
attstattarget  |
attacl         |
attoptions     |
attfdwoptions  |
attmissingval  |
--查看虚拟列表达式存储，可以看到以下表达式为虚拟列表达式
postgres=# select pg_get_expr(adbin, adrelid) from pg_attrdef where adnum = 5;
pg_get_expr
-------------------------------------
(price * ((1)::numeric + tax_rate))
(1 row)
</code></pre>
<h4>虚拟列的插入或更新</h4>
<p>由于虚拟列的数据不占据存储空间，所以任何指定更新或插入虚拟列的操作都将被限制。</p>
<pre><code>--指定插入虚拟列
INSERT INTO products (name, price, tax_rate, price_with_tax) VALUES ('Laptop', 1000.00, 0.20, 1);
ERROR:  cannot insert a non-DEFAULT value into column "price_with_tax"
DETAIL:  Column "price_with_tax" is a generated column.
--指定更新虚拟列
update products set price_with_tax = 1 where name = 'Laptop';
ERROR:  column "price_with_tax" can only be updated to DEFAULT
DETAIL:  Column "price_with_tax" is a generated column.
</code></pre>
<h4>虚拟列的查询</h4>
<p>PostgreSQL 18 中查询虚拟列的实现是在生成执行计划阶段完成。在逻辑重写优化阶段，判断查询的范围表中是否包含虚拟列，如果包含虚拟列，则将该虚拟列的表达式从 pg_attrdef 中获取出来并替换原虚拟列名。这样查询虚拟列的值就相当于计算其表达式的值，即 <code>select price_with_tax</code> 相当于 <code>select (price \* ('1'::numeric + tax_rate)) as price_with_tax</code>。可以看到以下虚拟列被替换成了其表达式：</p>
<pre><code>postgres=# explain verbose SELECT name, price, tax_rate, price_with_tax
FROM products;
QUERY PLAN
----------------------------------------------------------------------
Seq Scan on public.products  (cost=0.00..23.12 rows=750 width=76)
Output: name, price, tax_rate, (price * ('1'::numeric + tax_rate))
(2 rows)
</code></pre>
<h3>适用场景</h3>
<ul>
<li>当考虑存储空间时，可以使用虚拟列，因为虚拟列不占用磁盘空间。</li>
<li>当列的值需要根据依赖的列变化而变化时，需要使用虚拟列。因为虚拟列的值是动态获取的。</li>
<li>当虚拟列表达式简单时，可以使用虚拟列。因为查询虚拟列需消耗 CPU 资源，表达式复杂会消耗太多 CPU 资源。</li>
<li>因为 Oracle 中有虚拟列功能，更加方便 Oracle 的虚拟列迁移至 PostgreSQL 中。</li>
</ul>
<h3>待完善部分</h3>
<p>一些功能目前尚不支持，但可能会作为增量功能在后续的版本中添加：</p>
<ul>
<li>在虚拟列上创建索引或使用虚拟列。</li>
<li>虚拟列上也没有唯一约束。</li>
<li>虚拟列上的扩展统计信息。</li>
<li>虚拟列上的外键约束。</li>
<li>虚拟列上的非空约束（支持检查约束）。</li>
<li>ALTER TABLE / DROP EXPRESSION。</li>
<li>虚拟列不能具有域类型。</li>
<li>逻辑复制不支持虚拟列。</li>
</ul>
<h3>小结</h3>
<p>虚拟列与普通列和存储列有着本质的不同，因为虚拟列的值不占磁盘空间，其获取值的方式也与普通列和存储列不同，普通列或存储列需要从磁盘获取数据，而虚拟列是通过动态计算获取虚拟列的值。</p>
<h2>四、UUID 功能增强：有序性与易用性提升</h2>
<h3>传统 UUID 的无序性是其用作主键的主要痛点：</h3>
<p>传统 UUID（尤其是 v4）的完全随机性是其作为数据库主键的痛点：</p>
<ul>
<li>UUID 随机生成，插入位置不确定，导致索引树频繁分裂和重组，大幅降低写入性能。</li>
<li>破坏聚簇索引（如 InnoDB）的物理存储顺序，增加磁盘 I/O。</li>
<li>范围查询和排序效率低下，性能低下。</li>
</ul>
<h3>UUIDv7 的关键突破：时间有序性架构设计</h3>
<p>UUIDv7 通过在 UUID 的高位部分引入时间戳来解决生成 UUID 完全随机的问题，使新生成的 UUID 能够按照创建时间自然排序。这样，B 树索引可以像自增整数一样进行顺序插入，同时仍然保持 UUID 的全局唯一性和分布式生成优势。</p>
<p>该特性使 UUIDv7 作为主键具备以下突出优势：</p>
<ul>
<li>严格按照创建时间先后顺序递增。</li>
<li>减少索引碎片。</li>
<li>提高缓存命中率。</li>
<li>适合高并发插入和高效查询的场景。</li>
</ul>
<h3>UUIDv7 的结构设计</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-c53e5aa31c.png" alt="uuidv7.png"></p>
<table>
<thead>
<tr>
<th>字段</th>
<th>位数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>毫秒级 Unix 时间戳</td>
<td>48 位</td>
<td>Unix 时间戳（毫秒）</td>
</tr>
<tr>
<td>亚毫秒级时间戳分数（用于额外排序）</td>
<td>12 位</td>
<td>时间戳的微秒精度扩展</td>
</tr>
<tr>
<td>随机数</td>
<td>62 位</td>
<td>随机数或计数器</td>
</tr>
<tr>
<td>版本号</td>
<td>4 位</td>
<td>固定为 0111（v7）</td>
</tr>
<tr>
<td>变体</td>
<td>2 位</td>
<td>固定为 10（RFC 4122）</td>
</tr>
</tbody>
</table>
<p><strong>设计关键点解析：</strong></p>
<ul>
<li><strong>高精度时间前缀(48 位):</strong> 精确到毫秒的 Unix 时间戳，确保 ID 严格按时间递增（需 NTP 时钟同步）。</li>
<li><strong>尾部随机位(62 位):</strong> 保证分布式唯一性，避免 v1 版本的 MAC 地址泄漏风险。</li>
</ul>
<p><strong>有序性如何解决性能问题？</strong></p>
<ul>
<li><strong>B-Tree 索引优化:</strong> 新生成的 UUIDv7 总是大于之前的值，因此被追加到索引尾部，避免中间节点分裂。</li>
<li><strong>缓冲池友好:</strong> 顺序写入使新记录集中在少数数据页。当页写满时，数据库只需分配新页追加，减少旧页淘汰与磁盘 I/O。</li>
<li><strong>范围查询加速:</strong> 时间有序性使 WHERE id &gt; '2025-06-01' 可转化为时间戳范围过滤，大幅降低扫描范围。</li>
</ul>
<h3>如何在 PostgreSQL18 中应用 UUIDv7</h3>
<p>PostgreSQL 18 引入了多个新函数来支持 UUIDv7，方便生成、操作和提取 UUID 信息。</p>
<ol>
<li><code>uuidv7()</code>函数：用于生成新的 UUIDv7 值</li>
</ol>
<pre><code>-- 使用当前时间戳生成 UUIDv7
SELECT uuidv7();
-- 输出示例: 0197f96c-b278-7f64-a32f-dae3cabe1ff0
-- 生成 1 小时前的 UUIDv7
SELECT uuidv7(INTERVAL '-1 hour');
-- 生成 30 分钟后的 UUIDv7
SELECT uuidv7(INTERVAL '30 minutes');
</code></pre>
<ol start="2">
<li><code>uuidv4()</code>函数：作为已有函数 <code>gen_random_uuid()</code> 的别名 ，便于和 uuidv7 一起使用</li>
</ol>
<pre><code>-- 两者等价
SELECT gen_random_uuid();
SELECT uuidv4();
</code></pre>
<ol start="3">
<li><code>uuid_extract_timestamp()</code>函数 : 该函数现在支持 UUIDv7（原本只支持 UUIDv1）</li>
</ol>
<pre><code>-- 从 UUIDv7 提取时间戳
SELECT uuid_extract_timestamp(uuidv7());
-- 示例输出: 2025-09-18 12:20:49.409+00
</code></pre>
<ol start="4">
<li><code>uuid_extract_version()</code>函数：用于检测 UUID 的版本：</li>
</ol>
<pre><code>-- 检查 UUID 版本
SELECT uuid_extract_version(uuidv7());  -- 返回 7
SELECT uuid_extract_version(uuidv4());  -- 返回 4
</code></pre>
<p>PostgreSQL 数据库中使用 UUIDv7 作为主键：</p>
<pre><code>--创建带 UUIDv7 主键的表
CREATE TABLE users (
id UUID PRIMARY KEY DEFAULT uuidv7(),
username VARCHAR(50) UNIQUE NOT NULL,
email VARCHAR(100) NOT NULL,
created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
</code></pre>
<p>这样，每条新记录的 <code>id</code> 都会自动分配一个按时间戳排序的 UUID。</p>
<pre><code>--插入数据
INSERT INTO users (username, email)
VALUES ('alice', 'alice@example.com');
INSERT INTO users (username, email)
VALUES ('bob', 'bob@example.com');
-- 按 UUID 时间顺序查看
SELECT id, username, uuid_extract_timestamp(id) as uuid_timestamp
FROM users
ORDER BY id;
</code></pre>
<h3>性能优势：</h3>
<ol>
<li>UUIDv7 的时间戳顺序能显著减少页分裂和缓存失效，有效提升 B 树索引效率。</li>
</ol>
<pre><code>--创建性能测试表
CREATE TABLE performance_test (
id_v4 UUID DEFAULT uuidv4(),
id_v7 UUID DEFAULT uuidv7(),
data TEXT DEFAULT 'sample data'
);
--使用UUIDv7作为索引
CREATE INDEX idx_v4 ON performance_test (id_v4);
CREATE INDEX idx_v7 ON performance_test (id_v7);
</code></pre>
<p>批量插入后，你可以用 <code>pg_statio_user_indexes</code> 查看索引命中情况，UUIDv7 通常表现更优。</p>
<ol start="2">
<li>UUIDv7 自带时间排序，大部分场景下显著提升排序性能。</li>
</ol>
<pre><code>-- 利用 UUIDv7 自然排序
SELECT id_v7, data
FROM performance_test
ORDER BY id_v7
LIMIT 10;
</code></pre>
<p>相比 UUIDv4 的随机顺序，UUIDv7 查询结果按创建顺序返回，更直观。</p>
<h3>UUIDv7 最佳实践</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-13baf49df9.png" alt="uuidv7_2.png"></p>
<p>适合使用 UUIDv7 的场景：</p>
<ul>
<li> <p><strong>多租户应用</strong>：可用 UUIDv7 做主键，并为 <code>(tenant_id, id)</code> 创建复合索引，既保持唯一性又能按时间排序。</p> </li>
<li> <p><strong>分布式系统</strong>：多个服务可独立生成 UUIDv7，并且在全局范围内仍能保持时间顺序。</p> </li>
</ul>
<h3>限制与注意事项</h3>
<ul>
<li><strong>依赖系统时钟</strong>：需启用 NTP 等时间同步机制，避免时钟漂移。</li>
<li><strong>时间戳精度</strong>：UUIDv7 以毫秒为单位，在同一毫秒内生成多个 UUID，顺序可能无法完全反映真实创建顺序，但仍保持唯一性。</li>
<li><strong>迁移规划</strong>：从 UUIDv4 迁移到 UUIDv7 时，需要检查应用逻辑、索引和外部依赖。</li>
</ul>
<h3>小结</h3>
<p>PostgreSQL 18 对 UUIDv7 的支持，解决了 UUID 作为主键的性能瓶颈。UUIDv7 在保持全局唯一性的同时，具备类似自增整数的顺序性，使 B 树插入更高效，查询更快。</p>
<p>对于需要分布式、高并发和高性能的现代应用，UUIDv7 提供了一种兼顾唯一性和性能的实用解决方案。</p>
<h2>五、EXPLAIN 增强：直观呈现执行细节</h2>
<p>PostgreSQL 18 对 EXPLAIN 命令进行了重大升级，通过提供更丰富、更直观的执行计划信息，让数据库开发者和 DBA 能够更加轻松地进行查询性能分析与优化。</p>
<h3>自动缓冲区分析</h3>
<p>EXPLAIN ANALYZE 现在默认包含 BUFFER 统计信息，无需手动添加 BUFFERS 选项：</p>
<ul>
<li><strong>共享命中(Shared Hits)</strong>：显示从缓存中读取的数据块数量，反映内存使用效率。</li>
<li><strong>共享读取(Shared Reads)</strong>：标识必须从磁盘读取的数据块，帮助识别 I/O 瓶颈。</li>
<li><strong>共享脏块(Shared Dirtied)</strong>：针对数据修改操作，显示被更改的块数量。</li>
</ul>
<h3>精细化索引监控</h3>
<p>新增索引扫描次数统计，让开发者能够精确了解索引使用效率：</p>
<pre><code>-- 示例输出显示索引使用情况
Index Scan using orders_pkey on orders
Index Searches: 1  -- 明确显示索引查找次数
Buffers: shared hit=2 read=2
</code></pre>
<h3>增强的统计信息</h3>
<ul>
<li>支持小数行（ fractional row counts），提供更精确的行数估计。</li>
<li>为 Material、Window Aggregate、CTE 节点输出内存和磁盘使用详情。</li>
<li>在窗口函数中显示详细的参数信息。</li>
<li>为 Parallel Bitmap Heap Scan 显示 worker 缓存统计。</li>
<li>输出禁用节点。</li>
<li>输出 WAL 缓冲区信息。</li>
</ul>
<h3>基础查询分析</h3>
<pre><code>-- 创建测试表
CREATE TABLE orders (
order_id SERIAL PRIMARY KEY,
customer_id INTEGER NOT NULL,
order_date DATE NOT NULL,
total_amount DECIMAL(10, 2) NOT NULL
);
CREATE INDEX idx_orders_customer_id ON orders(customer_id);
-- 插入测试数据
INSERT INTO orders (customer_id, order_date, total_amount)
SELECT
(n % 10) + 1,
CURRENT_DATE - (n % 365),
(50 + (random() * 950))::decimal(10,2)
FROM generate_series(1, 50000) n;
-- 查看增强的执行计划
EXPLAIN ANALYZE
SELECT *,sum(total_amount) OVER (PARTITION BY customer_id)
FROM orders WHERE order_id&gt;49900;
</code></pre>
<p>执行计划：</p>
<pre><code>QUERY PLAN
----------------------------------------------------------------------------------------------------------------------------------------
WindowAgg  (cost=13.46..15.04 rows=99 width=50) (actual time=0.630..0.745 rows=100.00 loops=1)
Window: w1 AS (PARTITION BY customer_id)
Storage: Memory  Maximum Storage: 17kB
Buffers: shared hit=5 read=2
-&gt;  Sort  (cost=13.30..13.55 rows=99 width=18) (actual time=0.307..0.333 rows=100.00 loops=1)
Sort Key: customer_id
Sort Method: quicksort  Memory: 28kB
Buffers: shared hit=5 read=2
-&gt;  Index Scan using orders_pkey on orders  (cost=0.29..10.02 rows=99 width=18) (actual time=0.089..0.174 rows=100.00 loops=1)
Index Cond: (order_id &gt; 49900)
Index Searches: 1
Buffers: shared hit=2 read=2
Planning:
Buffers: shared hit=64 read=22
Planning Time: 2.080 ms
Execution Time: 1.343 ms
</code></pre>
<p>执行计划输出洞察：</p>
<ul>
<li>缓冲区使用情况（缓存命中 vs 磁盘读取）：生成执行计划时从缓存中访问了 64 个共享缓冲区，从磁盘中读取了 22 个缓冲区，执行时从缓冲区访问了 5 个共享缓冲区，从磁盘读取了 2 个缓冲区。</li>
<li>索引效率统计：执行了 1 次 orders_pkey 索引扫描，并从缓存中访问了 2 个共享缓冲区，清晰地显示了索引的使用效率。</li>
<li>窗口函数内存使用详情：使用的 17kB 磁盘空间。</li>
<li>精确的行统计信息。</li>
<li>窗口函数的详细参数。</li>
</ul>
<h3>WAL 日志分析</h3>
<pre><code>EXPLAIN (ANALYZE, WAL)
INSERT INTO orders (customer_id, order_date, total_amount)
SELECT
(n % 10) + 1,
CURRENT_DATE - (n % 365),
(50 + (random() * 950))::decimal(10,2)
FROM generate_series(1, 50000) n;
</code></pre>
<p>执行计划：</p>
<pre><code>QUERY PLAN
---------------------------------------------------------------------------------------------------------------------------------------------
Insert on orders  (cost=0.00..2000.00 rows=0 width=0) (actual time=767.116..767.118 rows=0.00 loops=1)
Buffers: shared hit=299156 read=2 dirtied=500 written=501
WAL: records=152158 bytes=10427828 buffers full=139
-&gt;  Subquery Scan on "*SELECT*"  (cost=0.00..2000.00 rows=50000 width=28) (actual time=5.742..336.699 rows=50000.00 loops=1)
Buffers: shared hit=50013
WAL: records=1516 bytes=150084 buffers full=2
-&gt;  Function Scan on generate_series n  (cost=0.00..1750.00 rows=50000 width=24) (actual time=5.460..227.650 rows=50000.00 loops=1)
Planning Time: 0.114 ms
Execution Time: 767.179 ms
</code></pre>
<p>WAL 统计：</p>
<ul>
<li>监控写入负载的日志生成量：WAL 缓冲区生成 1516 条日志，共 150084 个字节的数据。</li>
<li>诊断写入性能瓶颈：缓冲区被写满了 2 次。</li>
</ul>
<h3>并行查询优化</h3>
<pre><code>EXPLAIN (ANALYZE)
SELECT * FROM orders WHERE customer_id IN (1, 2, 3, 4, 5, 6);
</code></pre>
<p>执行计划：</p>
<pre><code>QUERY PLAN
-------------------------------------------------------------------------------------------------------------------------------------------------------
Gather  (cost=2752.40..10357.99 rows=327855 width=18) (actual time=22.375..121.296 rows=330000.00 loops=1)
Workers Planned: 2
Workers Launched: 2
Buffers: shared hit=3810
-&gt;  Parallel Bitmap Heap Scan on orders  (cost=2751.40..10029.13 rows=136606 width=18) (actual time=12.868..88.329 rows=110000.00 loops=3)
Recheck Cond: (customer_id = ANY ('{1,2,3,4,5,6}'::integer[]))
Rows Removed by Index Recheck: 53967
Heap Blocks: exact=170 lossy=566
Buffers: shared hit=3810
Worker 0:  Heap Blocks: exact=387 lossy=957
Worker 1:  Heap Blocks: exact=369 lossy=1055
-&gt;  Bitmap Index Scan on idx_orders_customer_id  (cost=0.00..2669.44 rows=327855 width=0) (actual time=21.219..21.220 rows=330000.00 loops=1)
Index Cond: (customer_id = ANY ('{1,2,3,4,5,6}'::integer[]))
Index Searches: 1
Buffers: shared hit=266
Planning:
Buffers: shared hit=30
Planning Time: 0.510 ms
Execution Time: 158.523 ms
</code></pre>
<p>并行执行效率洞察：</p>
<ul>
<li>每个工作进程的缓存统计详情：worker 0 命中 387 个精确块和 957 个有损块，worker 1 命中 369 个精确块和 1055 个有损块。</li>
<li>精确块与有损块分析：出现有损块说明可能 work_mem 太小导致 bitmap 无法精准定位元组。</li>
</ul>
<h3>技术优势与价值</h3>
<h4>即时性能诊断</h4>
<ul>
<li><strong>降低门槛</strong>：自动化的缓冲区统计让初学者快速识别 I/O 问题。</li>
<li><strong>深度洞察</strong>：为专家级用户提供更细粒度的性能数据。</li>
<li><strong>全面覆盖</strong>：单条命令获取执行计划、缓存使用、索引效率等多维信息。</li>
</ul>
<h4>优化指导</h4>
<ul>
<li><strong>索引优化</strong>：通过精确的索引使用统计，避免过度索引或索引不足。</li>
<li><strong>内存调优</strong>：根据有损块出现频率指导 work_mem 参数调整</li>
<li><strong>查询重写</strong>：基于详细的执行成本数据优化 SQL 语句结构</li>
</ul>
<h3>功能未来展望</h3>
<p>尽管 PostgreSQL 18 的 EXPLAIN 增强带来了显著改进，但仍有一些方面可以进一步完善：</p>
<ol>
<li><strong>输出可读性</strong>：随着信息量的增加，输出变得更加复杂，可能需要更好的格式化或可视化工具支持</li>
<li><strong>历史对比</strong>：缺乏直接与历史执行计划对比的内置机制，使得性能回归分析仍需依赖外部工具</li>
<li><strong>阈值警报</strong>：没有内置机制对异常值（如异常高的缓冲区读取）发出警告，需要手动分析</li>
<li><strong>执行计划可视化</strong>：文本形式的输出在复杂查询中仍难以直观理解，需要第三方工具补充</li>
</ol>
<h3>小结</h3>
<p>PostgreSQL 18 的 EXPLAIN 增强代表了数据库可观测性的重大进步。通过自动化收集关键性能指标并提供更深入的执行洞察，它显著降低了查询优化的门槛，同时为经验丰富的 DBA 提供了更强大的分析能力。</p>
<h2>六、OAuth 2.0 认证支持：筑牢数据防护壁垒</h2>
<p>在安全方面，IvorySQL 致力于加入多种国密认证功能来保障数据安全。而这次 PostgreSQL18 在身份认证方面继续加强，引入对 OAuth 2 的支持。这是一种开放标准的授权协议，用于授权一个应用程序或服务访问用户在另一个应用程序中的资源，而无需提供用户名和密码。</p>
<p>该特性主要包含以下几个核心要素：</p>
<ul>
<li><strong>OAuth2 验证器框架</strong>：提供了一个可扩展的框架，使 PostgreSQL 能够与 OAuth 2.0 提供程序集成。PostgreSQL 本身不实现具体的令牌验证算法（如 JWT 验证），而是将这项工作委托给一个外部共享库 (<code>*.so</code> 文件)。</li>
<li><strong>客户端认证支持</strong>：libpq（PostgreSQL 的 C 客户端库）现在支持 OAuth 2.0 认证流程。</li>
<li><strong>自定义验证逻辑</strong>：通过回调机制允许实现自定义的令牌验证和用户映射逻辑。</li>
</ul>
<h3>配置 OAuth 认证方式</h3>
<h4>服务端配置</h4>
<ol>
<li> <p>选择 OAuth 认证的方式与瀚高数据库选择国密认证的方式类似，需要通过在 pg_hba.conf 文件中指定 METHOD 为 oauth，开启 OAuth 认证。</p> <p>同时 OPTIONS 必须指定 issuer 和 scope 参数，除此之外还有几个可选参数：validator、map、delegate_ident_mapping，以下是一个最简配置示例：</p> <pre><code>local all test oauth issuer="http://127.0.0.1:9000" scope="openid postgre"
</code></pre> </li>
<li> <p>指定外部 OAuth 验证器，在 postgresql.conf 文件中配置新提供的 oauth_validator_libraries 参数，配置内容为 OAuth 验证器提供的库文件。</p> </li>
</ol>
<h4>客户端配置</h4>
<p>客户端在连接时需要指定以下连接参数从而实现连接：</p>
<ul>
<li>oauth_issuer：必要参数，HTTPS URL，是授权服务器的颁发者标识符。</li>
<li>oauth_client_id：必要参数，由授权服务器颁发的 OAuth 2.0 客户端标识符。</li>
<li>oauth_client_secret：可选参数，访问 OAuth 授权服务器时要使用的客户端密码。</li>
<li>oauth_scope：可选参数，发送到授权服务器的访问请求的范围，指定为 OAuth 范围标识符的空格分隔列表。</li>
</ul>
<h3>认证实现原理</h3>
<p>oauth 整体认证流程大致如下图所示：</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-c473a4e1d2.png" alt="oauth.png"></p>
<h4>客户端（libpq）</h4>
<p>PostgreSQL 实现了一个<strong>非阻塞的、基于状态机的异步网络客户端</strong>。状态机包含 <code>OAUTH_STEP_INIT</code>、<code>OAUTH_STEP_DISCOVERY</code>、<code>OAUTH_STEP_DEVICE_AUTHORIZATION</code>、<code>OAUTH_STEP_TOKEN_REQUEST</code>、<code>OAUTH_STEP_WAIT_INTERVAL</code> 这几个状态。其核心原理包含以下几个部分：</p>
<ul>
<li><strong>DISCOVERY</strong>：客户端从用户请求中获取授权服务器元信息。</li>
<li><strong>DEVICE_AUTHORIZATION</strong>：客户端向授权服务器发送请求，授权服务器返回 device_code 和 verification_uri。客户端输出信息"Visit xxxxxx and enter the code: xxxxxx"，提示用户进行操作。</li>
<li><strong>TOKEN_REQUEST 和 WAIT_INTERVAL</strong>：轮询访问授权服务器，直到用户完成授权，授权服务器返回 access_token 给客户端。</li>
<li>将获取到的 <code>access_token</code> 设置到连接对象中。<code>libpq</code> 会将它作为密码发送给 PostgreSQL 服务器，服务器端的 OAuth 验证器会负责校验这个令牌。</li>
</ul>
<h4>服务端</h4>
<p>以下 PostgreSQL 服务端处理 OAuth 认证流程，同样通过状态机实现，但要比客户端简单得多，总共分为 <code>OAUTH_STATE_INIT</code>、<code>OAUTH_STATE_ERROR</code>、<code>OAUTH_STATE_FINISHED</code> 三个状态。以下是核心步骤：</p>
<ul>
<li>首先解析客户端发送的消息，该消息格式遵循 RFC 7628 第 3.1 节部分。</li>
<li>从客户端消息中提取出纯粹的 Bearer Token，并验证其格式（是否为合法的 Base64 字符串）。</li>
<li>将提取出的令牌传递给验证器模块进行实质性的验证。
<ul>
<li>验证成功：状态转为 <code>OAUTH_STATE_FINISHED</code>，返回 <code>PG_SASL_EXCHANGE_SUCCESS</code>。进行建立连接的后续操作。</li>
<li>验证失败：生成一个符合 RFC 7628 第 3.2.2 节的 JSON 错误响应，告知客户端所需的 <code>scope</code> 和到哪里获取令牌。状态转为 <code>OAUTH_STATE_ERROR</code>，并返回 <code>PG_SASL_EXCHANGE_CONTINUE</code>，等待客户端发送最终的 <code>KVSEP</code> 来结束失败的握手。</li>
</ul> </li>
</ul>
<h4>外部验证器</h4>
<p>外部验证器通常需要处理以下事项：</p>
<ul>
<li>令牌验证：可以通过在线验证和本地验证两种方式，由验证器自行决定。在线验证下验证器通常将令牌发送到授权服务器专门的 <code>Introspection Endpoint</code>，授权服务器会返回一个 JSON 响应，告知令牌是否有效。本地验证则需要验证器内部实现一套验证流程，本地验证令牌的签名和有效期。本地验证的好处在于能够快速响应，但缺点是无法实时检测令牌撤销。</li>
<li>身份映射：在验证通过后，验证器需要提取令牌中的唯一用户标识，并转换为数据库可理解的身份标识，也就是数据库用户。</li>
<li>连接决策：如果令牌处于有效期并且存在相应的数据库用户映射关系，则以该用户的身份创建会话连接。</li>
</ul>
<h3>优缺点剖析</h3>
<p><strong>优点：</strong></p>
<ol>
<li>OAuth2 提供了现代、标准化的身份验证机制，提高了安全性。通过 OAuth2 认证，规避了传统密码认证在数据传输过程中暴露密码导致的安全风险。</li>
<li>简化了数据库用户管理，支持统一的身份策略和访问控制，提高了管理效率。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>相较于传统的密码认证，实现更加复杂，需要额外的配置和维护工作，包括 OAuth2 提供程序的设置和管理，提高了运维的复杂度。比如瀚高数据库的国密认证功能，同样在保障口令安全的同时仅需要通过非常简单的配置即可使用。</li>
<li>依赖于外部 OAuth 提供程序的可用性和可靠性，若 OAuth 提供程序出现问题，可能会影响数据库访问。</li>
<li>每次连接可能需要额外的网络请求来验证令牌，可能会增加连接建立的时间，特别是在高并发场景下。</li>
</ol>
<h3>小结</h3>
<p>PostgreSQL 18 引入的 OAuth2 支持是一个重要的安全增强功能，它允许组织使用现代的身份验证机制来保护数据库访问。通过提供灵活的验证器框架和回调机制，PostgreSQL 18 可以适应各种 OAuth2 部署场景和业务需求。</p>
<p>虽然 OAuth2 认证增加了系统的复杂性，但它提供了显著的安全优势，特别是在集中式身份管理和单点登录方面。对于希望采用现代安全最佳实践的组织来说，这是一个值得考虑的新功能。OAuth 认证并不与已有的认证方式冲突，在不希望进行繁琐的配置时，仍然可以选择类似于瀚高数据库提供的国密认证等方式。</p>
<p>在实施 OAuth2 认证时，组织应该仔细评估其需求、现有基础设施和技术能力，以确保成功部署和运维。同时应该考虑到性能、可用性和兼容性等因素，以提供最佳的用户体验和系统可靠性。</p>
<h1>总结</h1>
<p>PostgreSQL 18 凭借六大核心特性实现了性能、功能与安全性的全方位升级：</p>
<ul>
<li>异步 I/O 突破了同步阻塞瓶颈，提升读取密集型场景的吞吐量。</li>
<li>跳跃式扫描让多列 B 树索引在非前导列查询中发挥高效作用。</li>
<li>虚拟生成列在存储与计算间找到了灵活平衡，优化了开发体验。</li>
<li>UUIDv7 解决了传统 UUID 无序性带来的性能痛点，兼顾唯一性与顺序性。</li>
<li>EXPLAIN 增强为查询优化提供了更直观、细致的执行洞察。</li>
<li>OAuth 2.0 认证则为数据安全筑牢了现代防护屏障。</li>
</ul>
<p>这些特性不仅满足了当前数据库在高性能、高并发、易开发、强安全等方面的需求，也为未来在跨平台适配、功能扩展等方向奠定了坚实基础，进一步巩固了 PostgreSQL 在开源数据库领域的领先地位，为各类应用场景提供了更强大、更灵活的技术支撑。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-caf40f3143.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/ifclub.com.cn-caf40f3143.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 17:06:06 +0800</pubDate>
  </item><item>
    <title><![CDATA[Thinking Machines Lab 联合创始人加入 Meta]]></title>
    <link>https://www.oschina.net/news/377175</link>
    <itunes:title><![CDATA[Thinking Machines Lab 联合创始人加入 Meta]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>AI 初创公司 Thinking Machines Lab 联合创始人 Andrew Tulloch 宣布<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2025%2F10%2F11%2Fthinking-machines-lab-co-founder-andrew-tulloch-heads-to-meta%2F" target="_blank">离职</a>，正式加入 Meta（原 Facebook）。</p>
<p>据知情人士透露，Tulloch 已在一则内部消息中确认了离职消息。Thinking Machines Lab 的发言人在声明中称：“Andrew 因个人原因决定寻求新的方向。他的贡献对公司初期的发展至关重要，我们对他所建立的基础深表感谢，并将坚定地继续推进我们共同开启的事业。”</p>
<p>Thinking Machines Lab 由前 OpenAI CTO Mira Murati 领导，专注于先进 AI 模型与系统研究。Tulloch 曾在 OpenAI 和 Facebook AI Research 任职，拥有深厚的工程与研究背景。</p>
<p>作为 AI 领域资深研究者，Tulloch 曾在 Meta 旗下 Facebook AI 研究组工作 11 年，主导 PyTorch 框架核心开发，后加盟 OpenAI 参与 GPT-4o 及 4.5 预训练系统研发。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1e030fb108.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-e83697535b.png"></p>
<p>早在 2025 年 8 月，有报道指出 Mark Zuckerberg 曾出价欲收购 Thinking Machines Lab。如果收购失败，Meta 也尝试用高额补偿（据称可能高达 15 亿美元、持续至少六年）挖角该公司的关键人物。Meta 发言人否认了这些数额和报导是准确的，称其 “不准确且荒谬”。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>AI 初创公司 Thinking Machines Lab 联合创始人 Andrew Tulloch 宣布<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2025%2F10%2F11%2Fthinking-machines-lab-co-founder-andrew-tulloch-heads-to-meta%2F" target="_blank">离职</a>，正式加入 Meta（原 Facebook）。</p>
<p>据知情人士透露，Tulloch 已在一则内部消息中确认了离职消息。Thinking Machines Lab 的发言人在声明中称：“Andrew 因个人原因决定寻求新的方向。他的贡献对公司初期的发展至关重要，我们对他所建立的基础深表感谢，并将坚定地继续推进我们共同开启的事业。”</p>
<p>Thinking Machines Lab 由前 OpenAI CTO Mira Murati 领导，专注于先进 AI 模型与系统研究。Tulloch 曾在 OpenAI 和 Facebook AI Research 任职，拥有深厚的工程与研究背景。</p>
<p>作为 AI 领域资深研究者，Tulloch 曾在 Meta 旗下 Facebook AI 研究组工作 11 年，主导 PyTorch 框架核心开发，后加盟 OpenAI 参与 GPT-4o 及 4.5 预训练系统研发。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1e030fb108.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-e83697535b.png"></p>
<p>早在 2025 年 8 月，有报道指出 Mark Zuckerberg 曾出价欲收购 Thinking Machines Lab。如果收购失败，Meta 也尝试用高额补偿（据称可能高达 15 亿美元、持续至少六年）挖角该公司的关键人物。Meta 发言人否认了这些数额和报导是准确的，称其 “不准确且荒谬”。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>AI 初创公司 Thinking Machines Lab 联合创始人 Andrew Tulloch 宣布<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2025%2F10%2F11%2Fthinking-machines-lab-co-founder-andrew-tulloch-heads-to-meta%2F" target="_blank">离职</a>，正式加入 Meta（原 Facebook）。</p>
<p>据知情人士透露，Tulloch 已在一则内部消息中确认了离职消息。Thinking Machines Lab 的发言人在声明中称：“Andrew 因个人原因决定寻求新的方向。他的贡献对公司初期的发展至关重要，我们对他所建立的基础深表感谢，并将坚定地继续推进我们共同开启的事业。”</p>
<p>Thinking Machines Lab 由前 OpenAI CTO Mira Murati 领导，专注于先进 AI 模型与系统研究。Tulloch 曾在 OpenAI 和 Facebook AI Research 任职，拥有深厚的工程与研究背景。</p>
<p>作为 AI 领域资深研究者，Tulloch 曾在 Meta 旗下 Facebook AI 研究组工作 11 年，主导 PyTorch 框架核心开发，后加盟 OpenAI 参与 GPT-4o 及 4.5 预训练系统研发。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1e030fb108.png"></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-e83697535b.png"></p>
<p>早在 2025 年 8 月，有报道指出 Mark Zuckerberg 曾出价欲收购 Thinking Machines Lab。如果收购失败，Meta 也尝试用高额补偿（据称可能高达 15 亿美元、持续至少六年）挖角该公司的关键人物。Meta 发言人否认了这些数额和报导是准确的，称其 “不准确且荒谬”。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1e030fb108.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1e030fb108.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 17:03:55 +0800</pubDate>
  </item><item>
    <title><![CDATA[前滴滴高级副总裁付强加入蘑菇车联，负责 AI 落地]]></title>
    <link>https://www.oschina.net/news/377173</link>
    <itunes:title><![CDATA[前滴滴高级副总裁付强加入蘑菇车联，负责 AI 落地]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><span>据科创板日报，前滴滴高级副总裁付强加入蘑菇车联（MOGOX），出任总裁，这也是该公司的首任总裁。据悉，付强将全面负责公司的AI业务的战略规划落地与商业化布局。</span></p>
<p><span>当天下午，蘑菇车联（MOGOX）创始人朱磊发布内部公开信官宣此事。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2f1de808bd.jpg"></p>
<p><span>付强出生于1981年，毕业于四川大学，早年曾在强生、葛兰素史克等跨国公司工作。</span></p>
<p><span>2014年，付强加入快的打车担任运营总监，后升任副总裁。2015年，快的与滴滴合并，付强调任滴滴代驾事业部，担任首任总经理。当时有报道称，在其领导下滴滴代驾业务用半年时间拿下市场份额第一，迅速实现盈利。2016年，付强晋升为滴滴品质出行事业部总经理，统管代驾、专车、豪车等多项业务。2017年付强调任滴滴当时投资的OFO担任执行总裁。2018年回归滴滴，出任高级副总裁，分管网约车平台和城市运输与服务事业群。</span></p>
<p><span>2023年，付强从滴滴离职，加盟聚焦长途货运的满帮集团，担任首席运营官。</span></p>
<p><span>公开信息显示，蘑菇车联（MOGOX）专注于车路云一体化赛道，是在车路协同方向上探索Robotaxi的玩家之一，打造了交通大模型MogoMind。</span></p>
<p><span>据其官网介绍，旗下自动驾驶巴士MOGOBUS已在中国10余个省份的开放道路、景区及园区实现常态化运营，累计安全行驶里程突破200万公里，服务乘客超20万人次，在联合国全球环境科学家大会、第31届世界大学生夏季运动会、F1中国大奖赛等国际活动和赛事中多次承担自动驾驶接驳服务。</span></p>
<p><span>天眼查APP显示，蘑菇车联信息科技有限公司成立于2019年，注册资本53544万元，万如是公司法定代表人。</span></p>
<p><span>10月初，由MKX Technologies、蘑菇车联（MOGOX）与比亚迪组成的联合体，中标新加坡自动驾驶巴士服务试点项目，这也是新加坡首个L4级自动驾驶巴士官方项目。</span></p>
<p><span>对于付强的加入，朱磊表示：公司长期深耕智能交通与AI基础设施领域，坚定推进“AI网络 + 自动驾驶”双引擎战略，依托自研的 MogoMind 大模型，构建了一张覆盖现实世界的AI网络，能够实时感知、理解并推理世界的变化。付强将为公司AI业务的拓展注入强大动能，加速AI技术在各类应用场景的落地成效，推动公司从技术优势向市场胜势的全面转化。</span></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><span>据科创板日报，前滴滴高级副总裁付强加入蘑菇车联（MOGOX），出任总裁，这也是该公司的首任总裁。据悉，付强将全面负责公司的AI业务的战略规划落地与商业化布局。</span></p>
<p><span>当天下午，蘑菇车联（MOGOX）创始人朱磊发布内部公开信官宣此事。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2f1de808bd.jpg"></p>
<p><span>付强出生于1981年，毕业于四川大学，早年曾在强生、葛兰素史克等跨国公司工作。</span></p>
<p><span>2014年，付强加入快的打车担任运营总监，后升任副总裁。2015年，快的与滴滴合并，付强调任滴滴代驾事业部，担任首任总经理。当时有报道称，在其领导下滴滴代驾业务用半年时间拿下市场份额第一，迅速实现盈利。2016年，付强晋升为滴滴品质出行事业部总经理，统管代驾、专车、豪车等多项业务。2017年付强调任滴滴当时投资的OFO担任执行总裁。2018年回归滴滴，出任高级副总裁，分管网约车平台和城市运输与服务事业群。</span></p>
<p><span>2023年，付强从滴滴离职，加盟聚焦长途货运的满帮集团，担任首席运营官。</span></p>
<p><span>公开信息显示，蘑菇车联（MOGOX）专注于车路云一体化赛道，是在车路协同方向上探索Robotaxi的玩家之一，打造了交通大模型MogoMind。</span></p>
<p><span>据其官网介绍，旗下自动驾驶巴士MOGOBUS已在中国10余个省份的开放道路、景区及园区实现常态化运营，累计安全行驶里程突破200万公里，服务乘客超20万人次，在联合国全球环境科学家大会、第31届世界大学生夏季运动会、F1中国大奖赛等国际活动和赛事中多次承担自动驾驶接驳服务。</span></p>
<p><span>天眼查APP显示，蘑菇车联信息科技有限公司成立于2019年，注册资本53544万元，万如是公司法定代表人。</span></p>
<p><span>10月初，由MKX Technologies、蘑菇车联（MOGOX）与比亚迪组成的联合体，中标新加坡自动驾驶巴士服务试点项目，这也是新加坡首个L4级自动驾驶巴士官方项目。</span></p>
<p><span>对于付强的加入，朱磊表示：公司长期深耕智能交通与AI基础设施领域，坚定推进“AI网络 + 自动驾驶”双引擎战略，依托自研的 MogoMind 大模型，构建了一张覆盖现实世界的AI网络，能够实时感知、理解并推理世界的变化。付强将为公司AI业务的拓展注入强大动能，加速AI技术在各类应用场景的落地成效，推动公司从技术优势向市场胜势的全面转化。</span></p>]]>
    </description>
    <content:encoded><![CDATA[<p><span>据科创板日报，前滴滴高级副总裁付强加入蘑菇车联（MOGOX），出任总裁，这也是该公司的首任总裁。据悉，付强将全面负责公司的AI业务的战略规划落地与商业化布局。</span></p>
<p><span>当天下午，蘑菇车联（MOGOX）创始人朱磊发布内部公开信官宣此事。</span></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2f1de808bd.jpg"></p>
<p><span>付强出生于1981年，毕业于四川大学，早年曾在强生、葛兰素史克等跨国公司工作。</span></p>
<p><span>2014年，付强加入快的打车担任运营总监，后升任副总裁。2015年，快的与滴滴合并，付强调任滴滴代驾事业部，担任首任总经理。当时有报道称，在其领导下滴滴代驾业务用半年时间拿下市场份额第一，迅速实现盈利。2016年，付强晋升为滴滴品质出行事业部总经理，统管代驾、专车、豪车等多项业务。2017年付强调任滴滴当时投资的OFO担任执行总裁。2018年回归滴滴，出任高级副总裁，分管网约车平台和城市运输与服务事业群。</span></p>
<p><span>2023年，付强从滴滴离职，加盟聚焦长途货运的满帮集团，担任首席运营官。</span></p>
<p><span>公开信息显示，蘑菇车联（MOGOX）专注于车路云一体化赛道，是在车路协同方向上探索Robotaxi的玩家之一，打造了交通大模型MogoMind。</span></p>
<p><span>据其官网介绍，旗下自动驾驶巴士MOGOBUS已在中国10余个省份的开放道路、景区及园区实现常态化运营，累计安全行驶里程突破200万公里，服务乘客超20万人次，在联合国全球环境科学家大会、第31届世界大学生夏季运动会、F1中国大奖赛等国际活动和赛事中多次承担自动驾驶接驳服务。</span></p>
<p><span>天眼查APP显示，蘑菇车联信息科技有限公司成立于2019年，注册资本53544万元，万如是公司法定代表人。</span></p>
<p><span>10月初，由MKX Technologies、蘑菇车联（MOGOX）与比亚迪组成的联合体，中标新加坡自动驾驶巴士服务试点项目，这也是新加坡首个L4级自动驾驶巴士官方项目。</span></p>
<p><span>对于付强的加入，朱磊表示：公司长期深耕智能交通与AI基础设施领域，坚定推进“AI网络 + 自动驾驶”双引擎战略，依托自研的 MogoMind 大模型，构建了一张覆盖现实世界的AI网络，能够实时感知、理解并推理世界的变化。付强将为公司AI业务的拓展注入强大动能，加速AI技术在各类应用场景的落地成效，推动公司从技术优势向市场胜势的全面转化。</span></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2f1de808bd.jpg"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2f1de808bd.jpg" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 17:00:14 +0800</pubDate>
  </item><item>
    <title><![CDATA[Git 3.0 有望在 2026 年发布，默认启用更安全的 SHA-256 哈希算法]]></title>
    <link>https://www.oschina.net/news/377170/git-3-0-release-talk-2026</link>
    <itunes:title><![CDATA[Git 3.0 有望在 2026 年发布，默认启用更安全的 SHA-256 哈希算法]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>在近期的 Git 贡献者峰会上，开发者们首次正式讨论了 Git 3.0 的发布时间表——共识倾向于不将发布时间无限拖延，希望在 2026 年完成这一重大版本更新。</p>
<p>据介绍，Git 3.0 所需的大部分功能已基本实现，目前计划在大约一年后发布，即距离当前版本约四个发布周期。 &nbsp;</p>
<blockquote>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1b9d13a892.png"></p>
<p><em>https://lore.kernel.org/all/aNxivuJEnSHbQNdr@fruit.crustytoothpaste.net/</em></p>
</blockquote>
<p>Git 3.0 的核心变化是默认切换至 SHA-256 哈希算法，以取代当前使用已久、存在安全隐患的 SHA-1。新版将带来更强的安全性和完整性验证机制。</p>
<p>不过，Git 团队也强调，SHA-1 与 SHA-256 的互操作性仍在完善中，确保新旧仓库、工具链、以及代码托管平台之间能够平滑协作。</p>
<p>目前，Git 生态系统中许多依赖组件（如托管平台、CI/CD 工具等）仍需时间完成兼容适配。开发者们希望在 3.0 发布时，整个生态能同步支持新算法，避免因哈希不兼容导致的协作问题。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>在近期的 Git 贡献者峰会上，开发者们首次正式讨论了 Git 3.0 的发布时间表——共识倾向于不将发布时间无限拖延，希望在 2026 年完成这一重大版本更新。</p>
<p>据介绍，Git 3.0 所需的大部分功能已基本实现，目前计划在大约一年后发布，即距离当前版本约四个发布周期。 &nbsp;</p>
<blockquote>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1b9d13a892.png"></p>
<p><em>https://lore.kernel.org/all/aNxivuJEnSHbQNdr@fruit.crustytoothpaste.net/</em></p>
</blockquote>
<p>Git 3.0 的核心变化是默认切换至 SHA-256 哈希算法，以取代当前使用已久、存在安全隐患的 SHA-1。新版将带来更强的安全性和完整性验证机制。</p>
<p>不过，Git 团队也强调，SHA-1 与 SHA-256 的互操作性仍在完善中，确保新旧仓库、工具链、以及代码托管平台之间能够平滑协作。</p>
<p>目前，Git 生态系统中许多依赖组件（如托管平台、CI/CD 工具等）仍需时间完成兼容适配。开发者们希望在 3.0 发布时，整个生态能同步支持新算法，避免因哈希不兼容导致的协作问题。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>在近期的 Git 贡献者峰会上，开发者们首次正式讨论了 Git 3.0 的发布时间表——共识倾向于不将发布时间无限拖延，希望在 2026 年完成这一重大版本更新。</p>
<p>据介绍，Git 3.0 所需的大部分功能已基本实现，目前计划在大约一年后发布，即距离当前版本约四个发布周期。 &nbsp;</p>
<blockquote>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1b9d13a892.png"></p>
<p><em>https://lore.kernel.org/all/aNxivuJEnSHbQNdr@fruit.crustytoothpaste.net/</em></p>
</blockquote>
<p>Git 3.0 的核心变化是默认切换至 SHA-256 哈希算法，以取代当前使用已久、存在安全隐患的 SHA-1。新版将带来更强的安全性和完整性验证机制。</p>
<p>不过，Git 团队也强调，SHA-1 与 SHA-256 的互操作性仍在完善中，确保新旧仓库、工具链、以及代码托管平台之间能够平滑协作。</p>
<p>目前，Git 生态系统中许多依赖组件（如托管平台、CI/CD 工具等）仍需时间完成兼容适配。开发者们希望在 3.0 发布时，整个生态能同步支持新算法，避免因哈希不兼容导致的协作问题。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1b9d13a892.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-1b9d13a892.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 16:44:19 +0800</pubDate>
  </item><item>
    <title><![CDATA[OpenAI 与微软达成重磅交易：股权结构再变，投资者面临稀释风险]]></title>
    <link>https://www.oschina.net/news/377167</link>
    <itunes:title><![CDATA[OpenAI 与微软达成重磅交易：股权结构再变，投资者面临稀释风险]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><span>《<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.ft.com%2Fcontent%2F61ab5bc8-a125-4246-9761-80473028a99e" target="_blank">金融时报</a>》报道了 OpenAI 近期一系列重要交易，这些交易让公司的股权结构变得更加复杂，也令投资者对于未来的回报更加不确定。根据这些交易，OpenAI 成为了全球最有价值的非上市公司，估值高达5000亿美元。这一成果主要得益于与芯片制造商英伟达和 AMD 签订的数十亿美元合同，这些资金将助力 OpenAI 在未来几年实现部署1万亿美元算力的目标。</span></p>
<p>然而，知情人士透露，OpenAI 对资金的持续需求意味着现有投资者将在后续融资中面临股权被稀释的风险，投资者包括微软、软银以及兴盛资本等。对于 OpenAI 而言，与微软的谈判尤为关键，这涉及到公司将转变为更传统的营利性企业结构。这样的转型是企业上市前必经之路，也是投资者获得丰厚回报的<span>最佳</span>途径。</p>
<p>在新的股权结构下，微软将成为 OpenAI 的<span>最大</span>单一股东，持股比例约为30%。与此同时，OpenAI 的员工和非营利性母公司也将各占近30% 的股份。这意味着，若按当前估值计算，这两部分股权的价值接近1500亿美元。知情人士称，OpenAI 的非营利母公司将不再享有特殊的股东权利，而是拥有提名营利性子公司董事的权力。这一变更旨在安抚加州和特拉华州的总检察长，以确保转型不损害 OpenAI 的慈善使命。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-46fbcbe164.png"></p>
<p>OpenAI 的 CEO 萨姆・奥特曼预计将获得股份，但具体安排需待公司完成转型后进行谈判。同时，Elon Musk 作为早期投资者，尽管曾向 OpenAI 捐赠了约4500万美元，但并不享有任何股权。他正在提起诉讼，试图阻止 OpenAI 的转型，认为这一举措偏离了创立时的使命。</p>
<p>最终的持股比例和安排仍待 OpenAI、微软及各州总检察长与其他投资者之间的进一步谈判结果。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><span>《<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.ft.com%2Fcontent%2F61ab5bc8-a125-4246-9761-80473028a99e" target="_blank">金融时报</a>》报道了 OpenAI 近期一系列重要交易，这些交易让公司的股权结构变得更加复杂，也令投资者对于未来的回报更加不确定。根据这些交易，OpenAI 成为了全球最有价值的非上市公司，估值高达5000亿美元。这一成果主要得益于与芯片制造商英伟达和 AMD 签订的数十亿美元合同，这些资金将助力 OpenAI 在未来几年实现部署1万亿美元算力的目标。</span></p>
<p>然而，知情人士透露，OpenAI 对资金的持续需求意味着现有投资者将在后续融资中面临股权被稀释的风险，投资者包括微软、软银以及兴盛资本等。对于 OpenAI 而言，与微软的谈判尤为关键，这涉及到公司将转变为更传统的营利性企业结构。这样的转型是企业上市前必经之路，也是投资者获得丰厚回报的<span>最佳</span>途径。</p>
<p>在新的股权结构下，微软将成为 OpenAI 的<span>最大</span>单一股东，持股比例约为30%。与此同时，OpenAI 的员工和非营利性母公司也将各占近30% 的股份。这意味着，若按当前估值计算，这两部分股权的价值接近1500亿美元。知情人士称，OpenAI 的非营利母公司将不再享有特殊的股东权利，而是拥有提名营利性子公司董事的权力。这一变更旨在安抚加州和特拉华州的总检察长，以确保转型不损害 OpenAI 的慈善使命。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-46fbcbe164.png"></p>
<p>OpenAI 的 CEO 萨姆・奥特曼预计将获得股份，但具体安排需待公司完成转型后进行谈判。同时，Elon Musk 作为早期投资者，尽管曾向 OpenAI 捐赠了约4500万美元，但并不享有任何股权。他正在提起诉讼，试图阻止 OpenAI 的转型，认为这一举措偏离了创立时的使命。</p>
<p>最终的持股比例和安排仍待 OpenAI、微软及各州总检察长与其他投资者之间的进一步谈判结果。</p>]]>
    </description>
    <content:encoded><![CDATA[<p><span>《<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.ft.com%2Fcontent%2F61ab5bc8-a125-4246-9761-80473028a99e" target="_blank">金融时报</a>》报道了 OpenAI 近期一系列重要交易，这些交易让公司的股权结构变得更加复杂，也令投资者对于未来的回报更加不确定。根据这些交易，OpenAI 成为了全球最有价值的非上市公司，估值高达5000亿美元。这一成果主要得益于与芯片制造商英伟达和 AMD 签订的数十亿美元合同，这些资金将助力 OpenAI 在未来几年实现部署1万亿美元算力的目标。</span></p>
<p>然而，知情人士透露，OpenAI 对资金的持续需求意味着现有投资者将在后续融资中面临股权被稀释的风险，投资者包括微软、软银以及兴盛资本等。对于 OpenAI 而言，与微软的谈判尤为关键，这涉及到公司将转变为更传统的营利性企业结构。这样的转型是企业上市前必经之路，也是投资者获得丰厚回报的<span>最佳</span>途径。</p>
<p>在新的股权结构下，微软将成为 OpenAI 的<span>最大</span>单一股东，持股比例约为30%。与此同时，OpenAI 的员工和非营利性母公司也将各占近30% 的股份。这意味着，若按当前估值计算，这两部分股权的价值接近1500亿美元。知情人士称，OpenAI 的非营利母公司将不再享有特殊的股东权利，而是拥有提名营利性子公司董事的权力。这一变更旨在安抚加州和特拉华州的总检察长，以确保转型不损害 OpenAI 的慈善使命。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-46fbcbe164.png"></p>
<p>OpenAI 的 CEO 萨姆・奥特曼预计将获得股份，但具体安排需待公司完成转型后进行谈判。同时，Elon Musk 作为早期投资者，尽管曾向 OpenAI 捐赠了约4500万美元，但并不享有任何股权。他正在提起诉讼，试图阻止 OpenAI 的转型，认为这一举措偏离了创立时的使命。</p>
<p>最终的持股比例和安排仍待 OpenAI、微软及各州总检察长与其他投资者之间的进一步谈判结果。</p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-46fbcbe164.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-46fbcbe164.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 16:39:13 +0800</pubDate>
  </item><item>
    <title><![CDATA[多模态 AI 创作平台 LiblibAI 2.0 发布]]></title>
    <link>https://www.oschina.net/news/377165</link>
    <itunes:title><![CDATA[多模态 AI 创作平台 LiblibAI 2.0 发布]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>多模态 AI 创作平台 LiblibAI 2.0 已正式上线，整合了图片生成、视频制作、特效模板等功能，支持 MidJourney、Seedream 4.0 等顶级模型。其核心优势在于提供一站式服务，用户可直接调用 19 种主流 AI 工具，无需切换平台或重复付费。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-e66869e906.png"></p>
<p>平台特别优化了新手体验，新增极简生图功能，支持中文排版和影视级特效制作，限时开放免费算力。目前拥有 2000 万创作者，涵盖摄影、建筑设计等多个领域。</p>
<p>LiblibAI 2.0 主要变化：</p>
<ul>
<li> <p>拥有<strong>极简生成器</strong>，让视频与图像生成在同一界面完成，零学习成本、即点即用；</p> </li>
<li> <p>兼容<strong>开源与闭源模型</strong>，图像方面一次集齐&nbsp;<strong>Qwen Image、F.1、Kontext、Seedream4、MidjourneyV7</strong>等最强阵容；它内置&nbsp;<strong>Kling、Hailuo、Vidu、WAN 等顶级视频模型</strong>，搭载 500+ 独家专业视觉特效，让每一帧都能达到影视级品质；</p> </li>
<li> <p>整合了<strong>全球最大图片风格开源模型库</strong>，覆盖插画、摄影、电商、海报、IP 等各类视觉风格，让最专业的视觉创作者能够快速找到灵感所需的风格与质感，也让初学者在一次次尝试中，逐渐找到属于自己的创作语言。</p> </li>
<li> <p>支持&nbsp;<strong>AI 工作流批量化处理</strong>—— 扩图、高清、换脸、产品精修等高频流程，一键批量完成，效率提升数倍。</p> </li>
</ul>
<p>除了全新的界面与体验，新版也保留了熟悉的 WebUI 和 ComfyUI。用户可以在新的平台上继续原有的工作流，也可以自由组合新的方式，拼一张属于自己的创作桌。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-d5698d79f8.png"></p>
<p>详情查看：<em>https://mp.weixin.qq.com/s/Y8aw2FT61rnPJyOxPoEQKw</em></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>多模态 AI 创作平台 LiblibAI 2.0 已正式上线，整合了图片生成、视频制作、特效模板等功能，支持 MidJourney、Seedream 4.0 等顶级模型。其核心优势在于提供一站式服务，用户可直接调用 19 种主流 AI 工具，无需切换平台或重复付费。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-e66869e906.png"></p>
<p>平台特别优化了新手体验，新增极简生图功能，支持中文排版和影视级特效制作，限时开放免费算力。目前拥有 2000 万创作者，涵盖摄影、建筑设计等多个领域。</p>
<p>LiblibAI 2.0 主要变化：</p>
<ul>
<li> <p>拥有<strong>极简生成器</strong>，让视频与图像生成在同一界面完成，零学习成本、即点即用；</p> </li>
<li> <p>兼容<strong>开源与闭源模型</strong>，图像方面一次集齐&nbsp;<strong>Qwen Image、F.1、Kontext、Seedream4、MidjourneyV7</strong>等最强阵容；它内置&nbsp;<strong>Kling、Hailuo、Vidu、WAN 等顶级视频模型</strong>，搭载 500+ 独家专业视觉特效，让每一帧都能达到影视级品质；</p> </li>
<li> <p>整合了<strong>全球最大图片风格开源模型库</strong>，覆盖插画、摄影、电商、海报、IP 等各类视觉风格，让最专业的视觉创作者能够快速找到灵感所需的风格与质感，也让初学者在一次次尝试中，逐渐找到属于自己的创作语言。</p> </li>
<li> <p>支持&nbsp;<strong>AI 工作流批量化处理</strong>—— 扩图、高清、换脸、产品精修等高频流程，一键批量完成，效率提升数倍。</p> </li>
</ul>
<p>除了全新的界面与体验，新版也保留了熟悉的 WebUI 和 ComfyUI。用户可以在新的平台上继续原有的工作流，也可以自由组合新的方式，拼一张属于自己的创作桌。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-d5698d79f8.png"></p>
<p>详情查看：<em>https://mp.weixin.qq.com/s/Y8aw2FT61rnPJyOxPoEQKw</em></p>]]>
    </description>
    <content:encoded><![CDATA[<p>多模态 AI 创作平台 LiblibAI 2.0 已正式上线，整合了图片生成、视频制作、特效模板等功能，支持 MidJourney、Seedream 4.0 等顶级模型。其核心优势在于提供一站式服务，用户可直接调用 19 种主流 AI 工具，无需切换平台或重复付费。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-e66869e906.png"></p>
<p>平台特别优化了新手体验，新增极简生图功能，支持中文排版和影视级特效制作，限时开放免费算力。目前拥有 2000 万创作者，涵盖摄影、建筑设计等多个领域。</p>
<p>LiblibAI 2.0 主要变化：</p>
<ul>
<li> <p>拥有<strong>极简生成器</strong>，让视频与图像生成在同一界面完成，零学习成本、即点即用；</p> </li>
<li> <p>兼容<strong>开源与闭源模型</strong>，图像方面一次集齐&nbsp;<strong>Qwen Image、F.1、Kontext、Seedream4、MidjourneyV7</strong>等最强阵容；它内置&nbsp;<strong>Kling、Hailuo、Vidu、WAN 等顶级视频模型</strong>，搭载 500+ 独家专业视觉特效，让每一帧都能达到影视级品质；</p> </li>
<li> <p>整合了<strong>全球最大图片风格开源模型库</strong>，覆盖插画、摄影、电商、海报、IP 等各类视觉风格，让最专业的视觉创作者能够快速找到灵感所需的风格与质感，也让初学者在一次次尝试中，逐渐找到属于自己的创作语言。</p> </li>
<li> <p>支持&nbsp;<strong>AI 工作流批量化处理</strong>—— 扩图、高清、换脸、产品精修等高频流程，一键批量完成，效率提升数倍。</p> </li>
</ul>
<p>除了全新的界面与体验，新版也保留了熟悉的 WebUI 和 ComfyUI。用户可以在新的平台上继续原有的工作流，也可以自由组合新的方式，拼一张属于自己的创作桌。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-d5698d79f8.png"></p>
<p>详情查看：<em>https://mp.weixin.qq.com/s/Y8aw2FT61rnPJyOxPoEQKw</em></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-e66869e906.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/static.oschina.net-e66869e906.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 16:34:17 +0800</pubDate>
  </item><item>
    <title><![CDATA[开源 AI 客户端 Cherry Studio 已集成 GitHub Copilot CLI]]></title>
    <link>https://www.oschina.net/news/377161</link>
    <itunes:title><![CDATA[开源 AI 客户端 Cherry Studio 已集成 GitHub Copilot CLI]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>Cherry Studio 是一款跨平台的 AI 桌面应用，支持 Windows、macOS 和 Linux 系统。<strong>Cherry Studio v1.6.3</strong>&nbsp;的版本更新已发布，重点增强了开发者工具、扩展了模型支持，并对笔记功能进行了多项实用性优化。</p>
<hr>
<h4><strong>主要更新</strong></h4>
<p>1.&nbsp;<strong>集成 GitHub Copilot CLI</strong></p>
<ul>
<li>现在，您可以在 Cherry Studio 的编码工具中直接调用 GitHub Copilot CLI，辅助进行代码分析与问题排查，提升开发效率。</li>
</ul>
<p>2.&nbsp;<strong>新增模型供应商支持</strong></p>
<ul>
<li><strong>LongCat</strong>：新增对长乘科技（LongCat）大模型的支持。</li>
<li><strong>Intel OVMS</strong>：新增对 Intel OpenVINO™ Model Server 的支持，为在英特尔硬件上部署模型的用户提供更多选择。</li>
</ul>
<p>3.&nbsp;<strong>笔记功能增强</strong></p>
<ul>
<li><strong>笔记导出</strong>：支持将单篇笔记内容导出，方便用户备份和迁移。</li>
<li><strong>LLM 智能重命名</strong>：可调用语言模型，根据笔记内容自动生成标题。</li>
<li><strong>表格自动换行</strong>：笔记中的表格内容现已支持自动换行，优化了长文本的阅读体验。</li>
<li><strong>拼写检查控制</strong>：新增拼写检查功能的开关，用户可根据需要自行启用或关闭。</li>
</ul>
<hr>
<h4><strong>功能优化</strong></h4>
<ul>
<li> <p><strong>模型兼容性</strong>：</p>
<ul>
<li>新增对&nbsp;gpt-5-codex&nbsp;的支持。</li>
<li>修复并兼容&nbsp;DeepSeek v3.2、Claude 4.5、GLM 4.6&nbsp;等新版本模型。</li>
<li>更新了 Google Gemini 模型的识别规则。</li>
</ul> </li>
<li> <p><strong>用户体验</strong>：</p>
<ul>
<li><strong>标签页操作</strong>：支持使用鼠标中键关闭标签页。</li>
<li><strong>界面交互</strong>：扩大了话题原地重命名的可点击区域；修复了并排视图下的多余滚动条和消息菜单栏溢出的问题。</li>
<li><strong>编辑器</strong>：文本文件预览时编辑器设为只读，但内容可复制。</li>
</ul> </li>
<li> <p><strong>技术与底层</strong>：</p>
<ul>
<li><strong>工具使用 (Tool Use)</strong>：将模型供应商定义的工具与 Prompt 内工具进行分离，使结构更清晰。</li>
<li><strong>性能</strong>：恢复了首 Token 延迟的性能指标上报，以便于监控和优化。</li>
<li><strong>依赖更新</strong>：更新了 Electron 版本及其他核心依赖，提升了应用的稳定性与安全性。</li>
</ul> </li>
</ul>
<hr>
<h4><strong>问题修复</strong></h4>
<ul>
<li>修复了从主题 Fork 后，重命名弹窗中仍保留旧名称的问题。</li>
<li>修复了视觉模型（Vision）相关的一些问题。</li>
<li>修复了部分 UI 显示异常。</li>
</ul>
<p>下载地址：<em>https://github.com/CherryHQ/cherry-studio/releases</em></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>Cherry Studio 是一款跨平台的 AI 桌面应用，支持 Windows、macOS 和 Linux 系统。<strong>Cherry Studio v1.6.3</strong>&nbsp;的版本更新已发布，重点增强了开发者工具、扩展了模型支持，并对笔记功能进行了多项实用性优化。</p>
<hr>
<h4><strong>主要更新</strong></h4>
<p>1.&nbsp;<strong>集成 GitHub Copilot CLI</strong></p>
<ul>
<li>现在，您可以在 Cherry Studio 的编码工具中直接调用 GitHub Copilot CLI，辅助进行代码分析与问题排查，提升开发效率。</li>
</ul>
<p>2.&nbsp;<strong>新增模型供应商支持</strong></p>
<ul>
<li><strong>LongCat</strong>：新增对长乘科技（LongCat）大模型的支持。</li>
<li><strong>Intel OVMS</strong>：新增对 Intel OpenVINO™ Model Server 的支持，为在英特尔硬件上部署模型的用户提供更多选择。</li>
</ul>
<p>3.&nbsp;<strong>笔记功能增强</strong></p>
<ul>
<li><strong>笔记导出</strong>：支持将单篇笔记内容导出，方便用户备份和迁移。</li>
<li><strong>LLM 智能重命名</strong>：可调用语言模型，根据笔记内容自动生成标题。</li>
<li><strong>表格自动换行</strong>：笔记中的表格内容现已支持自动换行，优化了长文本的阅读体验。</li>
<li><strong>拼写检查控制</strong>：新增拼写检查功能的开关，用户可根据需要自行启用或关闭。</li>
</ul>
<hr>
<h4><strong>功能优化</strong></h4>
<ul>
<li> <p><strong>模型兼容性</strong>：</p>
<ul>
<li>新增对&nbsp;gpt-5-codex&nbsp;的支持。</li>
<li>修复并兼容&nbsp;DeepSeek v3.2、Claude 4.5、GLM 4.6&nbsp;等新版本模型。</li>
<li>更新了 Google Gemini 模型的识别规则。</li>
</ul> </li>
<li> <p><strong>用户体验</strong>：</p>
<ul>
<li><strong>标签页操作</strong>：支持使用鼠标中键关闭标签页。</li>
<li><strong>界面交互</strong>：扩大了话题原地重命名的可点击区域；修复了并排视图下的多余滚动条和消息菜单栏溢出的问题。</li>
<li><strong>编辑器</strong>：文本文件预览时编辑器设为只读，但内容可复制。</li>
</ul> </li>
<li> <p><strong>技术与底层</strong>：</p>
<ul>
<li><strong>工具使用 (Tool Use)</strong>：将模型供应商定义的工具与 Prompt 内工具进行分离，使结构更清晰。</li>
<li><strong>性能</strong>：恢复了首 Token 延迟的性能指标上报，以便于监控和优化。</li>
<li><strong>依赖更新</strong>：更新了 Electron 版本及其他核心依赖，提升了应用的稳定性与安全性。</li>
</ul> </li>
</ul>
<hr>
<h4><strong>问题修复</strong></h4>
<ul>
<li>修复了从主题 Fork 后，重命名弹窗中仍保留旧名称的问题。</li>
<li>修复了视觉模型（Vision）相关的一些问题。</li>
<li>修复了部分 UI 显示异常。</li>
</ul>
<p>下载地址：<em>https://github.com/CherryHQ/cherry-studio/releases</em></p>]]>
    </description>
    <content:encoded><![CDATA[<p>Cherry Studio 是一款跨平台的 AI 桌面应用，支持 Windows、macOS 和 Linux 系统。<strong>Cherry Studio v1.6.3</strong>&nbsp;的版本更新已发布，重点增强了开发者工具、扩展了模型支持，并对笔记功能进行了多项实用性优化。</p>
<hr>
<h4><strong>主要更新</strong></h4>
<p>1.&nbsp;<strong>集成 GitHub Copilot CLI</strong></p>
<ul>
<li>现在，您可以在 Cherry Studio 的编码工具中直接调用 GitHub Copilot CLI，辅助进行代码分析与问题排查，提升开发效率。</li>
</ul>
<p>2.&nbsp;<strong>新增模型供应商支持</strong></p>
<ul>
<li><strong>LongCat</strong>：新增对长乘科技（LongCat）大模型的支持。</li>
<li><strong>Intel OVMS</strong>：新增对 Intel OpenVINO™ Model Server 的支持，为在英特尔硬件上部署模型的用户提供更多选择。</li>
</ul>
<p>3.&nbsp;<strong>笔记功能增强</strong></p>
<ul>
<li><strong>笔记导出</strong>：支持将单篇笔记内容导出，方便用户备份和迁移。</li>
<li><strong>LLM 智能重命名</strong>：可调用语言模型，根据笔记内容自动生成标题。</li>
<li><strong>表格自动换行</strong>：笔记中的表格内容现已支持自动换行，优化了长文本的阅读体验。</li>
<li><strong>拼写检查控制</strong>：新增拼写检查功能的开关，用户可根据需要自行启用或关闭。</li>
</ul>
<hr>
<h4><strong>功能优化</strong></h4>
<ul>
<li> <p><strong>模型兼容性</strong>：</p>
<ul>
<li>新增对&nbsp;gpt-5-codex&nbsp;的支持。</li>
<li>修复并兼容&nbsp;DeepSeek v3.2、Claude 4.5、GLM 4.6&nbsp;等新版本模型。</li>
<li>更新了 Google Gemini 模型的识别规则。</li>
</ul> </li>
<li> <p><strong>用户体验</strong>：</p>
<ul>
<li><strong>标签页操作</strong>：支持使用鼠标中键关闭标签页。</li>
<li><strong>界面交互</strong>：扩大了话题原地重命名的可点击区域；修复了并排视图下的多余滚动条和消息菜单栏溢出的问题。</li>
<li><strong>编辑器</strong>：文本文件预览时编辑器设为只读，但内容可复制。</li>
</ul> </li>
<li> <p><strong>技术与底层</strong>：</p>
<ul>
<li><strong>工具使用 (Tool Use)</strong>：将模型供应商定义的工具与 Prompt 内工具进行分离，使结构更清晰。</li>
<li><strong>性能</strong>：恢复了首 Token 延迟的性能指标上报，以便于监控和优化。</li>
<li><strong>依赖更新</strong>：更新了 Electron 版本及其他核心依赖，提升了应用的稳定性与安全性。</li>
</ul> </li>
</ul>
<hr>
<h4><strong>问题修复</strong></h4>
<ul>
<li>修复了从主题 Fork 后，重命名弹窗中仍保留旧名称的问题。</li>
<li>修复了视觉模型（Vision）相关的一些问题。</li>
<li>修复了部分 UI 显示异常。</li>
</ul>
<p>下载地址：<em>https://github.com/CherryHQ/cherry-studio/releases</em></p>]]></content:encoded>
    
    <pubDate>Mon, 13 Oct 2025 16:23:28 +0800</pubDate>
  </item><item>
    <title><![CDATA[优麒麟 25.10 版本正式发布]]></title>
    <link>https://www.oschina.net/news/377149</link>
    <itunes:title><![CDATA[优麒麟 25.10 版本正式发布]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>优麒麟 25.10 现已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FK42ylplwPn0GBq-iaFvUDw" target="_blank">发布</a>，该版本将支持9个月，基于Linux 6.17内核构建，对基础库、子系统和核心软件等进行了重大升级，增强了系统的稳定性和兼容性，并上线全新的软件商店。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8ef845a676.png"></p>
<h4><strong>新功能</strong></h4>
<p><strong>1. Linux 6.17 内核</strong></p>
<p>优麒麟25.10 预装了 Linux 6.17 内核，引入了一系列增强功能：</p>
<ul>
<li> <p>支持 Intel Panther Lake 显卡</p> </li>
<li> <p>支持 AMD SmartMux 混合显卡切换</p> </li>
<li> <p>新增 Raspberry Pi RP1 主线支持</p> </li>
<li> <p>Btrfs 新增实验性大页支持</p> </li>
<li> <p>EXT4 块分配可扩展性提升</p> </li>
<li> <p>优化 SSD 写入，延长寿命</p> </li>
<li> <p>引入代理执行初始支持</p> </li>
<li> <p>移除单处理器 (UP) 内核支持</p> </li>
<li> <p>新增 file_getattr 和 file_setattr 系统调用</p> </li>
<li> <p>支持 DualPI2 拥塞控制协议</p> </li>
<li> <p>AppArmor 增强 AF_UNIX 套接字控制</p> </li>
</ul>
<p><strong>2. Systemd V257.9 版本</strong></p>
<p>systemd更新至v257.9，对比25.04默认的257.4版本，主要是Bug修复，具体改动如下：</p>
<ul>
<li> <p>在 systemd-resolved 中默认禁用 DNSSEC</p> </li>
<li> <p>在 systemd-resolved 重载时重新创建单播作用域链接</p> </li>
<li> <p>在 nsswitch.conf 中将 libnss-systemd.nss 安装在 'compat' 之后</p> </li>
<li> <p>将 systemd-boot-tools 架构改为 linux-any，支持跨架构构建</p> </li>
<li> <p>在 get_timezones() 函数中验证时区有效性</p> </li>
<li> <p>新增 systemd-resolved-dnssec 二进制包，提供 DNSSEC 配置</p> </li>
</ul>
<p><strong>3. 工具链升级</strong></p>
<ul>
<li> <p>GCC更新到15.2, binutils更新到2.45, glibc更新到2.42</p> </li>
<li> <p>Python更新到3.13.7</p> </li>
<li> <p>LLVM 默认版本为20</p> </li>
<li> <p>Rust 默认版本为1.85</p> </li>
<li> <p>Golang更新到1.24</p> </li>
<li> <p>OpenJDK默认版本为21</p> </li>
<li> <p>首次引入Zig，版本为0.14</p> </li>
</ul>
<p><strong>4.软件商店</strong></p>
<p>全新改版的软件商店，经过UI升级和功能重构，使得用户界面更加简洁直观，提高了页面利用率应用管理更轻松，用户可以更轻松地下载应用，并且应用管理功能也得到了优化，变得更加便捷。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-85b20e604d.png"></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>优麒麟 25.10 现已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FK42ylplwPn0GBq-iaFvUDw" target="_blank">发布</a>，该版本将支持9个月，基于Linux 6.17内核构建，对基础库、子系统和核心软件等进行了重大升级，增强了系统的稳定性和兼容性，并上线全新的软件商店。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8ef845a676.png"></p>
<h4><strong>新功能</strong></h4>
<p><strong>1. Linux 6.17 内核</strong></p>
<p>优麒麟25.10 预装了 Linux 6.17 内核，引入了一系列增强功能：</p>
<ul>
<li> <p>支持 Intel Panther Lake 显卡</p> </li>
<li> <p>支持 AMD SmartMux 混合显卡切换</p> </li>
<li> <p>新增 Raspberry Pi RP1 主线支持</p> </li>
<li> <p>Btrfs 新增实验性大页支持</p> </li>
<li> <p>EXT4 块分配可扩展性提升</p> </li>
<li> <p>优化 SSD 写入，延长寿命</p> </li>
<li> <p>引入代理执行初始支持</p> </li>
<li> <p>移除单处理器 (UP) 内核支持</p> </li>
<li> <p>新增 file_getattr 和 file_setattr 系统调用</p> </li>
<li> <p>支持 DualPI2 拥塞控制协议</p> </li>
<li> <p>AppArmor 增强 AF_UNIX 套接字控制</p> </li>
</ul>
<p><strong>2. Systemd V257.9 版本</strong></p>
<p>systemd更新至v257.9，对比25.04默认的257.4版本，主要是Bug修复，具体改动如下：</p>
<ul>
<li> <p>在 systemd-resolved 中默认禁用 DNSSEC</p> </li>
<li> <p>在 systemd-resolved 重载时重新创建单播作用域链接</p> </li>
<li> <p>在 nsswitch.conf 中将 libnss-systemd.nss 安装在 'compat' 之后</p> </li>
<li> <p>将 systemd-boot-tools 架构改为 linux-any，支持跨架构构建</p> </li>
<li> <p>在 get_timezones() 函数中验证时区有效性</p> </li>
<li> <p>新增 systemd-resolved-dnssec 二进制包，提供 DNSSEC 配置</p> </li>
</ul>
<p><strong>3. 工具链升级</strong></p>
<ul>
<li> <p>GCC更新到15.2, binutils更新到2.45, glibc更新到2.42</p> </li>
<li> <p>Python更新到3.13.7</p> </li>
<li> <p>LLVM 默认版本为20</p> </li>
<li> <p>Rust 默认版本为1.85</p> </li>
<li> <p>Golang更新到1.24</p> </li>
<li> <p>OpenJDK默认版本为21</p> </li>
<li> <p>首次引入Zig，版本为0.14</p> </li>
</ul>
<p><strong>4.软件商店</strong></p>
<p>全新改版的软件商店，经过UI升级和功能重构，使得用户界面更加简洁直观，提高了页面利用率应用管理更轻松，用户可以更轻松地下载应用，并且应用管理功能也得到了优化，变得更加便捷。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-85b20e604d.png"></p>]]>
    </description>
    <content:encoded><![CDATA[<p>优麒麟 25.10 现已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FK42ylplwPn0GBq-iaFvUDw" target="_blank">发布</a>，该版本将支持9个月，基于Linux 6.17内核构建，对基础库、子系统和核心软件等进行了重大升级，增强了系统的稳定性和兼容性，并上线全新的软件商店。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8ef845a676.png"></p>
<h4><strong>新功能</strong></h4>
<p><strong>1. Linux 6.17 内核</strong></p>
<p>优麒麟25.10 预装了 Linux 6.17 内核，引入了一系列增强功能：</p>
<ul>
<li> <p>支持 Intel Panther Lake 显卡</p> </li>
<li> <p>支持 AMD SmartMux 混合显卡切换</p> </li>
<li> <p>新增 Raspberry Pi RP1 主线支持</p> </li>
<li> <p>Btrfs 新增实验性大页支持</p> </li>
<li> <p>EXT4 块分配可扩展性提升</p> </li>
<li> <p>优化 SSD 写入，延长寿命</p> </li>
<li> <p>引入代理执行初始支持</p> </li>
<li> <p>移除单处理器 (UP) 内核支持</p> </li>
<li> <p>新增 file_getattr 和 file_setattr 系统调用</p> </li>
<li> <p>支持 DualPI2 拥塞控制协议</p> </li>
<li> <p>AppArmor 增强 AF_UNIX 套接字控制</p> </li>
</ul>
<p><strong>2. Systemd V257.9 版本</strong></p>
<p>systemd更新至v257.9，对比25.04默认的257.4版本，主要是Bug修复，具体改动如下：</p>
<ul>
<li> <p>在 systemd-resolved 中默认禁用 DNSSEC</p> </li>
<li> <p>在 systemd-resolved 重载时重新创建单播作用域链接</p> </li>
<li> <p>在 nsswitch.conf 中将 libnss-systemd.nss 安装在 'compat' 之后</p> </li>
<li> <p>将 systemd-boot-tools 架构改为 linux-any，支持跨架构构建</p> </li>
<li> <p>在 get_timezones() 函数中验证时区有效性</p> </li>
<li> <p>新增 systemd-resolved-dnssec 二进制包，提供 DNSSEC 配置</p> </li>
</ul>
<p><strong>3. 工具链升级</strong></p>
<ul>
<li> <p>GCC更新到15.2, binutils更新到2.45, glibc更新到2.42</p> </li>
<li> <p>Python更新到3.13.7</p> </li>
<li> <p>LLVM 默认版本为20</p> </li>
<li> <p>Rust 默认版本为1.85</p> </li>
<li> <p>Golang更新到1.24</p> </li>
<li> <p>OpenJDK默认版本为21</p> </li>
<li> <p>首次引入Zig，版本为0.14</p> </li>
</ul>
<p><strong>4.软件商店</strong></p>
<p>全新改版的软件商店，经过UI升级和功能重构，使得用户界面更加简洁直观，提高了页面利用率应用管理更轻松，用户可以更轻松地下载应用，并且应用管理功能也得到了优化，变得更加便捷。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-85b20e604d.png"></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8ef845a676.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8ef845a676.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 15:54:31 +0800</pubDate>
  </item><item>
    <title><![CDATA[Wireshark 4.6 发布，最强开源网络数据包分析器]]></title>
    <link>https://www.oschina.net/news/377146/wireshark-4-6-released</link>
    <itunes:title><![CDATA[Wireshark 4.6 发布，最强开源网络数据包分析器]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p>Wireshark 4.6.0 现已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.wireshark.org%2Farchives%2Fwireshark-announce%2F202510%2Fmsg00000.html" target="_blank">发布</a>，一些具体更新内容如下：</p>
<p>以下功能为自&nbsp;<strong>4.6.0rc1</strong>&nbsp;以来的新功能或重大更新：</p>
<ul>
<li>Wireshark 现在可以解析 macOS 上&nbsp;<code>tcpdump</code>&nbsp;提供的进程信息、数据包元数据、流 ID、丢包信息等内容。</li>
</ul>
<p>以下功能为自&nbsp;<strong>4.4.0</strong>&nbsp;以来的新功能或重大更新：</p>
<ul>
<li>Windows 安装包现附带&nbsp;<strong>Npcap 1.83</strong>（之前为 1.79）。</li>
<li>Windows 和 macOS 安装包现附带&nbsp;<strong>Qt 6.9.3</strong>（之前为 6.5.3）。</li>
<li>macOS 版本现提供&nbsp;<strong>通用安装包（Universal Installer）</strong>，不再区分 Arm64 与 Intel 架构。Issue 17294</li>
<li> <p><strong>WinPcap 不再受支持</strong>。在 Windows 上请改用 Npcap，并卸载 WinPcap。WinPcap 最后一个版本为 4.1.3（2013 年发布），仅支持至 Windows 8，该系统现已被 Microsoft 与 Wireshark 停止支持。</p> </li>
<li> <p>新增&nbsp;<strong>“Plots（散点图）”对话框</strong>，与 “I/O Graphs（直方图）” 相比，可展示多种散点数据，支持多图、标记及自动滚动。</p> </li>
<li> <p><strong>实时捕获可压缩写入</strong>。此前仅在多文件捕获轮换时可压缩。现在&nbsp;<code>TShark</code>&nbsp;的&nbsp;<code>--compress</code>&nbsp;参数可在实时捕获中使用。Issue 9311</p> </li>
<li> <p><strong>绝对时间字段</strong>（Absolute time fields）在使用&nbsp;<code>-T json</code>&nbsp;输出时，无论在“数据包详情”中如何显示，均使用 UTC 的 ISO 8601 格式。这在&nbsp;<code>-T ek</code>&nbsp;中自 4.2.0 版起已启用。</p> </li>
<li> <p><strong>时间显示格式更新</strong>：<code>-T fields</code>、<code>-T pdml</code>、自定义列或 CSV 输出中，不再使用&nbsp;<code>asctime</code>&nbsp;样式（如&nbsp;<code>Dec 18, 2017 05:28:39 EST</code>），改用 ISO 8601。为向后兼容，可通过&nbsp;<code>protocols.display_abs_time_ascii</code>&nbsp;选项切换显示方式。未来版本可能彻底移除&nbsp;<code>asctime</code>&nbsp;格式。</p> </li>
<li> <p><strong>UTC 时间列</strong>&nbsp;现在按 ISO 8601 标准添加 “Z” 后缀。</p> </li>
<li> <p><strong>TShark&nbsp;<code>-G</code>&nbsp;参数增强</strong>：不再要求必须为第一个参数，并支持与&nbsp;<code>-o</code>、<code>-d</code>、<code>--disable-protocol</code>&nbsp;等其他选项配合使用。</p> </li>
<li> <p><strong>EUI-64 字段类型</strong>&nbsp;改为字节（bytes）类型，以便进行切片与匹配（如&nbsp;<code>wpan.src64[:3] == eth.src[:3]</code>）。</p> </li>
<li> <p><strong>NTP 解密支持 NTS（Network Time Security）</strong>&nbsp;协议 (sysin)，前提是捕获中包含 NTS-KE 包和 TLS 密钥。</p> </li>
<li> <p><strong>MACsec 解密增强</strong>：支持使用 MKA 解封的 SAK 或在 MACsec 解析器中配置的 PSK。</p> </li>
<li> <p><strong>TCP 流图（Stream Graph）坐标轴</strong>&nbsp;现采用 SI 单位。Issue 20197</p> </li>
<li> <p><strong>自定义列增强</strong>：可选择使用与“数据包详情”相同的格式显示值，并支持复杂表达式按数值排序。</p> </li>
<li> <p>新增显示过滤函数&nbsp;<strong><code>float</code>&nbsp;与&nbsp;<code>double</code></strong>，支持类型显式转换与算术操作。</p> </li>
<li> <p><strong>I/O 图窗口</strong>&nbsp;最小宽度减小，更适合低分辨率桌面。Issue 20147</p> </li>
<li> <p><strong>X.509 证书导出</strong>：可通过菜单&nbsp;<strong>File › Export Objects › X509AF</strong>&nbsp;导出证书，也可在 TShark 中使用&nbsp;<code>--export-objects x509af</code>。</p> </li>
<li> <p><strong>HTTP / HTTP2</strong>&nbsp;解析器新增对&nbsp;<strong>Zstandard 压缩编码</strong>&nbsp;的支持。</p> </li>
<li> <p><strong>跟踪流（Follow Stream）</strong>&nbsp;现支持&nbsp;<strong>MPEG-2 TS PID</strong>&nbsp;与内部&nbsp;<strong>Packetized Elementary Streams</strong>。</p> </li>
<li> <p><strong>DNP3（分布式网络协议 3）</strong>&nbsp;现支持在会话与端点表中显示。</p> </li>
<li> <p><strong>Lua 改进</strong>：内置的&nbsp;<code>bit</code>&nbsp;与&nbsp;<code>rex_pcre2</code>&nbsp;库现会自动加入&nbsp;<code>package.loaded</code>，可通过&nbsp;<code>require</code>&nbsp;调用。Issue 20213</p> </li>
<li> <p><strong>数据包列表</strong>&nbsp;与&nbsp;<strong>事件列表</strong>&nbsp;不再支持多行显示。Issue 14424</p> </li>
<li> <p><strong>ethers 文件</strong>&nbsp;现支持&nbsp;<strong>EUI-64 映射</strong>。Issue 15487</p> </li>
<li> <p><strong>Hex Dump 导入功能</strong>&nbsp;支持 2–4 字节分组（含小端模式），并识别带&nbsp;<code>0x</code>&nbsp;前缀的偏移。Issue 16193</p> </li>
<li> <p><strong>Hex Dump 导出</strong>&nbsp;现可包含时间戳前缀（Wireshark 的“打印”与“导出”对话框，以及 TShark 的&nbsp;<code>--hexdump time</code>&nbsp;选项）。Issue 17132</p> </li>
<li> <p><strong>Lua 新增&nbsp;<code>Conversation</code>&nbsp;对象</strong>，可在脚本中访问会话数据 (sysin)。Issue 15396</p> </li>
<li> <p>新增&nbsp;<strong>“编辑 › 复制 › 作为 HTML”</strong>&nbsp;菜单项，可自定义复制格式与键盘快捷键。</p> </li>
<li> <p>GUI 导出对话框新增 “无重复键 JSON 输出” 选项。</p> </li>
<li> <p>导出对话框可输出&nbsp;<strong>原始十六进制字节数据</strong>。</p> </li>
<li> <p>会话与端点表新增显示&nbsp;<strong>精确字节与比特率</strong>&nbsp;的选项，由偏好项&nbsp;<code>conv.machine_readable</code>&nbsp;控制。</p> </li>
<li> <p><code>-z &lt;tap&gt;,tree</code>&nbsp;的 TShark 统计输出格式可通过&nbsp;<code>-o statistics.output_format</code>&nbsp;控制。</p> </li>
<li> <p><strong>配色方案可独立设置浅色/深色模式</strong>，与系统设置无关（需 Qt 6.8+）。Issue 19328</p> </li>
<li> <p><strong>libxml2</strong>&nbsp;现为构建必需依赖（不支持 2.15.0 版本）。</p> </li>
<li> <p><strong>View 菜单新增“重新解析数据包（Redissect Packets）”选项</strong>，适用于地址解析或解密密钥更新后。</p> </li>
<li> <p><strong>HTTP2 可选跟踪 3GPP 会话（5G SBI）</strong>。</p> </li>
<li> <p><strong>Windows 构建文档不再需要 Java。</strong></p> </li>
<li> <p><strong>Linux</strong>上捕获过滤器支持 BPF 扩展（如<code>inbound</code>、<code>outbound</code>、<code>ifindex</code>）。</p> </li>
</ul>
<p>&nbsp;<strong>移除的功能与支持</strong></p>
<ul>
<li> <p>不再支持&nbsp;<strong>AirPcap</strong>&nbsp;与&nbsp;<strong>WinPcap</strong>。</p> </li>
<li> <p>不再支持&nbsp;<strong>libnl v1 / v2</strong>。</p> </li>
<li> <p><strong>CMake 选项&nbsp;<code>ENABLE_STATIC</code>&nbsp;已弃用</strong>，请改用&nbsp;<code>BUILD_SHARED_LIBS</code>。</p> </li>
</ul>
<p><strong>新增文件格式解析支持</strong></p>
<ul>
<li> <p>RIFF（资源交换文件格式）</p> </li>
<li> <p>TTL 文件格式</p> </li>
</ul>
<p><strong>新增协议支持</strong></p>
<p>支持以下协议：</p>
<p>Asymmetric &nbsp;Key Packages (AKP)、Binary HTTP、BIST TotalView-ITCH、BIST &nbsp;TotalView-OUCH、Bluetooth Android HCI、Bluetooth Intel HCI、BPSec COSE &nbsp;Context、BPSec Default SC、Commsignia Capture Protocol (C2P)、DECT &nbsp;NR+、DLMS/COSEM、Ephemeral Diffie-Hellman Over COSE、ILNP、LDA Neo Device &nbsp;Trailer、Lenbrook Service Discovery Protocol、LLC &nbsp;V1、Navitrol、NTS-KE、Ouster VLP-16、Private Line Emulation、RC &nbsp;V3、RCG、Roughtime、SBAS L5、SGP.22、SGP.32、SICK CoLA Ascii/Binary、Silabs &nbsp;Debug Channel、XCP、USB-PTP、VLP-16、vSomeIP Internal Protocol 等。</p>
<p><strong>更新的协议支持</strong></p>
<p>协议更新数量众多，此处不一一列出。</p>
<p><strong>新增与更新的捕获文件支持</strong></p>
<ul>
<li>改进了&nbsp;<strong>BLF 文件格式</strong>（包括写入功能）。</li>
</ul>
<p><strong>新增与更新的捕获接口支持</strong></p>
<ul>
<li><strong>Windows：</strong>&nbsp;改进了&nbsp;<code>etwdump</code>&nbsp;的用户体验，显示未知事件的原始字节内容。</li>
</ul>
<p><strong>主要 API 变更</strong></p>
<ul>
<li><strong>Lua API</strong>&nbsp;现支持&nbsp;<strong>Libgcrypt 对称加密函数</strong>。</li>
</ul>
<p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.wireshark.org%2Farchives%2Fwireshark-announce%2F202510%2Fmsg00000.html" target="_blank">查看官方公告</a>。</p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p>Wireshark 4.6.0 现已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.wireshark.org%2Farchives%2Fwireshark-announce%2F202510%2Fmsg00000.html" target="_blank">发布</a>，一些具体更新内容如下：</p>
<p>以下功能为自&nbsp;<strong>4.6.0rc1</strong>&nbsp;以来的新功能或重大更新：</p>
<ul>
<li>Wireshark 现在可以解析 macOS 上&nbsp;<code>tcpdump</code>&nbsp;提供的进程信息、数据包元数据、流 ID、丢包信息等内容。</li>
</ul>
<p>以下功能为自&nbsp;<strong>4.4.0</strong>&nbsp;以来的新功能或重大更新：</p>
<ul>
<li>Windows 安装包现附带&nbsp;<strong>Npcap 1.83</strong>（之前为 1.79）。</li>
<li>Windows 和 macOS 安装包现附带&nbsp;<strong>Qt 6.9.3</strong>（之前为 6.5.3）。</li>
<li>macOS 版本现提供&nbsp;<strong>通用安装包（Universal Installer）</strong>，不再区分 Arm64 与 Intel 架构。Issue 17294</li>
<li> <p><strong>WinPcap 不再受支持</strong>。在 Windows 上请改用 Npcap，并卸载 WinPcap。WinPcap 最后一个版本为 4.1.3（2013 年发布），仅支持至 Windows 8，该系统现已被 Microsoft 与 Wireshark 停止支持。</p> </li>
<li> <p>新增&nbsp;<strong>“Plots（散点图）”对话框</strong>，与 “I/O Graphs（直方图）” 相比，可展示多种散点数据，支持多图、标记及自动滚动。</p> </li>
<li> <p><strong>实时捕获可压缩写入</strong>。此前仅在多文件捕获轮换时可压缩。现在&nbsp;<code>TShark</code>&nbsp;的&nbsp;<code>--compress</code>&nbsp;参数可在实时捕获中使用。Issue 9311</p> </li>
<li> <p><strong>绝对时间字段</strong>（Absolute time fields）在使用&nbsp;<code>-T json</code>&nbsp;输出时，无论在“数据包详情”中如何显示，均使用 UTC 的 ISO 8601 格式。这在&nbsp;<code>-T ek</code>&nbsp;中自 4.2.0 版起已启用。</p> </li>
<li> <p><strong>时间显示格式更新</strong>：<code>-T fields</code>、<code>-T pdml</code>、自定义列或 CSV 输出中，不再使用&nbsp;<code>asctime</code>&nbsp;样式（如&nbsp;<code>Dec 18, 2017 05:28:39 EST</code>），改用 ISO 8601。为向后兼容，可通过&nbsp;<code>protocols.display_abs_time_ascii</code>&nbsp;选项切换显示方式。未来版本可能彻底移除&nbsp;<code>asctime</code>&nbsp;格式。</p> </li>
<li> <p><strong>UTC 时间列</strong>&nbsp;现在按 ISO 8601 标准添加 “Z” 后缀。</p> </li>
<li> <p><strong>TShark&nbsp;<code>-G</code>&nbsp;参数增强</strong>：不再要求必须为第一个参数，并支持与&nbsp;<code>-o</code>、<code>-d</code>、<code>--disable-protocol</code>&nbsp;等其他选项配合使用。</p> </li>
<li> <p><strong>EUI-64 字段类型</strong>&nbsp;改为字节（bytes）类型，以便进行切片与匹配（如&nbsp;<code>wpan.src64[:3] == eth.src[:3]</code>）。</p> </li>
<li> <p><strong>NTP 解密支持 NTS（Network Time Security）</strong>&nbsp;协议 (sysin)，前提是捕获中包含 NTS-KE 包和 TLS 密钥。</p> </li>
<li> <p><strong>MACsec 解密增强</strong>：支持使用 MKA 解封的 SAK 或在 MACsec 解析器中配置的 PSK。</p> </li>
<li> <p><strong>TCP 流图（Stream Graph）坐标轴</strong>&nbsp;现采用 SI 单位。Issue 20197</p> </li>
<li> <p><strong>自定义列增强</strong>：可选择使用与“数据包详情”相同的格式显示值，并支持复杂表达式按数值排序。</p> </li>
<li> <p>新增显示过滤函数&nbsp;<strong><code>float</code>&nbsp;与&nbsp;<code>double</code></strong>，支持类型显式转换与算术操作。</p> </li>
<li> <p><strong>I/O 图窗口</strong>&nbsp;最小宽度减小，更适合低分辨率桌面。Issue 20147</p> </li>
<li> <p><strong>X.509 证书导出</strong>：可通过菜单&nbsp;<strong>File › Export Objects › X509AF</strong>&nbsp;导出证书，也可在 TShark 中使用&nbsp;<code>--export-objects x509af</code>。</p> </li>
<li> <p><strong>HTTP / HTTP2</strong>&nbsp;解析器新增对&nbsp;<strong>Zstandard 压缩编码</strong>&nbsp;的支持。</p> </li>
<li> <p><strong>跟踪流（Follow Stream）</strong>&nbsp;现支持&nbsp;<strong>MPEG-2 TS PID</strong>&nbsp;与内部&nbsp;<strong>Packetized Elementary Streams</strong>。</p> </li>
<li> <p><strong>DNP3（分布式网络协议 3）</strong>&nbsp;现支持在会话与端点表中显示。</p> </li>
<li> <p><strong>Lua 改进</strong>：内置的&nbsp;<code>bit</code>&nbsp;与&nbsp;<code>rex_pcre2</code>&nbsp;库现会自动加入&nbsp;<code>package.loaded</code>，可通过&nbsp;<code>require</code>&nbsp;调用。Issue 20213</p> </li>
<li> <p><strong>数据包列表</strong>&nbsp;与&nbsp;<strong>事件列表</strong>&nbsp;不再支持多行显示。Issue 14424</p> </li>
<li> <p><strong>ethers 文件</strong>&nbsp;现支持&nbsp;<strong>EUI-64 映射</strong>。Issue 15487</p> </li>
<li> <p><strong>Hex Dump 导入功能</strong>&nbsp;支持 2–4 字节分组（含小端模式），并识别带&nbsp;<code>0x</code>&nbsp;前缀的偏移。Issue 16193</p> </li>
<li> <p><strong>Hex Dump 导出</strong>&nbsp;现可包含时间戳前缀（Wireshark 的“打印”与“导出”对话框，以及 TShark 的&nbsp;<code>--hexdump time</code>&nbsp;选项）。Issue 17132</p> </li>
<li> <p><strong>Lua 新增&nbsp;<code>Conversation</code>&nbsp;对象</strong>，可在脚本中访问会话数据 (sysin)。Issue 15396</p> </li>
<li> <p>新增&nbsp;<strong>“编辑 › 复制 › 作为 HTML”</strong>&nbsp;菜单项，可自定义复制格式与键盘快捷键。</p> </li>
<li> <p>GUI 导出对话框新增 “无重复键 JSON 输出” 选项。</p> </li>
<li> <p>导出对话框可输出&nbsp;<strong>原始十六进制字节数据</strong>。</p> </li>
<li> <p>会话与端点表新增显示&nbsp;<strong>精确字节与比特率</strong>&nbsp;的选项，由偏好项&nbsp;<code>conv.machine_readable</code>&nbsp;控制。</p> </li>
<li> <p><code>-z &lt;tap&gt;,tree</code>&nbsp;的 TShark 统计输出格式可通过&nbsp;<code>-o statistics.output_format</code>&nbsp;控制。</p> </li>
<li> <p><strong>配色方案可独立设置浅色/深色模式</strong>，与系统设置无关（需 Qt 6.8+）。Issue 19328</p> </li>
<li> <p><strong>libxml2</strong>&nbsp;现为构建必需依赖（不支持 2.15.0 版本）。</p> </li>
<li> <p><strong>View 菜单新增“重新解析数据包（Redissect Packets）”选项</strong>，适用于地址解析或解密密钥更新后。</p> </li>
<li> <p><strong>HTTP2 可选跟踪 3GPP 会话（5G SBI）</strong>。</p> </li>
<li> <p><strong>Windows 构建文档不再需要 Java。</strong></p> </li>
<li> <p><strong>Linux</strong>上捕获过滤器支持 BPF 扩展（如<code>inbound</code>、<code>outbound</code>、<code>ifindex</code>）。</p> </li>
</ul>
<p>&nbsp;<strong>移除的功能与支持</strong></p>
<ul>
<li> <p>不再支持&nbsp;<strong>AirPcap</strong>&nbsp;与&nbsp;<strong>WinPcap</strong>。</p> </li>
<li> <p>不再支持&nbsp;<strong>libnl v1 / v2</strong>。</p> </li>
<li> <p><strong>CMake 选项&nbsp;<code>ENABLE_STATIC</code>&nbsp;已弃用</strong>，请改用&nbsp;<code>BUILD_SHARED_LIBS</code>。</p> </li>
</ul>
<p><strong>新增文件格式解析支持</strong></p>
<ul>
<li> <p>RIFF（资源交换文件格式）</p> </li>
<li> <p>TTL 文件格式</p> </li>
</ul>
<p><strong>新增协议支持</strong></p>
<p>支持以下协议：</p>
<p>Asymmetric &nbsp;Key Packages (AKP)、Binary HTTP、BIST TotalView-ITCH、BIST &nbsp;TotalView-OUCH、Bluetooth Android HCI、Bluetooth Intel HCI、BPSec COSE &nbsp;Context、BPSec Default SC、Commsignia Capture Protocol (C2P)、DECT &nbsp;NR+、DLMS/COSEM、Ephemeral Diffie-Hellman Over COSE、ILNP、LDA Neo Device &nbsp;Trailer、Lenbrook Service Discovery Protocol、LLC &nbsp;V1、Navitrol、NTS-KE、Ouster VLP-16、Private Line Emulation、RC &nbsp;V3、RCG、Roughtime、SBAS L5、SGP.22、SGP.32、SICK CoLA Ascii/Binary、Silabs &nbsp;Debug Channel、XCP、USB-PTP、VLP-16、vSomeIP Internal Protocol 等。</p>
<p><strong>更新的协议支持</strong></p>
<p>协议更新数量众多，此处不一一列出。</p>
<p><strong>新增与更新的捕获文件支持</strong></p>
<ul>
<li>改进了&nbsp;<strong>BLF 文件格式</strong>（包括写入功能）。</li>
</ul>
<p><strong>新增与更新的捕获接口支持</strong></p>
<ul>
<li><strong>Windows：</strong>&nbsp;改进了&nbsp;<code>etwdump</code>&nbsp;的用户体验，显示未知事件的原始字节内容。</li>
</ul>
<p><strong>主要 API 变更</strong></p>
<ul>
<li><strong>Lua API</strong>&nbsp;现支持&nbsp;<strong>Libgcrypt 对称加密函数</strong>。</li>
</ul>
<p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.wireshark.org%2Farchives%2Fwireshark-announce%2F202510%2Fmsg00000.html" target="_blank">查看官方公告</a>。</p>]]>
    </description>
    <content:encoded><![CDATA[<p>Wireshark 4.6.0 现已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.wireshark.org%2Farchives%2Fwireshark-announce%2F202510%2Fmsg00000.html" target="_blank">发布</a>，一些具体更新内容如下：</p>
<p>以下功能为自&nbsp;<strong>4.6.0rc1</strong>&nbsp;以来的新功能或重大更新：</p>
<ul>
<li>Wireshark 现在可以解析 macOS 上&nbsp;<code>tcpdump</code>&nbsp;提供的进程信息、数据包元数据、流 ID、丢包信息等内容。</li>
</ul>
<p>以下功能为自&nbsp;<strong>4.4.0</strong>&nbsp;以来的新功能或重大更新：</p>
<ul>
<li>Windows 安装包现附带&nbsp;<strong>Npcap 1.83</strong>（之前为 1.79）。</li>
<li>Windows 和 macOS 安装包现附带&nbsp;<strong>Qt 6.9.3</strong>（之前为 6.5.3）。</li>
<li>macOS 版本现提供&nbsp;<strong>通用安装包（Universal Installer）</strong>，不再区分 Arm64 与 Intel 架构。Issue 17294</li>
<li> <p><strong>WinPcap 不再受支持</strong>。在 Windows 上请改用 Npcap，并卸载 WinPcap。WinPcap 最后一个版本为 4.1.3（2013 年发布），仅支持至 Windows 8，该系统现已被 Microsoft 与 Wireshark 停止支持。</p> </li>
<li> <p>新增&nbsp;<strong>“Plots（散点图）”对话框</strong>，与 “I/O Graphs（直方图）” 相比，可展示多种散点数据，支持多图、标记及自动滚动。</p> </li>
<li> <p><strong>实时捕获可压缩写入</strong>。此前仅在多文件捕获轮换时可压缩。现在&nbsp;<code>TShark</code>&nbsp;的&nbsp;<code>--compress</code>&nbsp;参数可在实时捕获中使用。Issue 9311</p> </li>
<li> <p><strong>绝对时间字段</strong>（Absolute time fields）在使用&nbsp;<code>-T json</code>&nbsp;输出时，无论在“数据包详情”中如何显示，均使用 UTC 的 ISO 8601 格式。这在&nbsp;<code>-T ek</code>&nbsp;中自 4.2.0 版起已启用。</p> </li>
<li> <p><strong>时间显示格式更新</strong>：<code>-T fields</code>、<code>-T pdml</code>、自定义列或 CSV 输出中，不再使用&nbsp;<code>asctime</code>&nbsp;样式（如&nbsp;<code>Dec 18, 2017 05:28:39 EST</code>），改用 ISO 8601。为向后兼容，可通过&nbsp;<code>protocols.display_abs_time_ascii</code>&nbsp;选项切换显示方式。未来版本可能彻底移除&nbsp;<code>asctime</code>&nbsp;格式。</p> </li>
<li> <p><strong>UTC 时间列</strong>&nbsp;现在按 ISO 8601 标准添加 “Z” 后缀。</p> </li>
<li> <p><strong>TShark&nbsp;<code>-G</code>&nbsp;参数增强</strong>：不再要求必须为第一个参数，并支持与&nbsp;<code>-o</code>、<code>-d</code>、<code>--disable-protocol</code>&nbsp;等其他选项配合使用。</p> </li>
<li> <p><strong>EUI-64 字段类型</strong>&nbsp;改为字节（bytes）类型，以便进行切片与匹配（如&nbsp;<code>wpan.src64[:3] == eth.src[:3]</code>）。</p> </li>
<li> <p><strong>NTP 解密支持 NTS（Network Time Security）</strong>&nbsp;协议 (sysin)，前提是捕获中包含 NTS-KE 包和 TLS 密钥。</p> </li>
<li> <p><strong>MACsec 解密增强</strong>：支持使用 MKA 解封的 SAK 或在 MACsec 解析器中配置的 PSK。</p> </li>
<li> <p><strong>TCP 流图（Stream Graph）坐标轴</strong>&nbsp;现采用 SI 单位。Issue 20197</p> </li>
<li> <p><strong>自定义列增强</strong>：可选择使用与“数据包详情”相同的格式显示值，并支持复杂表达式按数值排序。</p> </li>
<li> <p>新增显示过滤函数&nbsp;<strong><code>float</code>&nbsp;与&nbsp;<code>double</code></strong>，支持类型显式转换与算术操作。</p> </li>
<li> <p><strong>I/O 图窗口</strong>&nbsp;最小宽度减小，更适合低分辨率桌面。Issue 20147</p> </li>
<li> <p><strong>X.509 证书导出</strong>：可通过菜单&nbsp;<strong>File › Export Objects › X509AF</strong>&nbsp;导出证书，也可在 TShark 中使用&nbsp;<code>--export-objects x509af</code>。</p> </li>
<li> <p><strong>HTTP / HTTP2</strong>&nbsp;解析器新增对&nbsp;<strong>Zstandard 压缩编码</strong>&nbsp;的支持。</p> </li>
<li> <p><strong>跟踪流（Follow Stream）</strong>&nbsp;现支持&nbsp;<strong>MPEG-2 TS PID</strong>&nbsp;与内部&nbsp;<strong>Packetized Elementary Streams</strong>。</p> </li>
<li> <p><strong>DNP3（分布式网络协议 3）</strong>&nbsp;现支持在会话与端点表中显示。</p> </li>
<li> <p><strong>Lua 改进</strong>：内置的&nbsp;<code>bit</code>&nbsp;与&nbsp;<code>rex_pcre2</code>&nbsp;库现会自动加入&nbsp;<code>package.loaded</code>，可通过&nbsp;<code>require</code>&nbsp;调用。Issue 20213</p> </li>
<li> <p><strong>数据包列表</strong>&nbsp;与&nbsp;<strong>事件列表</strong>&nbsp;不再支持多行显示。Issue 14424</p> </li>
<li> <p><strong>ethers 文件</strong>&nbsp;现支持&nbsp;<strong>EUI-64 映射</strong>。Issue 15487</p> </li>
<li> <p><strong>Hex Dump 导入功能</strong>&nbsp;支持 2–4 字节分组（含小端模式），并识别带&nbsp;<code>0x</code>&nbsp;前缀的偏移。Issue 16193</p> </li>
<li> <p><strong>Hex Dump 导出</strong>&nbsp;现可包含时间戳前缀（Wireshark 的“打印”与“导出”对话框，以及 TShark 的&nbsp;<code>--hexdump time</code>&nbsp;选项）。Issue 17132</p> </li>
<li> <p><strong>Lua 新增&nbsp;<code>Conversation</code>&nbsp;对象</strong>，可在脚本中访问会话数据 (sysin)。Issue 15396</p> </li>
<li> <p>新增&nbsp;<strong>“编辑 › 复制 › 作为 HTML”</strong>&nbsp;菜单项，可自定义复制格式与键盘快捷键。</p> </li>
<li> <p>GUI 导出对话框新增 “无重复键 JSON 输出” 选项。</p> </li>
<li> <p>导出对话框可输出&nbsp;<strong>原始十六进制字节数据</strong>。</p> </li>
<li> <p>会话与端点表新增显示&nbsp;<strong>精确字节与比特率</strong>&nbsp;的选项，由偏好项&nbsp;<code>conv.machine_readable</code>&nbsp;控制。</p> </li>
<li> <p><code>-z &lt;tap&gt;,tree</code>&nbsp;的 TShark 统计输出格式可通过&nbsp;<code>-o statistics.output_format</code>&nbsp;控制。</p> </li>
<li> <p><strong>配色方案可独立设置浅色/深色模式</strong>，与系统设置无关（需 Qt 6.8+）。Issue 19328</p> </li>
<li> <p><strong>libxml2</strong>&nbsp;现为构建必需依赖（不支持 2.15.0 版本）。</p> </li>
<li> <p><strong>View 菜单新增“重新解析数据包（Redissect Packets）”选项</strong>，适用于地址解析或解密密钥更新后。</p> </li>
<li> <p><strong>HTTP2 可选跟踪 3GPP 会话（5G SBI）</strong>。</p> </li>
<li> <p><strong>Windows 构建文档不再需要 Java。</strong></p> </li>
<li> <p><strong>Linux</strong>上捕获过滤器支持 BPF 扩展（如<code>inbound</code>、<code>outbound</code>、<code>ifindex</code>）。</p> </li>
</ul>
<p>&nbsp;<strong>移除的功能与支持</strong></p>
<ul>
<li> <p>不再支持&nbsp;<strong>AirPcap</strong>&nbsp;与&nbsp;<strong>WinPcap</strong>。</p> </li>
<li> <p>不再支持&nbsp;<strong>libnl v1 / v2</strong>。</p> </li>
<li> <p><strong>CMake 选项&nbsp;<code>ENABLE_STATIC</code>&nbsp;已弃用</strong>，请改用&nbsp;<code>BUILD_SHARED_LIBS</code>。</p> </li>
</ul>
<p><strong>新增文件格式解析支持</strong></p>
<ul>
<li> <p>RIFF（资源交换文件格式）</p> </li>
<li> <p>TTL 文件格式</p> </li>
</ul>
<p><strong>新增协议支持</strong></p>
<p>支持以下协议：</p>
<p>Asymmetric &nbsp;Key Packages (AKP)、Binary HTTP、BIST TotalView-ITCH、BIST &nbsp;TotalView-OUCH、Bluetooth Android HCI、Bluetooth Intel HCI、BPSec COSE &nbsp;Context、BPSec Default SC、Commsignia Capture Protocol (C2P)、DECT &nbsp;NR+、DLMS/COSEM、Ephemeral Diffie-Hellman Over COSE、ILNP、LDA Neo Device &nbsp;Trailer、Lenbrook Service Discovery Protocol、LLC &nbsp;V1、Navitrol、NTS-KE、Ouster VLP-16、Private Line Emulation、RC &nbsp;V3、RCG、Roughtime、SBAS L5、SGP.22、SGP.32、SICK CoLA Ascii/Binary、Silabs &nbsp;Debug Channel、XCP、USB-PTP、VLP-16、vSomeIP Internal Protocol 等。</p>
<p><strong>更新的协议支持</strong></p>
<p>协议更新数量众多，此处不一一列出。</p>
<p><strong>新增与更新的捕获文件支持</strong></p>
<ul>
<li>改进了&nbsp;<strong>BLF 文件格式</strong>（包括写入功能）。</li>
</ul>
<p><strong>新增与更新的捕获接口支持</strong></p>
<ul>
<li><strong>Windows：</strong>&nbsp;改进了&nbsp;<code>etwdump</code>&nbsp;的用户体验，显示未知事件的原始字节内容。</li>
</ul>
<p><strong>主要 API 变更</strong></p>
<ul>
<li><strong>Lua API</strong>&nbsp;现支持&nbsp;<strong>Libgcrypt 对称加密函数</strong>。</li>
</ul>
<p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.wireshark.org%2Farchives%2Fwireshark-announce%2F202510%2Fmsg00000.html" target="_blank">查看官方公告</a>。</p>]]></content:encoded>
    
    <pubDate>Mon, 13 Oct 2025 15:44:00 +0800</pubDate>
  </item><item>
    <title><![CDATA[Flink 基于 Paimon 的实时湖仓解决方案的演进]]></title>
    <link>https://my.oschina.net/u/2828172/blog/18692226</link>
    <itunes:title><![CDATA[Flink 基于 Paimon 的实时湖仓解决方案的演进]]></itunes:title>
    <itunes:author><![CDATA[OSCHINA 社区最新新闻[RSS+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-897a40de65.png"></p>
<h2>引言</h2>
<p>现代数据环境要求架构能够无缝融合数据湖的灵活性与传统数据仓库的性能特征。随着企业越来越多地采用实时分析来驱动业务决策，Apache Flink作为流处理引擎与Apache Paimon作为湖存储格式的结合，已成为构建强大实时湖仓平台的引人注目的解决方案。</p>
<p><strong>本文</strong>整理自 Apache CommunityOverCode Asia 2025 大会上，阿里云技术专家，Apache Flink Committer 苏轩楠分享了基于 Paimon构建的 Flink 实时湖仓解决方案持续演进的深刻见解。这一技术深度解析探讨了为解决大规模流式分析平台实施过程中面临的现实挑战而开发的关键优化和架构改进。</p>
<p>随着需要处理日益增长的结构化和半结构化数据量，传统的数据处理方法在性能、成本效益和运营复杂性方面往往力不从心。所讨论的增强功能代表了在生产环境中经过测试和完善的实用解决方案，为寻求现代化数据基础设施的组织提供了具体的实施路径。</p>
<h2>实时湖仓架构格局</h2>
<p>在深入技术优化之前，有必要了解围绕Flink和Paimon集成而形成的典型架构模式。实时湖仓方法代表了从传统批处理导向的数据仓库模式的根本转变，拥抱了数据到达时连续处理的模型。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-bef478601e.png"></p>
<h3>基础架构组件</h3>
<p>基于Flink和Paimon构建的现代实时湖仓通常由几个相互连接的处理层组成，每个层都服务于不同的目的，同时在整个系统中保持无缝的数据流。在基础层，Flink CDC（变更数据捕获）在建立统一的全量和增量数据同步能力方面发挥着关键作用。</p>
<p>Flink CDC在弥合操作数据库和分析系统之间的差距方面已被证明特别有价值。与需要在固定时间表上运行的复杂ETL管道不同，Flink CDC使组织能够实时捕获来自MySQL等源系统的变更，并直接将其流式传输到Paimon的ODS（操作数据存储）层。这种方法不仅减少了延迟，还通过消除对中间暂存区和复杂协调机制的需要来简化整体架构。</p>
<p>这些能力超越了简单的数据复制。现代实现支持完整的数据库同步场景，其中整个数据库模式可以迁移到湖仓格式，并完整支持自动模式演化处理。这意味着当源系统模式发生变化时------无论是通过添加新列、修改数据类型还是重构关系------下游Paimon表都可以自动适应，无需手动干预或管道重建。</p>
<h3>数据处理与转换层</h3>
<p>一旦数据通过摄取层进入湖仓，它就会经历一系列处理阶段，逐步完善和丰富信息。数据仓库明细（DWD）层代表第一个主要转换阶段，原始操作数据在此经历清洗、标准化和丰富操作。</p>
<p>这些转换通常涉及复杂的数据连接操作，通过组合来自多个源系统的信息来创建"宽表"。例如，电子商务组织可能会将客户档案数据与交易历史、产品目录和营销活动信息连接起来，以创建全面的客户行为数据集。这种处理的实时特性意味着随着源数据的变化，这些丰富的视图保持最新，为分析师和应用程序提供新鲜的见解，而没有传统批处理方法固有的延迟。</p>
<p>处理继续进展到数据仓库汇总（DWS）层，聚合计算产生业务指标和关键绩效指标。与传统数据仓库中这些聚合可能每天或每小时计算一次不同，实时湖仓方法能够在事件发生时连续计算业务指标。这种能力对于需要实时监控业务绩效、快速响应运营问题或基于分析见解触发自动化操作的组织来说是变革性的。</p>
<h3>湖仓管理与优化</h3>
<p>湖仓内的数据管理带来了与传统数据仓库管理显著不同的独特挑战。Paimon通过一套全面的湖仓管理工具和优化技术来解决这些挑战，这些工具和技术透明地运行以维持系统性能和效率。</p>
<p>小文件管理代表了任何基于湖的存储系统中最关键的运营挑战之一。随着流数据的连续到达，自然倾向于创建大量小文件，这会降低读取性能并增加元数据开销。Paimon的自动文件合并功能通过基于可配置策略智能合并小文件来解决这一挑战，确保存储保持优化而无需手动干预。</p>
<p>这些功能协同工作，在保持查询性能的同时最大限度地降低存储成本，这对于处理大量历史数据和实时流的组织来说是特别重要的考虑因素。</p>
<h2>技术演进：应对现实世界的挑战</h2>
<p>Flink和Paimon生态系统的成熟导致了越来越复杂的优化，这些优化解决了生产部署中遇到的特定性能瓶颈和运营挑战。从这一演进中出现了两个特别重要的改进：半结构化数据的增强处理和优化的Lookup Join操作。</p>
<h3>半结构化数据挑战</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-4d8c96c1ac.png"></p>
<p>现代数据环境的特点是半结构化数据格式的激增，其中JSON是最普遍的。Web应用程序、移动设备、物联网传感器和API驱动集成的兴起使JSON在企业数据管道中无处不在。然而，在使用传统流处理方法时，这种普遍性带来了显著的性能影响。</p>
<p>根本挑战在于JSON的自描述特性。与结构化数据中模式信息与数据本身分离不同，JSON直接在数据有效负载中嵌入类型和结构信息。虽然这提供了巨大的灵活性并启用了动态模式演化，但在流环境中处理大量JSON数据时会创建大量的计算开销。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d5d55e4776.png"></p>
<p>Flink中JSON处理的传统方法将半结构化数据视为简单的字符串值，每次需要访问任何字段时都需要完整的解析操作。这种架构决策虽然实现简单，但性能不太理想。即使访问JSON对象中的第一个字段也需要解析整个文档，由于数据作为字符串值在Flink的作业中分发，每个需要处理JSON的下游算子都必须重复完整的解析操作。</p>
<p>存储也有同样的问题。JSON的基于文本的格式虽然人类可读且广泛支持，但消耗的存储空间比等效的二进制表示形式多得多。这种增加的存储占用直接转化为更高的成本和在随机操作期间增加的网络带宽消耗，其中数据在流处理管道中的算子之间移动。</p>
<h3>Variant数据类型解决方案</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a87ec58242.png"></p>
<p>Variant数据类型的引入代表了Flink处理半结构化数据处理方法的根本转变。Variant不是将JSON视为不透明文本，而是提供了一种原生二进制表示，保持了半结构化数据的灵活性，同时提供了更接近结构化数据处理的性能特征。</p>
<p>Variant格式从更广泛的数据处理生态系统中的类似努力中汲取灵感，特别是Parquet提议的半结构化数据格式。通过采用开放标准方法，实现确保了与其他处理引擎的兼容性。</p>
<p>Variant采用的二进制编码策略通过几种机制实现性能改进。模式信息不是在整个数据中重复嵌入，而是在元数据部分中编码一次，显著减少存储开销。字段访问操作可以利用这些元数据直接导航到特定字段，而不解析数据结构的无关部分，大大提高了选择性查询的访问性能。</p>
<h3>增强的开发者体验</h3>
<p>除了性能改进之外，Variant在处理半结构化数据时为开发者体验引入了显著的增强。传统方法要求开发者使用复杂的SQL函数进行字段访问，创建冗长且容易出错的查询。Variant启用了更直观的语法模式，与开发者从其他编程环境的期望一致。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-82446879b2.png"></p>
<p>使用熟悉的括号表示法和点语法的直接字段访问简化了查询开发并使代码更易维护。数组元素访问遵循类似的模式，使开发者能够使用自然语法处理嵌套结构。类型转换功能允许与强类型下游处理的无缝集成，其中Variant字段可以根据需要转换为特定的数据类型。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-1b90993b38.png"></p>
<p>JSON字符串和Variant类型之间的转换函数为现有系统提供了迁移路径，同时启用了新格式的渐进采用。<code>PARSE_JSON</code>和<code>TRY_PARSE_JSON</code>函数处理从基于文本的JSON到二进制Variant格式的转换，后者为格式错误的输入数据提供错误处理功能。<code>JSON_STRING</code>函数在与尚未采用Variant支持的系统接口时启用转换回文本格式。</p>
<h3>Variant Shredding：针对现实世界模式的优化</h3>
<p>Variant实现中最复杂的优化解决了半结构化数据中的一个常见模式：经常访问的公共字段与真正动态部分的存在。虽然JSON的灵活性允许完全任意的结构，但生产系统经常表现出某些字段在记录中一致出现的模式，即使数据结构的其他部分变化显著。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b224333e5.png"></p>
<p>Variant Shredding通过将经常访问的字段作为单独的物理列存储在主Variant二进制结构之外来利用这一观察。这种混合方法结合了半结构化数据的灵活性与经常访问字段的列式存储性能特征。以这种方式 shredding 的字段可以以几乎与常规结构化列相同的性能进行访问。</p>
<p>这种优化的影响超越了简单的字段访问性能。shredding 字段可以完全参与Flink的查询优化，包括 projection 下推（其中仅从存储中读取所需列）和 filter 下推（其中谓词在尽可能接近数据源的地方进行评估）。这些优化可以大大减少I/O需求，在处理大型历史数据集和实时流时特别重要。</p>
<p>适合 shredding 的字段识别可以通过两种方法发生。手动配置允许开发者和数据工程师基于他们对数据访问模式和业务需求的理解明确指定 shredding 字段。对于处理多样化或不断发展的半结构化数据的组织，自动化发现机制可以分析传入的数据样本以识别出现频率足以从 shredding 优化中受益的字段。</p>
<h2>Lookup Join优化：解决可扩展性瓶颈</h2>
<p>第二个主要优化领域解决了实时分析中的一个常见架构模式：使用Lookup Join用存储在Paimon表中的维度信息来丰富流数据。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2cb09d4137.png"></p>
<h3>理解Lookup Join挑战</h3>
<p>Lookup Join代表了流分析中的关键操作，其中实时事件数据需要用相对静态的维度信息进行丰富。常见示例包括用客户档案信息丰富交易事件、向购买事件添加产品详细信息或用配置数据增强日志条目。挑战在于高效访问可能分布在多个存储分区中的维度数据，同时保持实时处理所需的低延迟特征。</p>
<p>Flink中的Lookup Join通常涉及三个阶段：基于连接键分发事实表数据的随机操作、从远程存储检索维度数据的获取操作，以及维护维度数据本地副本以供快速访问的缓存操作。</p>
<p>这种方法在传统的维度数据存储如Redis中运行得相当不错，因为其中数据本质上不分区，所有的算子都需要读取完整的数据。然而，Paimon的数据存储分桶策略与这种方法产生了根本性的不匹配，导致性能的浪费。</p>
<h3>分桶不匹配问题</h3>
<p>Paimon使用分桶策略组织数据，其中记录基于基于键的哈希函数分布在多个分桶中。这种方法提供了出色的可扩展性并启用了高效的数据组织，但它在传统Lookup Join实现中创造了显著的低效率。</p>
<p>核心问题是Flink的 Lookup 算子不知道Paimon的分桶策略。每个并发 Lookup 算子假设它可能需要与维度表中的任何记录连接，导致每个算子需要维护所有维度数据的完整副本。这意味着无论有多少并行算子处理Lookup Join，每一个都需要读取整个Paimon表并在本地缓存所有维度数据。</p>
<p>这种方法的影响在大规模部署中变得严重。作业启动时间可能延长到数十分钟，因为每个算子拉取完整的维度数据集。内存消耗随算子并行度提高，因为每个算子维护所有维度数据的重复副本。由于管理这些大型本地缓存的开销和为每个查找操作搜索完整数据集的计算成本，整体Lookup Join性能受到影响。</p>
<h3>自定义Shuffle策略解决方案</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-05c0be05e2.png"></p>
<p>该解决方案涉及扩展Flink的Lookup Join架构以支持自定义 shuffle 策略。这确保了预期到特定Paimon分桶的记录由负责该分桶维度数据的相同 Lookup 算子处理。</p>
<p>有了这种对齐，每个 Lookup 算子可以专门关注其分配的分桶数据。算子只需要维护其分配分桶数据的本地副本，而不是读取和缓存整个维度表。这大大减少了每个算子需要管理的数据量，并消除了算子之间的冗余存储。</p>
<p>性能改进效果非常明显，在高并行性场景中，作业启动时间也可以从数十分钟减少到几秒钟。每个算子的内存消耗显著下降，允许更高效的资源利用。由于较小的本地缓存和更集中的数据访问模式，整体Lookup Join性能提高。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-aea113f5c3.png"></p>
<h2>其他关键优化特性</h2>
<p>除了Variant数据类型和Lookup Join优化这两个核心改进之外，Flink和Paimon的集成还包含了一系列其他重要的优化特性，这些功能共同构成了完整的实时湖仓解决方案优化体系。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8b560c1552.png"></p>
<h3>Paimon Action/Procedure 易用性优化</h3>
<p>Paimon Action和Procedure功能的易用性优化代表了用户体验的重大改进。传统的湖仓管理操作往往需要复杂的配置和深度的技术专业知识，这对于开发者和运维人员来说构成了显著的使用门槛。</p>
<p>新的易用性优化简化了常见的湖仓管理任务，包括表的创建、数据的压缩、快照的管理和元数据的维护等操作。用户可以通过简单的SQL语句完成复杂的湖仓管理任务。</p>
<h3>Materialized Table 物化表支持</h3>
<p>物化表（Materialized Table）是 Flink 新引入的功能，主要是为了让用户能够通过 Flink SQL 写业务的代码，Flink 会自动根据用户指定的数据新鲜度需求来决定启动流作业或者批作业来生成 materialized table，保证表中的数据符合新鲜度的需求。用户免去了配置作业，以及维护作业的工作。</p>
<h3>Nested Projection Pushdown</h3>
<p>Nested Projection Pushdown优化专门针对复杂嵌套数据结构的查询性能问题。在现代数据环境中，JSON、Avro、Parquet等格式经常包含深层嵌套的数据结构，传统的查询处理往往需要读取整个嵌套对象，即使查询只需要其中的少数几个字段。</p>
<p>Nested Projection Pushdown技术能够分析查询中对嵌套字段的访问模式，并将字段选择操作推送到数据读取的最早阶段。这意味着在从存储系统读取数据时，就可以只提取查询实际需要的嵌套字段，而不是读取完整的嵌套结构。</p>
<p>这种优化对于包含大量嵌套字段的数据特别有效。例如，对于包含数百个字段的用户行为事件数据，如果查询只需要其中的几个关键字段，Nested Projection Pushdown能够将I/O开销降低一个数量级。同时，这种优化还能减少网络传输的数据量和内存使用量，从而提升整个查询处理管道的效率。</p>
<h3>Partial Update Sink Reuse 和性能优化</h3>
<p>Paimon 的 partial update 也是非常常用的功能。但是在做 partial update 的时候，通常需要在一个作业中有多个数据源写入同一张 Paimon 表，当 Flink 作业做 checkpoint 的时候，会有 sink 同时做 compaction，这会导致作业一直 Failover。为此我们对 Flink 的 sql planner 做了改动，让它能够识别出相同的 sink 进行复用，从而避免这种问题</p>
<h2>技术发展路线图与版本规划</h2>
<p>Flink和Paimon的集成发展遵循着清晰的技术路线图，不同的优化特性按照成熟度和优先级被分配到不同的版本发布周期中。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-eb423f1e6e.png"></p>
<h3>已发布功能特性</h3>
<p>在当前已发布的版本中，核心的优化功能已经投入生产使用，包括前面详细讨论的Lookup Join优化、Paimon Action/Procedure的易用性改进、物化表支持、Nested Projection Pushdown，以及Partial Update Sink Reuse等关键特性。这些功能已经通过了大规模生产环境的验证，能够为企业级实时湖仓部署提供稳定可靠的性能保障。</p>
<h3>Flink 2.1 与 Paimon 1.3 版本特性</h3>
<p>在Flink 2.1和Paimon 1.3的版本发布中，重点关注Variant数据类型的基础支持能力。这个版本将提供完整的Variant类型读写功能，使得用户能够在Flink和Paimon中原生处理半结构化数据，而不需要依赖复杂的字符串解析操作。</p>
<p>同时，这个版本还将支持Variant配置的shredding字段功能，允许用户根据数据访问模式手动配置哪些字段需要进行shredding优化。这为有明确数据访问模式的企业用户提供了精细控制的能力，能够针对具体的业务场景进行深度的性能调优。</p>
<h3>Flink 2.2 版本增强功能</h3>
<p>Flink 2.2版本将进一步完善Variant数据类型的功能，重点提供更加灵活和强大的字段访问能力。用户将能够使用直观的语法访问Variant类型中的嵌套字段，同时支持灵活的类型转换操作，使得Variant类型能够与现有的强类型处理流程无缝集成。</p>
<p>这个版本的Variant类型支持将使得半结构化数据的处理体验接近传统结构化数据，同时保持了半结构化数据的灵活性优势。</p>
<h2>展望未来：技术创新的新方向</h2>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ed9f8dca57.png"></p>
<h3>半结构化数据处理的智能化发展</h3>
<p>在半结构化数据处理领域，未来的发展将更加注重自动化和智能化。Variant shredding功能将支持自动设置shredding字段，系统能够通过分析历史查询模式和数据访问频率，自动识别哪些字段适合进行shredding优化，无需用户手动配置。</p>
<p>更进一步，Flink将支持Variant字段访问的下推优化到数据源层面，结合读裁剪优化技术，能够在数据读取阶段就完成字段的选择和过滤操作。这种源端优化将大大减少数据传输和处理的开销，特别是在处理大规模半结构化数据时能够带来显著的性能提升。</p>
<h3>非结构化数据处理能力扩展</h3>
<p>Flink的发展规划还包括对非结构化数据处理能力的重要扩展。未来版本将能够处理文本、图像、音频等非结构化数据类型，这为构建更加全面的数据处理平台奠定了基础。</p>
<p>非结构化数据处理能力的引入将使得Flink不仅能够处理传统的业务数据，还能够支持内容分析、多媒体处理、文档解析等更广泛的应用场景。这种扩展将进一步巩固Flink作为统一数据处理平台的地位。</p>
<hr>
<h3>更多内容</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d80a524eac.jpg"></p>
<hr>
<h3>活动推荐</h3>
<p>复制下方链接或者扫描二维码 即可快速体验 "一体化的实时数仓联合解决方案" 了解活动详情：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.aliyun.com%2Fsolution%2Ftech-solution%2Fflink-hologres" target="_blank">https://www.aliyun.com/solution/tech-solution/flink-hologres</a></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ae89a88bf2.png"></p>]]>
    </itunes:summary>
    <description>
      <![CDATA[<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-897a40de65.png"></p>
<h2>引言</h2>
<p>现代数据环境要求架构能够无缝融合数据湖的灵活性与传统数据仓库的性能特征。随着企业越来越多地采用实时分析来驱动业务决策，Apache Flink作为流处理引擎与Apache Paimon作为湖存储格式的结合，已成为构建强大实时湖仓平台的引人注目的解决方案。</p>
<p><strong>本文</strong>整理自 Apache CommunityOverCode Asia 2025 大会上，阿里云技术专家，Apache Flink Committer 苏轩楠分享了基于 Paimon构建的 Flink 实时湖仓解决方案持续演进的深刻见解。这一技术深度解析探讨了为解决大规模流式分析平台实施过程中面临的现实挑战而开发的关键优化和架构改进。</p>
<p>随着需要处理日益增长的结构化和半结构化数据量，传统的数据处理方法在性能、成本效益和运营复杂性方面往往力不从心。所讨论的增强功能代表了在生产环境中经过测试和完善的实用解决方案，为寻求现代化数据基础设施的组织提供了具体的实施路径。</p>
<h2>实时湖仓架构格局</h2>
<p>在深入技术优化之前，有必要了解围绕Flink和Paimon集成而形成的典型架构模式。实时湖仓方法代表了从传统批处理导向的数据仓库模式的根本转变，拥抱了数据到达时连续处理的模型。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-bef478601e.png"></p>
<h3>基础架构组件</h3>
<p>基于Flink和Paimon构建的现代实时湖仓通常由几个相互连接的处理层组成，每个层都服务于不同的目的，同时在整个系统中保持无缝的数据流。在基础层，Flink CDC（变更数据捕获）在建立统一的全量和增量数据同步能力方面发挥着关键作用。</p>
<p>Flink CDC在弥合操作数据库和分析系统之间的差距方面已被证明特别有价值。与需要在固定时间表上运行的复杂ETL管道不同，Flink CDC使组织能够实时捕获来自MySQL等源系统的变更，并直接将其流式传输到Paimon的ODS（操作数据存储）层。这种方法不仅减少了延迟，还通过消除对中间暂存区和复杂协调机制的需要来简化整体架构。</p>
<p>这些能力超越了简单的数据复制。现代实现支持完整的数据库同步场景，其中整个数据库模式可以迁移到湖仓格式，并完整支持自动模式演化处理。这意味着当源系统模式发生变化时------无论是通过添加新列、修改数据类型还是重构关系------下游Paimon表都可以自动适应，无需手动干预或管道重建。</p>
<h3>数据处理与转换层</h3>
<p>一旦数据通过摄取层进入湖仓，它就会经历一系列处理阶段，逐步完善和丰富信息。数据仓库明细（DWD）层代表第一个主要转换阶段，原始操作数据在此经历清洗、标准化和丰富操作。</p>
<p>这些转换通常涉及复杂的数据连接操作，通过组合来自多个源系统的信息来创建"宽表"。例如，电子商务组织可能会将客户档案数据与交易历史、产品目录和营销活动信息连接起来，以创建全面的客户行为数据集。这种处理的实时特性意味着随着源数据的变化，这些丰富的视图保持最新，为分析师和应用程序提供新鲜的见解，而没有传统批处理方法固有的延迟。</p>
<p>处理继续进展到数据仓库汇总（DWS）层，聚合计算产生业务指标和关键绩效指标。与传统数据仓库中这些聚合可能每天或每小时计算一次不同，实时湖仓方法能够在事件发生时连续计算业务指标。这种能力对于需要实时监控业务绩效、快速响应运营问题或基于分析见解触发自动化操作的组织来说是变革性的。</p>
<h3>湖仓管理与优化</h3>
<p>湖仓内的数据管理带来了与传统数据仓库管理显著不同的独特挑战。Paimon通过一套全面的湖仓管理工具和优化技术来解决这些挑战，这些工具和技术透明地运行以维持系统性能和效率。</p>
<p>小文件管理代表了任何基于湖的存储系统中最关键的运营挑战之一。随着流数据的连续到达，自然倾向于创建大量小文件，这会降低读取性能并增加元数据开销。Paimon的自动文件合并功能通过基于可配置策略智能合并小文件来解决这一挑战，确保存储保持优化而无需手动干预。</p>
<p>这些功能协同工作，在保持查询性能的同时最大限度地降低存储成本，这对于处理大量历史数据和实时流的组织来说是特别重要的考虑因素。</p>
<h2>技术演进：应对现实世界的挑战</h2>
<p>Flink和Paimon生态系统的成熟导致了越来越复杂的优化，这些优化解决了生产部署中遇到的特定性能瓶颈和运营挑战。从这一演进中出现了两个特别重要的改进：半结构化数据的增强处理和优化的Lookup Join操作。</p>
<h3>半结构化数据挑战</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-4d8c96c1ac.png"></p>
<p>现代数据环境的特点是半结构化数据格式的激增，其中JSON是最普遍的。Web应用程序、移动设备、物联网传感器和API驱动集成的兴起使JSON在企业数据管道中无处不在。然而，在使用传统流处理方法时，这种普遍性带来了显著的性能影响。</p>
<p>根本挑战在于JSON的自描述特性。与结构化数据中模式信息与数据本身分离不同，JSON直接在数据有效负载中嵌入类型和结构信息。虽然这提供了巨大的灵活性并启用了动态模式演化，但在流环境中处理大量JSON数据时会创建大量的计算开销。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d5d55e4776.png"></p>
<p>Flink中JSON处理的传统方法将半结构化数据视为简单的字符串值，每次需要访问任何字段时都需要完整的解析操作。这种架构决策虽然实现简单，但性能不太理想。即使访问JSON对象中的第一个字段也需要解析整个文档，由于数据作为字符串值在Flink的作业中分发，每个需要处理JSON的下游算子都必须重复完整的解析操作。</p>
<p>存储也有同样的问题。JSON的基于文本的格式虽然人类可读且广泛支持，但消耗的存储空间比等效的二进制表示形式多得多。这种增加的存储占用直接转化为更高的成本和在随机操作期间增加的网络带宽消耗，其中数据在流处理管道中的算子之间移动。</p>
<h3>Variant数据类型解决方案</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a87ec58242.png"></p>
<p>Variant数据类型的引入代表了Flink处理半结构化数据处理方法的根本转变。Variant不是将JSON视为不透明文本，而是提供了一种原生二进制表示，保持了半结构化数据的灵活性，同时提供了更接近结构化数据处理的性能特征。</p>
<p>Variant格式从更广泛的数据处理生态系统中的类似努力中汲取灵感，特别是Parquet提议的半结构化数据格式。通过采用开放标准方法，实现确保了与其他处理引擎的兼容性。</p>
<p>Variant采用的二进制编码策略通过几种机制实现性能改进。模式信息不是在整个数据中重复嵌入，而是在元数据部分中编码一次，显著减少存储开销。字段访问操作可以利用这些元数据直接导航到特定字段，而不解析数据结构的无关部分，大大提高了选择性查询的访问性能。</p>
<h3>增强的开发者体验</h3>
<p>除了性能改进之外，Variant在处理半结构化数据时为开发者体验引入了显著的增强。传统方法要求开发者使用复杂的SQL函数进行字段访问，创建冗长且容易出错的查询。Variant启用了更直观的语法模式，与开发者从其他编程环境的期望一致。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-82446879b2.png"></p>
<p>使用熟悉的括号表示法和点语法的直接字段访问简化了查询开发并使代码更易维护。数组元素访问遵循类似的模式，使开发者能够使用自然语法处理嵌套结构。类型转换功能允许与强类型下游处理的无缝集成，其中Variant字段可以根据需要转换为特定的数据类型。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-1b90993b38.png"></p>
<p>JSON字符串和Variant类型之间的转换函数为现有系统提供了迁移路径，同时启用了新格式的渐进采用。<code>PARSE_JSON</code>和<code>TRY_PARSE_JSON</code>函数处理从基于文本的JSON到二进制Variant格式的转换，后者为格式错误的输入数据提供错误处理功能。<code>JSON_STRING</code>函数在与尚未采用Variant支持的系统接口时启用转换回文本格式。</p>
<h3>Variant Shredding：针对现实世界模式的优化</h3>
<p>Variant实现中最复杂的优化解决了半结构化数据中的一个常见模式：经常访问的公共字段与真正动态部分的存在。虽然JSON的灵活性允许完全任意的结构，但生产系统经常表现出某些字段在记录中一致出现的模式，即使数据结构的其他部分变化显著。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b224333e5.png"></p>
<p>Variant Shredding通过将经常访问的字段作为单独的物理列存储在主Variant二进制结构之外来利用这一观察。这种混合方法结合了半结构化数据的灵活性与经常访问字段的列式存储性能特征。以这种方式 shredding 的字段可以以几乎与常规结构化列相同的性能进行访问。</p>
<p>这种优化的影响超越了简单的字段访问性能。shredding 字段可以完全参与Flink的查询优化，包括 projection 下推（其中仅从存储中读取所需列）和 filter 下推（其中谓词在尽可能接近数据源的地方进行评估）。这些优化可以大大减少I/O需求，在处理大型历史数据集和实时流时特别重要。</p>
<p>适合 shredding 的字段识别可以通过两种方法发生。手动配置允许开发者和数据工程师基于他们对数据访问模式和业务需求的理解明确指定 shredding 字段。对于处理多样化或不断发展的半结构化数据的组织，自动化发现机制可以分析传入的数据样本以识别出现频率足以从 shredding 优化中受益的字段。</p>
<h2>Lookup Join优化：解决可扩展性瓶颈</h2>
<p>第二个主要优化领域解决了实时分析中的一个常见架构模式：使用Lookup Join用存储在Paimon表中的维度信息来丰富流数据。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2cb09d4137.png"></p>
<h3>理解Lookup Join挑战</h3>
<p>Lookup Join代表了流分析中的关键操作，其中实时事件数据需要用相对静态的维度信息进行丰富。常见示例包括用客户档案信息丰富交易事件、向购买事件添加产品详细信息或用配置数据增强日志条目。挑战在于高效访问可能分布在多个存储分区中的维度数据，同时保持实时处理所需的低延迟特征。</p>
<p>Flink中的Lookup Join通常涉及三个阶段：基于连接键分发事实表数据的随机操作、从远程存储检索维度数据的获取操作，以及维护维度数据本地副本以供快速访问的缓存操作。</p>
<p>这种方法在传统的维度数据存储如Redis中运行得相当不错，因为其中数据本质上不分区，所有的算子都需要读取完整的数据。然而，Paimon的数据存储分桶策略与这种方法产生了根本性的不匹配，导致性能的浪费。</p>
<h3>分桶不匹配问题</h3>
<p>Paimon使用分桶策略组织数据，其中记录基于基于键的哈希函数分布在多个分桶中。这种方法提供了出色的可扩展性并启用了高效的数据组织，但它在传统Lookup Join实现中创造了显著的低效率。</p>
<p>核心问题是Flink的 Lookup 算子不知道Paimon的分桶策略。每个并发 Lookup 算子假设它可能需要与维度表中的任何记录连接，导致每个算子需要维护所有维度数据的完整副本。这意味着无论有多少并行算子处理Lookup Join，每一个都需要读取整个Paimon表并在本地缓存所有维度数据。</p>
<p>这种方法的影响在大规模部署中变得严重。作业启动时间可能延长到数十分钟，因为每个算子拉取完整的维度数据集。内存消耗随算子并行度提高，因为每个算子维护所有维度数据的重复副本。由于管理这些大型本地缓存的开销和为每个查找操作搜索完整数据集的计算成本，整体Lookup Join性能受到影响。</p>
<h3>自定义Shuffle策略解决方案</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-05c0be05e2.png"></p>
<p>该解决方案涉及扩展Flink的Lookup Join架构以支持自定义 shuffle 策略。这确保了预期到特定Paimon分桶的记录由负责该分桶维度数据的相同 Lookup 算子处理。</p>
<p>有了这种对齐，每个 Lookup 算子可以专门关注其分配的分桶数据。算子只需要维护其分配分桶数据的本地副本，而不是读取和缓存整个维度表。这大大减少了每个算子需要管理的数据量，并消除了算子之间的冗余存储。</p>
<p>性能改进效果非常明显，在高并行性场景中，作业启动时间也可以从数十分钟减少到几秒钟。每个算子的内存消耗显著下降，允许更高效的资源利用。由于较小的本地缓存和更集中的数据访问模式，整体Lookup Join性能提高。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-aea113f5c3.png"></p>
<h2>其他关键优化特性</h2>
<p>除了Variant数据类型和Lookup Join优化这两个核心改进之外，Flink和Paimon的集成还包含了一系列其他重要的优化特性，这些功能共同构成了完整的实时湖仓解决方案优化体系。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8b560c1552.png"></p>
<h3>Paimon Action/Procedure 易用性优化</h3>
<p>Paimon Action和Procedure功能的易用性优化代表了用户体验的重大改进。传统的湖仓管理操作往往需要复杂的配置和深度的技术专业知识，这对于开发者和运维人员来说构成了显著的使用门槛。</p>
<p>新的易用性优化简化了常见的湖仓管理任务，包括表的创建、数据的压缩、快照的管理和元数据的维护等操作。用户可以通过简单的SQL语句完成复杂的湖仓管理任务。</p>
<h3>Materialized Table 物化表支持</h3>
<p>物化表（Materialized Table）是 Flink 新引入的功能，主要是为了让用户能够通过 Flink SQL 写业务的代码，Flink 会自动根据用户指定的数据新鲜度需求来决定启动流作业或者批作业来生成 materialized table，保证表中的数据符合新鲜度的需求。用户免去了配置作业，以及维护作业的工作。</p>
<h3>Nested Projection Pushdown</h3>
<p>Nested Projection Pushdown优化专门针对复杂嵌套数据结构的查询性能问题。在现代数据环境中，JSON、Avro、Parquet等格式经常包含深层嵌套的数据结构，传统的查询处理往往需要读取整个嵌套对象，即使查询只需要其中的少数几个字段。</p>
<p>Nested Projection Pushdown技术能够分析查询中对嵌套字段的访问模式，并将字段选择操作推送到数据读取的最早阶段。这意味着在从存储系统读取数据时，就可以只提取查询实际需要的嵌套字段，而不是读取完整的嵌套结构。</p>
<p>这种优化对于包含大量嵌套字段的数据特别有效。例如，对于包含数百个字段的用户行为事件数据，如果查询只需要其中的几个关键字段，Nested Projection Pushdown能够将I/O开销降低一个数量级。同时，这种优化还能减少网络传输的数据量和内存使用量，从而提升整个查询处理管道的效率。</p>
<h3>Partial Update Sink Reuse 和性能优化</h3>
<p>Paimon 的 partial update 也是非常常用的功能。但是在做 partial update 的时候，通常需要在一个作业中有多个数据源写入同一张 Paimon 表，当 Flink 作业做 checkpoint 的时候，会有 sink 同时做 compaction，这会导致作业一直 Failover。为此我们对 Flink 的 sql planner 做了改动，让它能够识别出相同的 sink 进行复用，从而避免这种问题</p>
<h2>技术发展路线图与版本规划</h2>
<p>Flink和Paimon的集成发展遵循着清晰的技术路线图，不同的优化特性按照成熟度和优先级被分配到不同的版本发布周期中。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-eb423f1e6e.png"></p>
<h3>已发布功能特性</h3>
<p>在当前已发布的版本中，核心的优化功能已经投入生产使用，包括前面详细讨论的Lookup Join优化、Paimon Action/Procedure的易用性改进、物化表支持、Nested Projection Pushdown，以及Partial Update Sink Reuse等关键特性。这些功能已经通过了大规模生产环境的验证，能够为企业级实时湖仓部署提供稳定可靠的性能保障。</p>
<h3>Flink 2.1 与 Paimon 1.3 版本特性</h3>
<p>在Flink 2.1和Paimon 1.3的版本发布中，重点关注Variant数据类型的基础支持能力。这个版本将提供完整的Variant类型读写功能，使得用户能够在Flink和Paimon中原生处理半结构化数据，而不需要依赖复杂的字符串解析操作。</p>
<p>同时，这个版本还将支持Variant配置的shredding字段功能，允许用户根据数据访问模式手动配置哪些字段需要进行shredding优化。这为有明确数据访问模式的企业用户提供了精细控制的能力，能够针对具体的业务场景进行深度的性能调优。</p>
<h3>Flink 2.2 版本增强功能</h3>
<p>Flink 2.2版本将进一步完善Variant数据类型的功能，重点提供更加灵活和强大的字段访问能力。用户将能够使用直观的语法访问Variant类型中的嵌套字段，同时支持灵活的类型转换操作，使得Variant类型能够与现有的强类型处理流程无缝集成。</p>
<p>这个版本的Variant类型支持将使得半结构化数据的处理体验接近传统结构化数据，同时保持了半结构化数据的灵活性优势。</p>
<h2>展望未来：技术创新的新方向</h2>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ed9f8dca57.png"></p>
<h3>半结构化数据处理的智能化发展</h3>
<p>在半结构化数据处理领域，未来的发展将更加注重自动化和智能化。Variant shredding功能将支持自动设置shredding字段，系统能够通过分析历史查询模式和数据访问频率，自动识别哪些字段适合进行shredding优化，无需用户手动配置。</p>
<p>更进一步，Flink将支持Variant字段访问的下推优化到数据源层面，结合读裁剪优化技术，能够在数据读取阶段就完成字段的选择和过滤操作。这种源端优化将大大减少数据传输和处理的开销，特别是在处理大规模半结构化数据时能够带来显著的性能提升。</p>
<h3>非结构化数据处理能力扩展</h3>
<p>Flink的发展规划还包括对非结构化数据处理能力的重要扩展。未来版本将能够处理文本、图像、音频等非结构化数据类型，这为构建更加全面的数据处理平台奠定了基础。</p>
<p>非结构化数据处理能力的引入将使得Flink不仅能够处理传统的业务数据，还能够支持内容分析、多媒体处理、文档解析等更广泛的应用场景。这种扩展将进一步巩固Flink作为统一数据处理平台的地位。</p>
<hr>
<h3>更多内容</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d80a524eac.jpg"></p>
<hr>
<h3>活动推荐</h3>
<p>复制下方链接或者扫描二维码 即可快速体验 "一体化的实时数仓联合解决方案" 了解活动详情：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.aliyun.com%2Fsolution%2Ftech-solution%2Fflink-hologres" target="_blank">https://www.aliyun.com/solution/tech-solution/flink-hologres</a></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ae89a88bf2.png"></p>]]>
    </description>
    <content:encoded><![CDATA[<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-897a40de65.png"></p>
<h2>引言</h2>
<p>现代数据环境要求架构能够无缝融合数据湖的灵活性与传统数据仓库的性能特征。随着企业越来越多地采用实时分析来驱动业务决策，Apache Flink作为流处理引擎与Apache Paimon作为湖存储格式的结合，已成为构建强大实时湖仓平台的引人注目的解决方案。</p>
<p><strong>本文</strong>整理自 Apache CommunityOverCode Asia 2025 大会上，阿里云技术专家，Apache Flink Committer 苏轩楠分享了基于 Paimon构建的 Flink 实时湖仓解决方案持续演进的深刻见解。这一技术深度解析探讨了为解决大规模流式分析平台实施过程中面临的现实挑战而开发的关键优化和架构改进。</p>
<p>随着需要处理日益增长的结构化和半结构化数据量，传统的数据处理方法在性能、成本效益和运营复杂性方面往往力不从心。所讨论的增强功能代表了在生产环境中经过测试和完善的实用解决方案，为寻求现代化数据基础设施的组织提供了具体的实施路径。</p>
<h2>实时湖仓架构格局</h2>
<p>在深入技术优化之前，有必要了解围绕Flink和Paimon集成而形成的典型架构模式。实时湖仓方法代表了从传统批处理导向的数据仓库模式的根本转变，拥抱了数据到达时连续处理的模型。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-bef478601e.png"></p>
<h3>基础架构组件</h3>
<p>基于Flink和Paimon构建的现代实时湖仓通常由几个相互连接的处理层组成，每个层都服务于不同的目的，同时在整个系统中保持无缝的数据流。在基础层，Flink CDC（变更数据捕获）在建立统一的全量和增量数据同步能力方面发挥着关键作用。</p>
<p>Flink CDC在弥合操作数据库和分析系统之间的差距方面已被证明特别有价值。与需要在固定时间表上运行的复杂ETL管道不同，Flink CDC使组织能够实时捕获来自MySQL等源系统的变更，并直接将其流式传输到Paimon的ODS（操作数据存储）层。这种方法不仅减少了延迟，还通过消除对中间暂存区和复杂协调机制的需要来简化整体架构。</p>
<p>这些能力超越了简单的数据复制。现代实现支持完整的数据库同步场景，其中整个数据库模式可以迁移到湖仓格式，并完整支持自动模式演化处理。这意味着当源系统模式发生变化时------无论是通过添加新列、修改数据类型还是重构关系------下游Paimon表都可以自动适应，无需手动干预或管道重建。</p>
<h3>数据处理与转换层</h3>
<p>一旦数据通过摄取层进入湖仓，它就会经历一系列处理阶段，逐步完善和丰富信息。数据仓库明细（DWD）层代表第一个主要转换阶段，原始操作数据在此经历清洗、标准化和丰富操作。</p>
<p>这些转换通常涉及复杂的数据连接操作，通过组合来自多个源系统的信息来创建"宽表"。例如，电子商务组织可能会将客户档案数据与交易历史、产品目录和营销活动信息连接起来，以创建全面的客户行为数据集。这种处理的实时特性意味着随着源数据的变化，这些丰富的视图保持最新，为分析师和应用程序提供新鲜的见解，而没有传统批处理方法固有的延迟。</p>
<p>处理继续进展到数据仓库汇总（DWS）层，聚合计算产生业务指标和关键绩效指标。与传统数据仓库中这些聚合可能每天或每小时计算一次不同，实时湖仓方法能够在事件发生时连续计算业务指标。这种能力对于需要实时监控业务绩效、快速响应运营问题或基于分析见解触发自动化操作的组织来说是变革性的。</p>
<h3>湖仓管理与优化</h3>
<p>湖仓内的数据管理带来了与传统数据仓库管理显著不同的独特挑战。Paimon通过一套全面的湖仓管理工具和优化技术来解决这些挑战，这些工具和技术透明地运行以维持系统性能和效率。</p>
<p>小文件管理代表了任何基于湖的存储系统中最关键的运营挑战之一。随着流数据的连续到达，自然倾向于创建大量小文件，这会降低读取性能并增加元数据开销。Paimon的自动文件合并功能通过基于可配置策略智能合并小文件来解决这一挑战，确保存储保持优化而无需手动干预。</p>
<p>这些功能协同工作，在保持查询性能的同时最大限度地降低存储成本，这对于处理大量历史数据和实时流的组织来说是特别重要的考虑因素。</p>
<h2>技术演进：应对现实世界的挑战</h2>
<p>Flink和Paimon生态系统的成熟导致了越来越复杂的优化，这些优化解决了生产部署中遇到的特定性能瓶颈和运营挑战。从这一演进中出现了两个特别重要的改进：半结构化数据的增强处理和优化的Lookup Join操作。</p>
<h3>半结构化数据挑战</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-4d8c96c1ac.png"></p>
<p>现代数据环境的特点是半结构化数据格式的激增，其中JSON是最普遍的。Web应用程序、移动设备、物联网传感器和API驱动集成的兴起使JSON在企业数据管道中无处不在。然而，在使用传统流处理方法时，这种普遍性带来了显著的性能影响。</p>
<p>根本挑战在于JSON的自描述特性。与结构化数据中模式信息与数据本身分离不同，JSON直接在数据有效负载中嵌入类型和结构信息。虽然这提供了巨大的灵活性并启用了动态模式演化，但在流环境中处理大量JSON数据时会创建大量的计算开销。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d5d55e4776.png"></p>
<p>Flink中JSON处理的传统方法将半结构化数据视为简单的字符串值，每次需要访问任何字段时都需要完整的解析操作。这种架构决策虽然实现简单，但性能不太理想。即使访问JSON对象中的第一个字段也需要解析整个文档，由于数据作为字符串值在Flink的作业中分发，每个需要处理JSON的下游算子都必须重复完整的解析操作。</p>
<p>存储也有同样的问题。JSON的基于文本的格式虽然人类可读且广泛支持，但消耗的存储空间比等效的二进制表示形式多得多。这种增加的存储占用直接转化为更高的成本和在随机操作期间增加的网络带宽消耗，其中数据在流处理管道中的算子之间移动。</p>
<h3>Variant数据类型解决方案</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-a87ec58242.png"></p>
<p>Variant数据类型的引入代表了Flink处理半结构化数据处理方法的根本转变。Variant不是将JSON视为不透明文本，而是提供了一种原生二进制表示，保持了半结构化数据的灵活性，同时提供了更接近结构化数据处理的性能特征。</p>
<p>Variant格式从更广泛的数据处理生态系统中的类似努力中汲取灵感，特别是Parquet提议的半结构化数据格式。通过采用开放标准方法，实现确保了与其他处理引擎的兼容性。</p>
<p>Variant采用的二进制编码策略通过几种机制实现性能改进。模式信息不是在整个数据中重复嵌入，而是在元数据部分中编码一次，显著减少存储开销。字段访问操作可以利用这些元数据直接导航到特定字段，而不解析数据结构的无关部分，大大提高了选择性查询的访问性能。</p>
<h3>增强的开发者体验</h3>
<p>除了性能改进之外，Variant在处理半结构化数据时为开发者体验引入了显著的增强。传统方法要求开发者使用复杂的SQL函数进行字段访问，创建冗长且容易出错的查询。Variant启用了更直观的语法模式，与开发者从其他编程环境的期望一致。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-82446879b2.png"></p>
<p>使用熟悉的括号表示法和点语法的直接字段访问简化了查询开发并使代码更易维护。数组元素访问遵循类似的模式，使开发者能够使用自然语法处理嵌套结构。类型转换功能允许与强类型下游处理的无缝集成，其中Variant字段可以根据需要转换为特定的数据类型。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-1b90993b38.png"></p>
<p>JSON字符串和Variant类型之间的转换函数为现有系统提供了迁移路径，同时启用了新格式的渐进采用。<code>PARSE_JSON</code>和<code>TRY_PARSE_JSON</code>函数处理从基于文本的JSON到二进制Variant格式的转换，后者为格式错误的输入数据提供错误处理功能。<code>JSON_STRING</code>函数在与尚未采用Variant支持的系统接口时启用转换回文本格式。</p>
<h3>Variant Shredding：针对现实世界模式的优化</h3>
<p>Variant实现中最复杂的优化解决了半结构化数据中的一个常见模式：经常访问的公共字段与真正动态部分的存在。虽然JSON的灵活性允许完全任意的结构，但生产系统经常表现出某些字段在记录中一致出现的模式，即使数据结构的其他部分变化显著。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-3b224333e5.png"></p>
<p>Variant Shredding通过将经常访问的字段作为单独的物理列存储在主Variant二进制结构之外来利用这一观察。这种混合方法结合了半结构化数据的灵活性与经常访问字段的列式存储性能特征。以这种方式 shredding 的字段可以以几乎与常规结构化列相同的性能进行访问。</p>
<p>这种优化的影响超越了简单的字段访问性能。shredding 字段可以完全参与Flink的查询优化，包括 projection 下推（其中仅从存储中读取所需列）和 filter 下推（其中谓词在尽可能接近数据源的地方进行评估）。这些优化可以大大减少I/O需求，在处理大型历史数据集和实时流时特别重要。</p>
<p>适合 shredding 的字段识别可以通过两种方法发生。手动配置允许开发者和数据工程师基于他们对数据访问模式和业务需求的理解明确指定 shredding 字段。对于处理多样化或不断发展的半结构化数据的组织，自动化发现机制可以分析传入的数据样本以识别出现频率足以从 shredding 优化中受益的字段。</p>
<h2>Lookup Join优化：解决可扩展性瓶颈</h2>
<p>第二个主要优化领域解决了实时分析中的一个常见架构模式：使用Lookup Join用存储在Paimon表中的维度信息来丰富流数据。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-2cb09d4137.png"></p>
<h3>理解Lookup Join挑战</h3>
<p>Lookup Join代表了流分析中的关键操作，其中实时事件数据需要用相对静态的维度信息进行丰富。常见示例包括用客户档案信息丰富交易事件、向购买事件添加产品详细信息或用配置数据增强日志条目。挑战在于高效访问可能分布在多个存储分区中的维度数据，同时保持实时处理所需的低延迟特征。</p>
<p>Flink中的Lookup Join通常涉及三个阶段：基于连接键分发事实表数据的随机操作、从远程存储检索维度数据的获取操作，以及维护维度数据本地副本以供快速访问的缓存操作。</p>
<p>这种方法在传统的维度数据存储如Redis中运行得相当不错，因为其中数据本质上不分区，所有的算子都需要读取完整的数据。然而，Paimon的数据存储分桶策略与这种方法产生了根本性的不匹配，导致性能的浪费。</p>
<h3>分桶不匹配问题</h3>
<p>Paimon使用分桶策略组织数据，其中记录基于基于键的哈希函数分布在多个分桶中。这种方法提供了出色的可扩展性并启用了高效的数据组织，但它在传统Lookup Join实现中创造了显著的低效率。</p>
<p>核心问题是Flink的 Lookup 算子不知道Paimon的分桶策略。每个并发 Lookup 算子假设它可能需要与维度表中的任何记录连接，导致每个算子需要维护所有维度数据的完整副本。这意味着无论有多少并行算子处理Lookup Join，每一个都需要读取整个Paimon表并在本地缓存所有维度数据。</p>
<p>这种方法的影响在大规模部署中变得严重。作业启动时间可能延长到数十分钟，因为每个算子拉取完整的维度数据集。内存消耗随算子并行度提高，因为每个算子维护所有维度数据的重复副本。由于管理这些大型本地缓存的开销和为每个查找操作搜索完整数据集的计算成本，整体Lookup Join性能受到影响。</p>
<h3>自定义Shuffle策略解决方案</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-05c0be05e2.png"></p>
<p>该解决方案涉及扩展Flink的Lookup Join架构以支持自定义 shuffle 策略。这确保了预期到特定Paimon分桶的记录由负责该分桶维度数据的相同 Lookup 算子处理。</p>
<p>有了这种对齐，每个 Lookup 算子可以专门关注其分配的分桶数据。算子只需要维护其分配分桶数据的本地副本，而不是读取和缓存整个维度表。这大大减少了每个算子需要管理的数据量，并消除了算子之间的冗余存储。</p>
<p>性能改进效果非常明显，在高并行性场景中，作业启动时间也可以从数十分钟减少到几秒钟。每个算子的内存消耗显著下降，允许更高效的资源利用。由于较小的本地缓存和更集中的数据访问模式，整体Lookup Join性能提高。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-aea113f5c3.png"></p>
<h2>其他关键优化特性</h2>
<p>除了Variant数据类型和Lookup Join优化这两个核心改进之外，Flink和Paimon的集成还包含了一系列其他重要的优化特性，这些功能共同构成了完整的实时湖仓解决方案优化体系。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-8b560c1552.png"></p>
<h3>Paimon Action/Procedure 易用性优化</h3>
<p>Paimon Action和Procedure功能的易用性优化代表了用户体验的重大改进。传统的湖仓管理操作往往需要复杂的配置和深度的技术专业知识，这对于开发者和运维人员来说构成了显著的使用门槛。</p>
<p>新的易用性优化简化了常见的湖仓管理任务，包括表的创建、数据的压缩、快照的管理和元数据的维护等操作。用户可以通过简单的SQL语句完成复杂的湖仓管理任务。</p>
<h3>Materialized Table 物化表支持</h3>
<p>物化表（Materialized Table）是 Flink 新引入的功能，主要是为了让用户能够通过 Flink SQL 写业务的代码，Flink 会自动根据用户指定的数据新鲜度需求来决定启动流作业或者批作业来生成 materialized table，保证表中的数据符合新鲜度的需求。用户免去了配置作业，以及维护作业的工作。</p>
<h3>Nested Projection Pushdown</h3>
<p>Nested Projection Pushdown优化专门针对复杂嵌套数据结构的查询性能问题。在现代数据环境中，JSON、Avro、Parquet等格式经常包含深层嵌套的数据结构，传统的查询处理往往需要读取整个嵌套对象，即使查询只需要其中的少数几个字段。</p>
<p>Nested Projection Pushdown技术能够分析查询中对嵌套字段的访问模式，并将字段选择操作推送到数据读取的最早阶段。这意味着在从存储系统读取数据时，就可以只提取查询实际需要的嵌套字段，而不是读取完整的嵌套结构。</p>
<p>这种优化对于包含大量嵌套字段的数据特别有效。例如，对于包含数百个字段的用户行为事件数据，如果查询只需要其中的几个关键字段，Nested Projection Pushdown能够将I/O开销降低一个数量级。同时，这种优化还能减少网络传输的数据量和内存使用量，从而提升整个查询处理管道的效率。</p>
<h3>Partial Update Sink Reuse 和性能优化</h3>
<p>Paimon 的 partial update 也是非常常用的功能。但是在做 partial update 的时候，通常需要在一个作业中有多个数据源写入同一张 Paimon 表，当 Flink 作业做 checkpoint 的时候，会有 sink 同时做 compaction，这会导致作业一直 Failover。为此我们对 Flink 的 sql planner 做了改动，让它能够识别出相同的 sink 进行复用，从而避免这种问题</p>
<h2>技术发展路线图与版本规划</h2>
<p>Flink和Paimon的集成发展遵循着清晰的技术路线图，不同的优化特性按照成熟度和优先级被分配到不同的版本发布周期中。</p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-eb423f1e6e.png"></p>
<h3>已发布功能特性</h3>
<p>在当前已发布的版本中，核心的优化功能已经投入生产使用，包括前面详细讨论的Lookup Join优化、Paimon Action/Procedure的易用性改进、物化表支持、Nested Projection Pushdown，以及Partial Update Sink Reuse等关键特性。这些功能已经通过了大规模生产环境的验证，能够为企业级实时湖仓部署提供稳定可靠的性能保障。</p>
<h3>Flink 2.1 与 Paimon 1.3 版本特性</h3>
<p>在Flink 2.1和Paimon 1.3的版本发布中，重点关注Variant数据类型的基础支持能力。这个版本将提供完整的Variant类型读写功能，使得用户能够在Flink和Paimon中原生处理半结构化数据，而不需要依赖复杂的字符串解析操作。</p>
<p>同时，这个版本还将支持Variant配置的shredding字段功能，允许用户根据数据访问模式手动配置哪些字段需要进行shredding优化。这为有明确数据访问模式的企业用户提供了精细控制的能力，能够针对具体的业务场景进行深度的性能调优。</p>
<h3>Flink 2.2 版本增强功能</h3>
<p>Flink 2.2版本将进一步完善Variant数据类型的功能，重点提供更加灵活和强大的字段访问能力。用户将能够使用直观的语法访问Variant类型中的嵌套字段，同时支持灵活的类型转换操作，使得Variant类型能够与现有的强类型处理流程无缝集成。</p>
<p>这个版本的Variant类型支持将使得半结构化数据的处理体验接近传统结构化数据，同时保持了半结构化数据的灵活性优势。</p>
<h2>展望未来：技术创新的新方向</h2>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ed9f8dca57.png"></p>
<h3>半结构化数据处理的智能化发展</h3>
<p>在半结构化数据处理领域，未来的发展将更加注重自动化和智能化。Variant shredding功能将支持自动设置shredding字段，系统能够通过分析历史查询模式和数据访问频率，自动识别哪些字段适合进行shredding优化，无需用户手动配置。</p>
<p>更进一步，Flink将支持Variant字段访问的下推优化到数据源层面，结合读裁剪优化技术，能够在数据读取阶段就完成字段的选择和过滤操作。这种源端优化将大大减少数据传输和处理的开销，特别是在处理大规模半结构化数据时能够带来显著的性能提升。</p>
<h3>非结构化数据处理能力扩展</h3>
<p>Flink的发展规划还包括对非结构化数据处理能力的重要扩展。未来版本将能够处理文本、图像、音频等非结构化数据类型，这为构建更加全面的数据处理平台奠定了基础。</p>
<p>非结构化数据处理能力的引入将使得Flink不仅能够处理传统的业务数据，还能够支持内容分析、多媒体处理、文档解析等更广泛的应用场景。这种扩展将进一步巩固Flink作为统一数据处理平台的地位。</p>
<hr>
<h3>更多内容</h3>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-d80a524eac.jpg"></p>
<hr>
<h3>活动推荐</h3>
<p>复制下方链接或者扫描二维码 即可快速体验 "一体化的实时数仓联合解决方案" 了解活动详情：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.aliyun.com%2Fsolution%2Ftech-solution%2Fflink-hologres" target="_blank">https://www.aliyun.com/solution/tech-solution/flink-hologres</a></p>
<p><img src="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-ae89a88bf2.png"></p>]]></content:encoded>
    <itunes:image href="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-897a40de65.png"/>
          <media:content url="https://democwise2016.github.io/action-RSS-Fulltext/file-cache/oscimg.oschina.net-897a40de65.png" medium="image"/>
    <pubDate>Mon, 13 Oct 2025 15:32:14 +0800</pubDate>
  </item></channel>
</rss>